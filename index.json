[{"authors":["admin"],"categories":null,"content":"I am a data scientist at Root Insurance. I have a Ph.D. in Statistics from Ohio State, and now focus on machine learning and natural language processing applications. Previously, I have been a research scientist at Battelle, a fellow in the Data Science for Social Good Fellowship at the University of Chicago, an energy forecaster at IGS Energy, and research assistant in the Campus Transit Lab.\n","date":1428451200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1428451200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a data scientist at Root Insurance. I have a Ph.D. in Statistics from Ohio State, and now focus on machine learning and natural language processing applications. Previously, I have been a research scientist at Battelle, a fellow in the Data Science for Social Good Fellowship at the University of Chicago, an energy forecaster at IGS Energy, and research assistant in the Campus Transit Lab.","tags":null,"title":"Andrew J. Landgraf","type":"authors"},{"authors":null,"categories":null,"content":"","date":1560379376,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560379376,"objectID":"6083a88ee3411b0d17ce02d738f69d47","permalink":"/about/","publishdate":"2019-06-12T15:42:56-07:00","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"About Me","type":"page"},{"authors":["Andrew J. Landgraf"],"categories":["data","R","scrape"],"content":"I am planning to finish school soon and I would like to shed some weight before moving on. I have collected a fair number of books that I will likely never use again and it would be nice to get some money for them. Sites like Amazon and eBay let you sell your book to other customers, but Amazon will also buy some of your books directly (Trade-Ins), saving you the hassle of waiting for a buyer.\nBefore selling, I remembered listening to a Planet Money episode about a couple of guys that tried to make money off of buying and selling used textbooks on Amazon. Their strategy was to buy books at the end of a semester when students are itching to get rid of them, and sell them to other students at the beginning of the next semester. To back up their business, they have been scraping Amazon\u0026rsquo;s website for years, keeping track of prices in order to find the optimal times to buy and sell.\nI collected a few books I was willing to part with and set up a scraper in R. I am primarily interested in selling my books to Amazon, so I tracked Amazon\u0026rsquo;s Trade-In prices for these books. This was done fairly easily with Hadley Wickham\u0026rsquo;s package rvest and the Chrome extension Selector Gadget. Selector Gadget tells me that the node I\u0026rsquo;m interested in is #tradeInButton_tradeInValue. The code to do the scraping is below.\nlibrary(rvest) urls_df = read.csv(\u0026quot;AmazonBookURLs.csv\u0026quot;, stringsAsFactors = FALSE, comment.char = \u0026quot;\u0026quot;) load(file = \u0026quot;AmazonBookPrices.RData\u0026quot;) price_df_temp = data.frame(Title = urls_df$Title, Date = Sys.time(), Price = NA_real_, stringsAsFactors = FALSE) for (i in 1:nrow(urls_df)) { tradein_html = urls_df$URL[i] %\u0026gt;% html() %\u0026gt;% html_node(\u0026quot;#tradeInButton_tradeInValue\u0026quot;) if (is.null(tradein_html)) { next } price = tradein_html %\u0026gt;% html_text() %\u0026gt;% gsub(\u0026quot;(^[[:space:]]+\\\\$|[[:space:]]+$)\u0026quot;, \u0026quot;\u0026quot;, .) %\u0026gt;% as.numeric() price_df_temp$Price[i] = price } price_df = rbind(price_df, price_df_temp) save(price_df, file = \u0026quot;AmazonBookPrices.RData\u0026quot;)  After manually collecting this data for less than a week, I am able to plot the trends for the eight books I am interested in selling. The plot and code are below.\nlibrary(ggplot2); theme_set(theme_bw()) library(scales) price_df$TitleTrunc = paste0(substring(price_df$TitleTrunc, 1, 30), ifelse(nchar(price_df$Title) \u0026gt; 30, \u0026quot;...\u0026quot;, \u0026quot;\u0026quot;)) ggplot(price_df, aes(Date, Price)) + geom_step() + geom_point() + facet_wrap(~ TitleTrunc, scales = \u0026quot;free_y\u0026quot;) + scale_y_continuous(labels = dollar) + theme(axis.text.x = element_text(angle = 90, vjust = 0))  I am surprised how much the prices fluctuate. I expected them to be constant for the most part, with large changes once a week or less often. Apparently Amazon is doing quite a bit of tweaking to determine optimal price points. I would also guess that $2 is their minimum trade-in price. It looks like I missed out on my best chance to sell A First Course in Stochastic Processes, but that the price of A Primer on Linear Models will keep rising forever.\n","date":1428451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1428451200,"objectID":"9a3882fe9e83536825aa0b95114d1f82","permalink":"/2015/04/08/monitoring-price-fluctuations-of-book-trade-in-values-on-amazon/","publishdate":"2015-04-08T00:00:00Z","relpermalink":"/2015/04/08/monitoring-price-fluctuations-of-book-trade-in-values-on-amazon/","section":"post","summary":"I am planning to finish school soon and I would like to shed some weight before moving on. I have collected a fair number of books that I will likely never use again and it would be nice to get some money for them. Sites like Amazon and eBay let you sell your book to other customers, but Amazon will also buy some of your books directly (Trade-Ins), saving you the hassle of waiting for a buyer.","tags":null,"title":"Monitoring Price Fluctuations of Book Trade-In Values on Amazon","type":"post"},{"authors":["Andrew J. Landgraf"],"categories":["photography","R"],"content":"Time lapses are a fun way to quickly show a long period of time. They typically involve setting up your camera on a tripod and taking photos at a regular interval, like every 5 seconds. After all the photos have been taken, they are combined into a movie at a much faster rate, for example 30 frames per second.\nTime stacking is a way to combine all the photos into a single photo, instead of a movie. This is a common method to make star trails, and Matt Molloy has recently been experimenting with it in many different settings. There are many possible ways to achieve a time stack, but the most common way is to combine the photos with a lighten layer blend. For every pixel in the final image, the combined photo will use the corresponding pixel from the photo that was the brightest in all of the photos. This gives the desired result of motion of the stars or clouds in a scene.\nAnother way to combine the photos is through time slicing (see, for example, this photo). In time slicing, the final combined image will contain a \u0026ldquo;slice\u0026rdquo; from each of the original photos. Time slices can go left-to-right, right-to-left, top-to-bottom, or bottom-to-top. For example, a time slice that goes from left to right will use vertical slices of the pixels. If you took 100 photos for your time lapse, each of which being 1000 pixels wide, the left-most 10 vertical pixel slices of the final image would contain the corresponding pixels from the first photo, the 11th through 20th vertical pixel slices would contain the corresponding pixels from the second photo, and so on. Different directions will produce different effects.\nThere is free software available to do lighten layer blending of two photos, but I could not find any to automatically do it for a large number of photos. Similarly for the time slice, it is easy enough to manually slice a few photos, but not hundreds of photos. Therefore, I wrote a couple of scripts in R that would do this automatically. A gist of the scripts to create a time stack script and a time slice is here. They both require you to give the directory containing the JPEG photos and the time slice script let\u0026rsquo;s you enter the direction you would like it to go.\n  To try this out on my own photos, I used the time lapse I had created of a sunset (movie created with FFmpeg), which consisted of 225 photos taken over 20 minutes. The source material isn\u0026rsquo;t that great (the photos were out of focus), but you can still see the effects.\nThe following picture is a time stack.\nThe following four pictures are the time slices with different directions.\n","date":1419379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1419379200,"objectID":"e920a01b2a015d156cb4991cea16d9e4","permalink":"/2014/12/24/time-stacking-and-time-slicing-in-r/","publishdate":"2014-12-24T00:00:00Z","relpermalink":"/2014/12/24/time-stacking-and-time-slicing-in-r/","section":"post","summary":"Time lapses are a fun way to quickly show a long period of time. They typically involve setting up your camera on a tripod and taking photos at a regular interval, like every 5 seconds. After all the photos have been taken, they are combined into a movie at a much faster rate, for example 30 frames per second.\nTime stacking is a way to combine all the photos into a single photo, instead of a movie.","tags":null,"title":"Time Stacking and Time Slicing in R","type":"post"},{"authors":["Andrew J. Landgraf"],"categories":["statistics","data","baseball","R"],"content":" Fangraphs recently published an interesting dataset that measures defensive efficiency of fielders. For each player, the Inside Edge dataset breaks their opportunities to make plays into five categories, ranging from almost impossible to routine. It also records the proportion of times that the player successfully made the play. With this data, we can see how successful each player is for each type of play. I wanted to think of a way to combine these five proportions into one fielding metric. From here on, I will assume that there is no error in categorizing a play as easy or hard and that there is no bias in the categorizations.\nModel The model I will build is motivated by ordinal regression. If we only were concerned with the success rate in one of the categories, we could use standard logistic regression, and the probability that player $i$ successfully made a play would be assumed to be $\\sigma(\\theta_i)$, where $\\sigma()$ is the logistic function. Using our prior knowledge that plays categorized as easy should have a higher success rate than plays categorized as difficult, I would like to generalize this.\nSay there are only two categories: easy and hard. We could model the probability that player $i$ successfully made an hard play as $\\sigma(\\theta_i)$ and the probability that he made an easy play as $\\sigma(\\theta_i+\\gamma)$. Here, we would assume that $\\gamma$ is the same for all players. This assumption implies that if player $i$ is better than player $j$ at easy plays, he will also be better at hard plays. This is a reasonable assumption, but maybe not true in all cases.\nSince we have five different categories of difficulty, we can generalize this by having $\\gamma_k, k=1,\\ldots,4$. Again, these $\\gamma_k$\u0026rsquo;s would be the same for everyone. A picture of what this looks like for shortstops is below. In this model, every player will effectively be shifting the curve either left or right. A positive $\\theta_i$ means the player is better than average and cause the curve to shift left and vice versa for negative $\\theta_i$.\nI modeled this as a multi-level mixed effects model, with the players being random effects and the $\\gamma_k$\u0026rsquo;s being fixed. Technically, I should optimize subject to the condition that the $\\gamma_k$\u0026rsquo;s are increasing, but the unconstrained optimization always yields increasing $\\gamma_k$\u0026rsquo;s because there is a big difference between success rate in the categories. I used combined data from 2012 and 2013 seasons and included all players with at least one success and one failure. I modeled each position separately. Modeling player effects as random, there is a fair amount of regression to the mean built in. In this sense, I am more trying to estimate the true ability of the player, rather than measuring what he did during the two years. This is an important distinction, which may differ from other defensive statistics.\nModel fit Below is a summary of the results of the model for shortstops. I am only plotting the players with the at least 800 innings, for readability. A bonus of modeling the data like this is that we get standard error estimates as a result. I plotted the estimated effect of each player along with +/- 2 standard errors. We can be fairly certain that the effects for the top few shortstops is greater than 0 since their confidence intervals do not include 0. The same is true for the bottom few. Images for the other positions can be found here.\nThe results seem to make sense for the most part. Simmons and Tulowitzki have reputations as being strong defenders and Derek Jeter has a reputation as a poor defender.\nFurther, I can validate this data by comparing it to other defensive metrics. One that is readily available on Fangraphs is UZR per 150 games. For each position, I took the correlation of my estimated effect with UZR per 150 games, weighted by the number of innings played. Pitchers and catchers do not have UZR\u0026rsquo;s so I cannot compare them. The correlations, which are in the plot below, range from about 0.2 to 0.65.\nInterpreting parameters In order to make this fielding metric more useful, I would like to convert the parameters to something more interpretable. One option which makes a lot of sense is \u0026ldquo;plays made above/below average\u0026rdquo;. Given an estimated $\\theta_i$ for a player, we can calculate the probability that he would make a play in each of the five categories. We can then compare those probabilities to the probability an average player would make a play in each of the categories, which would be fit with $\\theta=0$. Finally, we can weight these differences in probabilities by the relative rate that plays of various difficulties occur.\nFor example, assuming there are only two categories again, suppose a player has a 0.10 and 0.05 higher probability than average of making hard and easy plays, respectively. Further assume that 75% of all plays are hard and 25% are easy. On a random play, the improvement in probability over an average player of making a play is $.10(.75)+.05(.25)=0.0875$. If a player has an opportunity for 300 plays in an average season, this player would be $300 \\times 0.0875=26.25$ plays better than average over a typical season.\nI will assume that the number of opportunities to make plays is directly related to the number of innings played. To convert innings to opportunities, I took the median number of opportunities per inning for each position. For example, shortstops had the highest opportunities per inning at 0.40 and catchers had the lowest at 0.08. The plot below shows the distribution of opportunities per inning for each position.\nWe can extend this to the impact on saving runs from being scored as well by assuming each successful play saves $x$ runs. I will not do this for this analysis.\nResults [Update: The Shiny app is no longer live, but you can view the resulting estimates in the Github repository.]\nFinally, I put together a Shiny app to display the results. You can search by team, position, and innings played. A team of \u0026lsquo;- - -\u0026rsquo; means the player played for multiple teams over this period. You can also choose to display the results as a rate statistic (extra plays made per season) or a count statistic (extra plays made over the two seasons). To get a seasonal number, I assume position players played 150 games with 8.5 innings in each game. For pitchers, I assumed that they pitched 30 games, 6 innings each.\nConclusions and future work I don\u0026rsquo;t know if I will do anything more with this data, but if I could do it again, I may have modeled each year separately instead of combining the two years together. With that, it would have been interesting to model the effect of age by observing how a player\u0026rsquo;s ability to make a play changes from one year to the next. I also think it would be interesting to see how changing positions affects a players efficiency. For example, we could have a $9 \\times 9$ matrix of fixed effects that represent the improvement or degradation in ability as a player switches from their main position to another one. Further assumptions would be needed to make sure the $\\theta$\u0026rsquo;s are on the same scale for every position.\nAt the very least, this model and its results can be considered another data point in the analysis of a player\u0026rsquo;s fielding ability. One thing we need to be concerned about is the classification of each play into the difficultness categories. The human eye can be fooled into thinking a routine play is hard just because a fielder dove to make the play, when a superior fielder could have made it look easier.\nCode I have put the R code together to do this analysis in a gist. If there is interest, I will put together a repo with all the data as well.\nUpdate: I have a Github repository with the data, R code for the analysis, the results, and code for the Shiny app. Let me know what you think.\n","date":1398124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398124800,"objectID":"509629f23c4d53910009d05f37809063","permalink":"/2014/04/22/yet-another-baseball-defense-statistic/","publishdate":"2014-04-22T00:00:00Z","relpermalink":"/2014/04/22/yet-another-baseball-defense-statistic/","section":"post","summary":"Fangraphs recently published an interesting dataset that measures defensive efficiency of fielders. For each player, the Inside Edge dataset breaks their opportunities to make plays into five categories, ranging from almost impossible to routine. It also records the proportion of times that the player successfully made the play. With this data, we can see how successful each player is for each type of play. I wanted to think of a way to combine these five proportions into one fielding metric.","tags":null,"title":"Yet Another Baseball Defense Statistic","type":"post"},{"authors":["Andrew J. Landgraf"],"categories":["statistics","data"],"content":"Trevor Hastie and Rob Tibshirani are currently teaching a MOOC covering an introduction to statistical learning. I am very familiar with most of the material in the course, having read Elements of Statistical Learning many times over.\nOne great thing about the class, however, is that they are truely experts and have collaborated with many of the influencial researchers in their field. Because of this, when covering certain topics, they have included interviews with statisticians who made important developments to the field. When introducing the class to R, they interviewed John Chambers, who was able to give a personal account of the history of S and R because he was one of the developers. Further, when covering resampling methods, they spoke with Brad Efron, who talked about the history of the bootstrap and how he struggled to get it published.\nToday, they released a video interview with Jerome Friedman. Friedman revealed many interesting facts about the history of tree-based methods, including the fact that there weren\u0026rsquo;t really any journal articles written about CART when they wrote their book. There was one quote that I particularly enjoyed.\n And of course, I\u0026rsquo;m very gratified that something that I was intellectually interested in for all this time has now become very popular and very important. I mean, data has risen to the top. My only regret is two of my mentors who also pushed it, probably harder and more effectively than I did \u0026ndash; namely, John Tukey and Leo Breiman \u0026ndash; are not around to actually see how data has triumphed over, say, theorem proving.\n   ","date":1393718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393718400,"objectID":"74a408dd673ff246d06acad6e9d35e5b","permalink":"/2014/03/02/hastie-and-tibshirani-interview-jerome-friedman/","publishdate":"2014-03-02T00:00:00Z","relpermalink":"/2014/03/02/hastie-and-tibshirani-interview-jerome-friedman/","section":"post","summary":"Trevor Hastie and Rob Tibshirani are currently teaching a MOOC covering an introduction to statistical learning. I am very familiar with most of the material in the course, having read Elements of Statistical Learning many times over.\nOne great thing about the class, however, is that they are truely experts and have collaborated with many of the influencial researchers in their field. Because of this, when covering certain topics, they have included interviews with statisticians who made important developments to the field.","tags":null,"title":"Hastie and Tibshirani Interview Jerome Friedman","type":"post"},{"authors":["Andrew J. Landgraf"],"categories":["shiny","data","ggplot","Visualization","R"],"content":" In a previous post, I showed you how to scrape playlist data from Columbus, OH alternative rock station CD102.5. Since it's the end of the year and best-of lists are all the fad, I thought I would share the most popular songs and artists of the year, according to this data. In addition to this, I am going to make an interactive graph using Shiny, where the user can select an artist and it will graph the most popular songs from that artist.\nFirst off, I am assuming that you have scraped the appropriate data using the code from the previous post.\nlibrary(lubridate)\nlibrary(sqldf)\nplaylist=read.csv(\"CD101Playlist.csv\",stringsAsFactors=FALSE)\ndates=mdy(substring(playlist[,3],nchar(playlist[,3])-9,nchar(playlist[,3])))\ntimes=hm(substring(playlist[,3],1,nchar(playlist[,3])-10))\nplaylist$Month=ymd(paste(year(dates),month(dates),\"1\",sep=\"-\"))\nplaylist$Day=dates\nplaylist$Time=times\nplaylist=playlist[order(playlist$Day,playlist$Time),]\nNext, I will select just the data from 2013 and find the songs that were played most often. playlist=subset(playlist,Day\u0026gt;=mdy(\"1/1/13\"))\nplaylist$ArtistSong=paste(playlist$Artist,playlist$Song,sep=\"-\")\ntop.songs=sqldf(\"Select ArtistSong, Count(ArtistSong) as Num\nFrom playlist\nGroup By ArtistSong\nOrder by Num DESC\nLimit 10\")\n\nThe top 10 songs are the following:\nArtist-Song Number Plays\n1 FITZ AND THE TANTRUMS-OUT OF MY LEAGUE 809\n2 ALT J-BREEZEBLOCKS 764\n3 COLD WAR KIDS-MIRACLE MILE 759\n4 ATLAS GENIUS-IF SO 750\n5 FOALS-MY NUMBER 687\n6 MS MR-HURRICANE 679\n7 THE NEIGHBOURHOOD-SWEATER WEATHER 657\n8 CAPITAL CITIES-SAFE AND SOUND 646\n9 VAMPIRE WEEKEND-DIANE YOUNG 639\n10 THE FEATURES-THIS DISORDER 632\n\nI will make a plot similar to the plots made in the last post to show when the top 5 songs were played throughout the year.\nplays.per.day=sqldf(\"Select Day, Count(Artist) as Num\nFrom playlist\nGroup By Day\nOrder by Day\")\nplaylist.top.songs=subset(playlist,ArtistSong %in% top.songs$ArtistSong[1:5])\nsong.per.day=sqldf(paste0(\"Select Day, ArtistSong, Count(ArtistSong) as Num\nFrom [playlist.top.songs]\nGroup By Day, ArtistSong\nOrder by Day, ArtistSong\"))\ndspd=dcast(song.per.day,Day~ArtistSong,sum,value.var=\"Num\")\nsong.per.day=merge(plays.per.day[,1,drop=FALSE],dspd,all.x=TRUE)\nsong.per.day[is.na(song.per.day)]=0\nsong.per.day=melt(song.per.day,1,variable.name=\"ArtistSong\",value.name=\"Num\")\nsong.per.day$Alpha=ifelse(song.per.day$Num\u0026gt;0,1,0)\nlibrary(ggplot2)\nggplot(song.per.day,aes(Day,Num,colour=ArtistSong))+geom_point(aes(alpha=Alpha))+\ngeom_smooth(method=\"gam\",family=poisson,formula=y~s(x),se=F,size=1)+\nlabs(x=\"Date\",y=\"Plays Per Day\",title=\"Top Songs\",colour=NULL)+\nscale_alpha_continuous(guide=FALSE,range=c(0,.5))+theme_bw()\nAlt-J was more popular in the beginning of the year and the Foals have been more popular recently.\nI can similarly summarize by artist as well.\ntop.artists=sqldf(\"Select Artist, Count(Artist) as Num\nFrom playlist\nGroup By Artist\nOrder by Num DESC\nLimit 10\")\n\nArtist Num\n1 MUSE 1683\n2 VAMPIRE WEEKEND 1504\n3 SILVERSUN PICKUPS 1442\n4 FOALS 1439\n5 PHOENIX 1434\n6 COLD WAR KIDS 1425\n7 JAKE BUGG 1316\n8 QUEENS OF THE STONE AGE 1296\n9 ALT J 1233\n10 OF MONSTERS AND MEN 1150\n\nplaylist.top.artists=subset(playlist,Artist %in% top.artists$Artist[1:5])\nartists.per.day=sqldf(paste0(\"Select Day, Artist, Count(Artist) as Num\nFrom [playlist.top.artists]\nGroup By Day, Artist\nOrder by Day, Artist\"))\ndspd=dcast(artists.per.day,Day~Artist,sum,value.var=\"Num\")\nartists.per.day=merge(plays.per.day[,1,drop=FALSE],dspd,all.x=TRUE)\nartists.per.day[is.na(artists.per.day)]=0\nartists.per.day=melt(artists.per.day,1,variable.name=\"Artist\",value.name=\"Num\")\nartists.per.day$Alpha=ifelse(artists.per.day$Num\u0026gt;0,1,0)\nggplot(artists.per.day,aes(Day,Num,colour=Artist))+geom_point(aes(alpha=Alpha))+\ngeom_smooth(method=\"gam\",family=poisson,formula=y~s(x),se=F,size=1)+\nlabs(x=\"Date\",y=\"Plays Per Day\",title=\"Top Artists\",colour=NULL)+\nscale_alpha_continuous(guide=FALSE,range=c(0,.5))+theme_bw()\nThe pattern for the artists are not as clear as it is for the songs.\nFinally, I wrote a Shiny interactive app. They are surprisingly easy to create and if you are thinking about experimenting with it, I suggest you try it. I will leave the code for the app in a gist. In the app, you can enter any artist you want, and it will show you the most popular songs on CD102.5 for that artist. You can also select the number of songs that it plots with the slider.\nFor example, even though Muse did not have one of the most popular songs of the year, they were still the band that was played the most. By typing in \"MUSE\" in the Artist text input, you will get the following output.\nThey had two songs that were very popular this year and a few others that were decently popular as well.\nPlay around with it and let me know what you think. ","date":1388102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388102400,"objectID":"c2483cef7d3d0a2be77ff47f0a5a8782","permalink":"/2013/12/27/top-songs-by-artist-on-cd102.5-in-2013/","publishdate":"2013-12-27T00:00:00Z","relpermalink":"/2013/12/27/top-songs-by-artist-on-cd102.5-in-2013/","section":"post","summary":"In a previous post, I showed you how to scrape playlist data from Columbus, OH alternative rock station CD102.5. Since it's the end of the year and best-of lists are all the fad, I thought I would share the most popular songs and artists of the year, according to this data. In addition to this, I am going to make an interactive graph using Shiny, where the user can select an artist and it will graph the most popular songs from that artist.","tags":null,"title":"Top Songs by Artist on CD102.5 in 2013","type":"post"},{"authors":null,"categories":["statistics","ggplot","Visualization","R"],"content":" \u0026amp;lt;p\u0026amp;gt;Loading ...\u0026amp;lt;/p\u0026amp;gt; ","date":1377561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1377561600,"objectID":"d6d1e34381513672aebc6d1089ea6944","permalink":"/2013/08/27/when-did-cd102.5-book-the-summerfest-artists/","publishdate":"2013-08-27T00:00:00Z","relpermalink":"/2013/08/27/when-did-cd102.5-book-the-summerfest-artists/","section":"post","summary":" \u0026amp;lt;p\u0026amp;gt;Loading ...\u0026amp;lt;/p\u0026amp;gt; ","tags":null,"title":"When Did CD102.5 Book the Summerfest Artists?","type":"post"},{"authors":null,"categories":["data","exploratory data analysis","ggplot","Visualization","R"],"content":"\rCD1025 is an “alternative” radio station here in Columbus. They are one of the few remaining radio stations that are independently owned and they take great pride in it. For data nerds like me, they also put a real time list of recently played songs on their website. The page has the most recent 50 songs played, but you can also click on “Older Tracks” to go back in time. When you do this, the URL ends “now-playing/?start=50”. If you got back again, it says “now-playing/?start=100”.\nUsing this structure, I decided to see if I could download all of their historical data and see how far it goes back. In the code below, I use the XML package to go to the website and download the 50 songs and then increment the number by 50 to find the previous 50 songs. I am telling the code to keep doing this until I get to January 1, 2012.\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(XML)\nlibrary(lubridate)\nlibrary(sqldf)\nstartNum = 0\nwhile (TRUE) {\ntheurl \u0026lt;- paste0(\"http://cd1025.com/about/playlists/now-playing/?start=\", startNum)\ntable \u0026lt;- readHTMLTable(theurl, stringsAsFactors = FALSE)[[1]]\nif (startNum == 0) {\nplaylist = table[, -1]\n} else {\nplaylist = rbind(playlist, table[, -1])\n}\ndt = mdy(substring(table[1, 4], nchar(table[1, 4]) - 9, nchar(table[1, 4])))\nprint(dt)\nif (dt \u0026lt; mdy(\"1/1/12\")) {\nbreak\n}\nstartNum = startNum + 50\n}\nplaylist = unique(playlist) # Remove Dupes\nwrite.csv(playlist, \"CD101Playlist.csv\", row.names = FALSE)\nThis takes a while and is fairly large. My file has over 150,000 songs. If you want just a little data, change the date to last week or so. The first thing I will do is parse the dates and times of the songs, order them, and look at the first few songs. You can see that data only goes back to March of 2012.\ndates = mdy(substring(playlist[, 3], nchar(playlist[, 3]) - 9, nchar(playlist[, 3])))\ntimes = hm(substring(playlist[, 3], 1, nchar(playlist[, 3]) - 10))\nplaylist$Month = ymd(paste(year(dates), month(dates), \"1\", sep = \"-\"))\nplaylist$Day = dates\nplaylist$Time = times\nplaylist = playlist[order(playlist$Day, playlist$Time), ]\nhead(playlist)\n## Artist Song Last.Played\n## 151638 DEATH CAB FOR CUTIE YOU ARE A TOURIST 12:34am03/01/2012\n## 151637 SLEEPER AGENT GET BURNED 12:38am03/01/2012\n## 151636 WASHED OUT AMOR FATI 12:41am03/01/2012\n## 151635 COLDPLAY CHARLIE BROWN 12:45am03/01/2012\n## 151634 GROUPLOVE TONGUE TIED 12:49am03/01/2012\n## 151633 SUGAR YOUR FAVORITE THING 12:52am03/01/2012\n## Month Day Time\n## 151638 2012-03-01 2012-03-01 34M 0S\n## 151637 2012-03-01 2012-03-01 38M 0S\n## 151636 2012-03-01 2012-03-01 41M 0S\n## 151635 2012-03-01 2012-03-01 45M 0S\n## 151634 2012-03-01 2012-03-01 49M 0S\n## 151633 2012-03-01 2012-03-01 52M 0S\nUsing the sqldf package, I can easily see what the most played artists and songs are from the data I scraped.\nsqldf(\"Select Artist, Count(Artist) as PlayCount\nFrom playlist\nGroup By Artist\nOrder by PlayCount DESC\nLimit 10\")\n## Artist PlayCount\n## 1 SILVERSUN PICKUPS 2340\n## 2 THE BLACK KEYS 2203\n## 3 MUSE 1988\n## 4 THE SHINS 1885\n## 5 OF MONSTERS AND MEN 1753\n## 6 PASSION PIT 1552\n## 7 GROUPLOVE 1544\n## 8 RED HOT CHILI PEPPERS 1514\n## 9 METRIC 1495\n## 10 ATLAS GENIUS 1494\nsqldf(\"Select Artist, Song, Count(Song) as PlayCount\nFrom playlist\nGroup By Artist, Song\nOrder by PlayCount DESC\nLimit 10\")\n## Artist Song PlayCount\n## 1 PASSION PIT TAKE A WALK 828\n## 2 SILVERSUN PICKUPS PIT, THE 825\n## 3 ATLAS GENIUS TROJANS 819\n## 4 WALK THE MOON ANNA SUN 742\n## 5 THE BLACK KEYS LITTLE BLACK SUBMARINES 736\n## 6 DIVINE FITS WOULD THAT NOT BE NICE 731\n## 7 THE LUMINEERS HO HEY 722\n## 8 CAPITAL CITIES SAFE AND SOUND 712\n## 9 OF MONSTERS AND MEN MOUNTAIN SOUND 711\n## 10 ALT J BREEZEBLOCKS 691\nI am a little surprised that Silversun Pickups are the number one band, but everyone on the list makes sense. Looking at how the plays of the top artists have varied from month to month, you can see a few patterns. Muse has been more popular recently and The Shins and Grouplove have lost some steam.\nartist.month=sqldf(\"Select Month, Artist, Count(Song) as Num\nFrom playlist\nGroup By Month, Artist\nOrder by Month, Artist\")\nartist=sqldf(\"Select Artist, Count(Artist) as Num\nFrom playlist\nGroup By Artist\nOrder by Num DESC\")\np=ggplot(subset(artist.month,Artist %in% head(artist$Artist,8)),aes(Month,Num))\np+geom_bar(stat=\"identity\",aes(fill=Artist),position='fill',colour=\"grey\")+labs(y=\"Percentage of Plays\")\nFor the play count of the top artists, I see some odd numbers in June and July of 2012. The number of plays went way down.\np + geom_area(aes(fill = Artist), position = \"stack\", colour = 1) + labs(y = \"Number of Plays\")\n\n\nLooking into this further, I plotted the date and time that the song was played by the cumulative number of songs played since the beginning of the list. The plot should be a line with a constant slope, meaning that the plays per day are relatively constant. You can see in June and July of 2012 there are flat spots where there is no playlist history. qplot(playlist$Day + playlist$Time, 1:length(dates), geom = \"path\")\n\nThere are also smaller flat spots in September and December, but I am going to decide that those are small enough not to affect any further analyses. From this, I am going to use data only from August 2012 to present.\nplaylist = subset(playlist, Day \u0026gt;= mdy(\"8/1/12\"))\nNext up, I am going to use this data to analyze the plays of artists from Summerfest, and try to infer if the play counts varied once they were added to the bill.\r","date":1376956800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1376956800,"objectID":"454be80a0581161819e0cfa9f9a0c332","permalink":"/2013/08/20/downloading-and-analyzing-cd1025s-playlist/","publishdate":"2013-08-20T00:00:00Z","relpermalink":"/2013/08/20/downloading-and-analyzing-cd1025s-playlist/","section":"post","summary":"CD1025 is an “alternative” radio station here in Columbus. They are one of the few remaining radio stations that are independently owned and they take great pride in it. For data nerds like me, they also put a real time list of recently played songs on their website. The page has the most recent 50 songs played, but you can also click on “Older Tracks” to go back in time.","tags":null,"title":"Downloading and Analyzing CD1025's Playlist","type":"post"},{"authors":null,"categories":["statistics","GAM","college basketball","ggplot","R"],"content":"\rNote: I started this post way back when the NCAA men's basketball tournament was going on, but didn't finish it until now. \nSince the NCAA Men's Basketball Tournament has moved to 64 teams, a 16 seed as never upset a 1 seed. You might be tempted to say that the probability of such an event must be 0 then. But we know better than that.\nIn this post, I am interested in looking at different ways of estimating how the odds of winning a game change as the difference between seeds increases. I was able to download tournament data going back to the 1930s until 2012 from hoopstournament.net/Database.html. The tournament expanded to 64 teams in 1985, which is what I used for this post. I only used match ups in which one of the seeds was higher than the other because this was the easiest way to remove duplicates. (The database has each game listed twice, once with the winner as the first team and once with the loser as the first team. The vast majority (98.9%) of games had one team as a higher seed because an equal seed can only happen at the Final Four or later.)\nlibrary(ggplot2); theme_set(theme_bw())\nbrackets=read.csv(\"NCAAHistory.csv\")\n\u0026nbsp;\n# use only data from 1985 on in which the first team has the higher seed\nbrackets=subset(brackets,Seed\u0026lt;Opponent.Seed \u0026amp; Year\u0026gt;=1985 \u0026amp; Round!=\"Opening Round\")\n\u0026nbsp;\nbrackets$SeedDiff=abs(brackets$Opponent.Seed-brackets$Seed)\nbrackets$HigherSeedWon=ifelse(brackets$Opponent.Seed\u0026gt;brackets$Seed,brackets$Wins,brackets$Losses)\nbrackets$HigherSeedScoreDiff=ifelse(brackets$Opponent.Seed\u0026gt;brackets$Seed,1,-1)*(brackets$Score-brackets$Opponent.Score)Created by Pretty R at inside-R.org\nUse FrequenciesThe first way is the most simple: look at the historical records when a 16 seed is playing a 1 seed (where the seed difference is 15). As you can see from the plot below, when the seed difference is 15, the higher seeded team has won every time. This is also true when the seed difference is 12, although there have only been 4 games in this scenario. Another oddity is that when the seed difference is 10, the higher seed only has only won 50% of the time. Again, this is largely due to the fact that there have only been 6 games with this seed difference.\nseed.diffs=sort(unique(brackets$SeedDiff))\nwin.pct=sapply(seed.diffs,function(x) mean(brackets$HigherSeedWon[brackets$SeedDiff==x]))\nggplot(data=data.frame(seed.diffs,win.pct),aes(seed.diffs,win.pct))+geom_point()+\ngeom_hline(yintercept=0.5,linetype=2)+\ngeom_line()+labs(x=\"Seed Difference\",y=\"Proportion of Games Won by Higher Seed\")Created by Pretty R at inside-R.org\n\nUse Score DifferenceIn many applications, it has been shown that using margin of victory is much more reliable than just wins and losses. For example, in the computer ranking of College Football teams, using score differences is more accurate, but outlawed for fear that teams would run up the score on weaker opponents. So the computer rankings are not as strong as they could be.\nWe have no such conflict of interest, so we should try to make use of any information available. A simple way to do that is to look at the mean and standard deviation of the margin of victory when the 16 seed is playing the 1 seed. Below is a plot of the mean score difference with a ribbon for the +/- 2 standard deviations.\nseed.diffs=sort(unique(brackets$SeedDiff))\nmeans=sapply(seed.diffs,function(x) mean(brackets$HigherSeedScoreDiff[brackets$SeedDiff==x]))\nsds=sapply(seed.diffs,function(x) sd(brackets$HigherSeedScoreDiff[brackets$SeedDiff==x]))\nggplot(data=data.frame(seed.diffs,means,sds),aes(seed.diffs,means))+\ngeom_ribbon(aes(ymin=means-2*sds,ymax=means+2*sds),alpha=.5)+geom_point()+geom_line()+\ngeom_hline(yintercept=0,linetype=2)+\nlabs(x=\"Seed Difference\",y=\"Margin of Victory by Higher Seed\")Created by Pretty R at inside-R.org\n\nYou can see that the ribbon includes zero for all seed differences except 15. If we assume that the score differences are roughly normal, we can calculate the probability that the score difference will be greater than 0. The results are largely the same as before, but we see now that there are no 100% estimates. Also, the 50% win percentage for a seed difference of 10 now looks a little more reasonable, albeit still out of line with the rest.\nggplot(data=data.frame(seed.diffs,means,sds),aes(seed.diffs,1-pnorm(0,means,sds)))+\ngeom_point()+geom_line()+geom_hline(yintercept=0.5,linetype=2)+\nlabs(x=\"Seed Difference\",y=\"Probability of Higher Seed Winning Based on Margin of Victory\")Created by Pretty R at inside-R.org\n\nModel Win Percentage as a Function of\u0026nbsp; Seed DifferenceIt is always good to incorporate as much knowledge as possible into an analysis. In this case, we have information on other games besides the 16 versus 1 seed game which help us estimate the 16 versus 1 game. For example, it is reasonable to assume that the larger the difference in seed is, the more likely the higher seed will win. We can build a logistic regression model which looks at all of the outcomes of all of the games and predicts the probability of winning based on the difference in seed. When the two teams have the same seed, I enforced the probability of the higher seed winning to be 0.5 by making the intercept 0.\nIn the plot below, you can see that the logistic model predicts that the probability of winning increases throughout until reaching about 90% for the 16 versus 1. I also included a non-linear generalized additive model (GAM) model for comparison. The GAM believes that being a big favorite (16 vs 1 or 15 vs 2) gives an little boost in win probability. An advantage of modeling is that we can make predictions for match-ups that have never occurred (like a seed difference of 14). ggplot(data=brackets,aes(SeedDiff,HigherSeedWon))+\nstat_smooth(method=\"gam\",family=\"binomial\",se=F,formula=y~0+x,aes(colour=\"Logistic\"),size=1)+\nstat_smooth(method=\"gam\",family=\"binomial\",se=F,formula=y~s(x),aes(colour=\"GAM\"),size=1)+\ngeom_jitter(alpha=.15,position = position_jitter(height = .025,width=.25))+\nlabs(x=\"Seed Difference\",y=\"Game Won by Higher Seed\",colour=\"Model\")Created by Pretty R at inside-R.org\n\nModel Score Difference as a Function of\u0026nbsp; Seed DifferenceWe can also do the same thing with margin of victory. Here, I constrain the linear model to have an intercept of 0, meaning that two teams with the same seed should be evenly matched. Again, I included the GAM fit for comparison. The interpretations are similar to before, in that it seems that there is an increase in margin of victory for the heavily favored teams.\nggplot(data=brackets,aes(SeedDiff,HigherSeedScoreDiff))+\nstat_smooth(method=\"lm\",se=F,formula=y~0+x,aes(colour=\"Linear\"),size=1)+\nstat_smooth(method=\"gam\",se=F,formula=y~s(x),aes(colour=\"GAM\"),size=1)+\ngeom_jitter(alpha=.25,position = position_jitter(height = 0,width=.25))+\nlabs(x=\"Seed Difference\",y=\"Margin of Victory by Higher Seed\",colour=\"Model\")Created by Pretty R at inside-R.org\n\nFrom these models of margin of victory we can infer the probability of the higher seed winning (again, assuming normality).\nlibrary(gam)\nlm.seed=lm(HigherSeedScoreDiff~0+SeedDiff,data=brackets)\ngam.seed=gam(HigherSeedScoreDiff~s(SeedDiff),data=brackets)\n\u0026nbsp;\npred.lm.seed=predict(lm.seed,data.frame(SeedDiff=0:15),se.fit=TRUE)\npred.gam.seed=predict(gam.seed,data.frame(SeedDiff=0:15),se.fit=TRUE)\nse.lm=sqrt(mean(lm.seed$residuals^2))\nse.gam=sqrt(mean(gam.seed$residuals^2))\n\u0026nbsp;\ndf1=data.frame(SeedDiff=0:15,ProbLM=1-pnorm(0,pred.lm.seed$fit,sqrt(se.lm^2+pred.lm.seed$se.fit^2)),\nProbGAM=1-pnorm(0,pred.gam.seed$fit,sqrt(se.gam^2+pred.gam.seed$se.fit^2)))\nggplot(df1)+geom_hline(yintercept=0.5,linetype=2)+\ngeom_line(aes(SeedDiff,ProbLM,colour=\"Linear\"),size=1)+\ngeom_line(aes(SeedDiff,ProbGAM,colour=\"GAM\"),size=1)+\nlabs(x=\"Seed Difference\",y=\"Probability of Higher Seed Winning\",colour=\"Model\")Created by Pretty R at inside-R.org\n\nSummaryPutting all of the estimates together, you can easily spot the differences between the models. The two assumptions that just used the data between specific seeds look pretty similar. It looks like using score differential is a little more reasonable of the two. The two GAMs have a similar trend and so did the\u0026nbsp; linear and logistic models. If someone asks you what the probability that a 16 seed beats a 1 seed, you have at least 6 different answers.\nThis post highlights the many different ways someone can analyze the same data. Simply statistics talked a bit about this in a recent podcast. In this case, the differences are not huge, but there are noticeable changes. So the next time you read about an analysis that someone did, keep in mind all the decisions that they had to make and what type a sensitivity they would have on the results.\nlogit.seed=glm(HigherSeedWon~0+SeedDiff,data=brackets,family=binomial(logit))\nlogit.seed.gam=gam(HigherSeedWon~s(SeedDiff),data=brackets,family=binomial(logit))\n\u0026nbsp;\ndf2=data.frame(SeedDiff=0:15,\nProbLM=1-pnorm(0,pred.lm.seed$fit,sqrt(se.lm^2+pred.lm.seed$se.fit^2)),\nProbGAM=1-pnorm(0,pred.gam.seed$fit,sqrt(se.gam^2+pred.gam.seed$se.fit^2)),\nProbLogit=predict(logit.seed,data.frame(SeedDiff=0:15),type=\"response\"),\nProbLogitGAM=predict(logit.seed.gam,data.frame(SeedDiff=0:15),type=\"response\"))\ndf2=merge(df2,data.frame(SeedDiff=seed.diffs,ProbFreq=win.pct),all.x=T)\ndf2=merge(df2,data.frame(SeedDiff=seed.diffs,ProbScore=1-pnorm(0,means,sds)),all.x=T)\nggplot(df2,aes(SeedDiff))+geom_hline(yintercept=0.5,linetype=2)+\ngeom_line(aes(y=ProbLM,colour=\"Linear\"),size=1)+\ngeom_line(aes(y=ProbGAM,colour=\"GAM\"),size=1)+\ngeom_line(aes(y=ProbLogit,colour=\"Logistic\"),size=1)+\ngeom_line(aes(y=ProbLogitGAM,colour=\"Logistic GAM\"),size=1)+\ngeom_line(aes(y=ProbFreq,colour=\"Frequencies\"),size=1)+\ngeom_line(aes(y=ProbScore,colour=\"Score Diff\"),size=1)+\ngeom_point(aes(y=ProbFreq,colour=\"Frequencies\"),size=3)+\ngeom_point(aes(y=ProbScore,colour=\"Score Diff\"),size=3)+\nlabs(x=\"Seed Difference\",y=\"Probability of Higher Seed Winning\",colour=\"Model\")\n\u0026nbsp;\nggplot(df2)+geom_hline(yintercept=0.5,linetype=2)+\ngeom_point(aes(x=SeedDiff,y=ProbFreq,colour=\"Frequencies\"),size=1)Created by Pretty R at inside-R.org\n\nNote that the GAM functions did not have a way to easily restrict the win probability be equal to exactly 0.5 when the seed difference is 0. That is why you may notice the GAM model is a bit above 0.5 at 0.\r","date":1366502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1366502400,"objectID":"d7e1406db49803a0f757cc6f5e1bed12","permalink":"/2013/04/21/what-is-the-probability-of-a-16-seed-beating-a-1-seed/","publishdate":"2013-04-21T00:00:00Z","relpermalink":"/2013/04/21/what-is-the-probability-of-a-16-seed-beating-a-1-seed/","section":"post","summary":"Note: I started this post way back when the NCAA men's basketball tournament was going on, but didn't finish it until now. \nSince the NCAA Men's Basketball Tournament has moved to 64 teams, a 16 seed as never upset a 1 seed. You might be tempted to say that the probability of such an event must be 0 then. But we know better than that.\nIn this post, I am interested in looking at different ways of estimating how the odds of winning a game change as the difference between seeds increases.","tags":null,"title":"What Is the Probability of a 16 Seed Beating a 1 Seed?","type":"post"},{"authors":null,"categories":["firefox","productivity"],"content":"\rAs a grad student, I do lots of searches for research related to my own. When I am off campus, a lot of the relevant results are not open access. In that case, I have to log onto my school's library website and search for the journal or article there. It is quite a hassle. Luckily, I recently noticed that the website is predictably modified after I log into the library. I go to Ohio State University, and before and after logging in the websites will be http://www.jstor.org/... and http://www.jstor.org.proxy.lib.ohio-state.edu/..., for example. It doesn't matter which journal or website I am looking at. If OSU has access, it will show up with .proxy.lib.ohio-state.edu after the .com/.org/etc. So I was able to save myself a few steps by typing that in instead of going through the library's website. I should also be sure to note that typing this in brings you to an OSU log in screen. Only those with valid access can use this shortcut, but I assume other schools have the same setup.\nThis is still more busy work than I would like, so I searched for ways to automatically do this. Lifehacker has some address bar shortcuts, but nothing that quite works. I came across a\u0026nbsp; Firefox add-on called URL Swap that seems like it could do the trick. However, when setting it up, I happened to find this post on Stack Overflow. The second answer has exactly what I am looking for, changing .ExtraPart to .proxy.lib.ohio-state.edu. I created a bookmark on my bookmarks toolbar with the javascript:\njavascript:(function(){ var curloc = window.location.toString(); var newloc = curloc.substring(0, curloc.indexOf(\u0026#34;/\u0026#34;, 8)) + \u0026#34;.proxy.lib.ohio-state.edu\u0026#34; + curloc.substring(curloc.indexOf(\u0026#34;/\u0026#34;, 8)); location.href=newloc; })()\nand it works perfectly. This trick is so useful I wanted to share it to save others some time.\r","date":1366243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1366243200,"objectID":"f9ac35f92780be89db52160f266bc5ca","permalink":"/2013/04/18/easily-access-academic-journals-off-campus-with-a-firefox-bookmark/","publishdate":"2013-04-18T00:00:00Z","relpermalink":"/2013/04/18/easily-access-academic-journals-off-campus-with-a-firefox-bookmark/","section":"post","summary":"As a grad student, I do lots of searches for research related to my own. When I am off campus, a lot of the relevant results are not open access. In that case, I have to log onto my school's library website and search for the journal or article there. It is quite a hassle. Luckily, I recently noticed that the website is predictably modified after I log into the library.","tags":null,"title":"Easily Access Academic Journals Off Campus with a Firefox Bookmark","type":"post"},{"authors":null,"categories":["data","R"],"content":"\rA lot of times we are given a data set in Excel format and we want to run a quick analysis using R's functionality to look at advanced statistics or make better visualizations. There are packages for importing/exporting data from/to Excel, but I have found them to be hard to work with or only work with old versions of Excel (*.xls, not *.xlsx). So for a one time analysis, I usually save the file as a csv and import it into R.\nThis can be a little burdensome if you are trying to do something quick and creates a file that needs to be cleaned up later. An easier option is to copy and paste the data directly into R. This can be done by using \"clipboard\" as the file and specifying that it is tab delimited, since that is how Excel's clipboard stores the data.\nFor example, say you have a table in excel you want to copy into R. First, copy it in Excel.\n\nThen go into R and use this function.\nread.excel \u0026lt;- function(header=TRUE,...) {\nread.table(\"clipboard\",sep=\"\\t\",header=header,...)\n}\n\u0026nbsp;\ndat=read.excel()\nThis function specifies that you are reading data from the clipboard, that it is tab delimited, and that it has a header.\nSimilarly, you can copy from R to Excel using the same logic. Here I also make row.name=FALSE as default since I rarely have meaningful row names and they mess up the header alignment.\nwrite.excel \u0026lt;- function(x,row.names=FALSE,col.names=TRUE,...) {\nwrite.table(x,\"clipboard\",sep=\"\\t\",row.names=row.names,col.names=col.names,...)\n}\n\u0026nbsp;\nwrite.excel(dat)Created by Pretty R at inside-R.org\nThese functions can be added to you .RProfile so that they are always ready for a quick analysis!\nObviously, this technique does not encourage reproducible research. It is meant to be used for quick, ad hoc analysis and plotting; not something you would use for an analysis that needs to be done on a regular basis.\n\rComments\rJustin Tapp\rI\u0026#39;m using this function in R for Windows. When I view the data numerically, it\u0026#39;s fine. But when I go to plot the data I get nonsense. How to describe it...the window has 0 to 250 on the x-axis (regardless of my data series) and white boxes across the middle. \r\rMarek Sz\rI create similar functions and got few tips. For reading from excel following settings can be useful:\nna.strings = \u0026quot;\u0026quot; # to prevent replacing NA string to missing value\ncomment.char = \u0026quot;\u0026quot; # to not loose everything after # sign\nquote = \u0026quot;\u0026quot; # or \u0026#39; or \u0026quot; could mess with data\ncheck.names = FALSE # if you want column names as in excel (spaces, special characters, etc.). You need to use `column name` in R to reference such columns.\nFor writing na=\u0026quot;\u0026quot; replace missing values by empty string and not \u0026quot;NA\u0026quot; as on default.\nSecond thing is that you can increase size of clipboard by using e.g. \u0026quot;clipboard-10240\u0026quot; instead of \u0026quot;clipboard\u0026quot; (it\u0026#39;s a size in Kb, so it\u0026#39;s around 10Mb; see help for connection, section Clipboard) which allow to copy and paste larger tables.\r\rAnonymous\rrkward (http://rkward.sourceforge.net/) has a very nifty feature (Edit -\u0026gt; Paste Special...), that allows you to paste the copied data directly into your R source code, already formatted as a single string, vector or matrix. \r\rWilliam Yarberry\rExcellent article. Real people are always busy and this is just the kind of article that helps us all. I have been doing scan() but when you start entering a few thousand rows that way, it gets a bit slow. \r\rAnonymous\rscan() allows you to just paste...\ny \u0026lt;- scan()\r\rAnonymous\rThank you for these!!\n\r\rAnonymous\rThank you for the tip, this will help me a lot.\nAlso, it seems that, libreOffice also uses clipboard to store copied things. This function also works for libreOffice\r\rAnonymous\rI can\u0026#39;t resist: you could just use\nread.delim(\u0026quot;clipboard\u0026quot;)\n(The \u0026quot;clipboard\u0026quot; parameter is \u0026#39;doze only for the foreseeable future)\nFrom \u0026quot;?read.delim\u0026quot;\nread.delim(file, header = TRUE, sep = \u0026quot;\\t\u0026quot;, quote=\u0026quot;\\\u0026quot;\u0026quot;, dec=\u0026quot;.\u0026quot;,\nfill = TRUE, comment.char=\u0026quot;\u0026quot;, ...)\r\rAnonymous\rThank you very much Tony for your quick answer (on a Sunday afternoon!!). Ernesto\r\rTony Hirst\r@Ernesto It seems that on a Mac, you can use pbpaste ( http://stackoverflow.com/questions/9035674/r-function-to-copy-to-clipboard-on-mac-osx )\nread.clipboard.mac \u0026lt;- function(header=TRUE,...) {\nread.table(pipe(\u0026quot;pbpaste\u0026quot;),sep=\u0026quot;\\t\u0026quot;,header=header,...)\n}\n\r\rAnonymous\rThank you very much. Very useful. I work in both Windows and Mac Environments. The trick you show seems to work only in Windows. Any idea what to do in Mac? Thanks in advance,\nErnesto\r\r\r","date":1361664000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1361664000,"objectID":"db81abee01cbfbd6fffb5da1521351a3","permalink":"/2013/02/24/copying-data-from-excel-to-r-and-back/","publishdate":"2013-02-24T00:00:00Z","relpermalink":"/2013/02/24/copying-data-from-excel-to-r-and-back/","section":"post","summary":"A lot of times we are given a data set in Excel format and we want to run a quick analysis using R's functionality to look at advanced statistics or make better visualizations. There are packages for importing/exporting data from/to Excel, but I have found them to be hard to work with or only work with old versions of Excel (*.xls, not *.xlsx). So for a one time analysis, I usually save the file as a csv and import it into R.","tags":null,"title":"Copying Data from Excel to R and Back","type":"post"},{"authors":null,"categories":["statistics","ggplot","Visualization","Bayesian","optimization","R"],"content":"\rThe famous probabilist and statistician Persi Diaconis wrote an article not too long ago about the \"Markov chain Monte Carlo (MCMC) Revolution.\" The paper describes how we are able to solve a diverse set of problems with MCMC. The first example he gives is a text decryption problem solved with a simple Metropolis Hastings sampler.\nI was always stumped by those cryptograms in the newspaper and thought it would be pretty cool if I could crack them with statistics. So I decided to try it out on my own. The example Diaconis gives is fleshed out in more details by its original authors in its own article. The decryption I will be attempting is called substitution cipher, where each letter of the alphabet corresponds to another letter (possibly the same one). This is the rule that you must follow for the cryptogram game too. There are 26! = 4e26 possible mappings of the letters of the alphabet to one another. This seems like a hopeless problem to solve, but I will show you that a relatively simple solution can be found as long as your reference text is large enough and the text you are trying to decipher is also large enough.\nThe strategy is to use a reference text to create transition probabilities from each letter to the next. This can be stored in a 26 by 26 matrix, where the ith row and jth column is the probability of the jth letter, given the ith letter preceded it. For example, the given the previous letter was a Q, U almost always follows it, so the Q-row and U-column would be close to probability of 1. Assuming these one step transition probabilities are what matter, the likelihood of any mapping is the product of the transition probabilities observed.\nTo create a transition matrix, I downloaded War and Peace from Project Gutenberg. I looped through each letter and kept track of the number of number of times each letter followed the previous. I also kept track of a 27th character, which was anything that was not a letter -- for example periods, commas, spaces, etc. This lets me know which letters are more likely to start a word or end a word. Consecutive entries of these special characters are not considered.\nAfter I have these counts, I normalize by dividing by the row total. Before normalizing, I add 1 to each cell so that there are no probabilities of 0. This also corresponds to prior information of each transition being equally likely. I could have tried to give more informative prior information (like Q to U being likely), but it would have been difficult and probably inaccurate for the whole matrix.\nBelow is the code for creating the transition probability matrix. Note that I loop through each line and within each line I loop through each character.\nreference=readLines(\"warandpeace.txt\")\nreference=toupper(reference)\n\u0026nbsp;\ntrans.mat=matrix(0,27,27)\nrownames(trans.mat)=colnames(trans.mat)=c(toupper(letters),\"\")\nlastletter=\"\"\nfor (ln in 1:length(reference)) {\nif (ln %% 1000 ==0) {cat(\"Line\",ln,\"\\n\")}\nfor (pos in 1:nchar(reference[ln])) {\ncurletter=substring(reference[ln],pos,pos)\nif (curletter %in% toupper(letters)) {\ntrans.mat[rownames(trans.mat)==lastletter,\ncolnames(trans.mat)==curletter]=\ntrans.mat[rownames(trans.mat)==lastletter,\ncolnames(trans.mat)==curletter]+1\nlastletter=curletter\n} else {\nif (lastletter!=\"\") {\ntrans.mat[rownames(trans.mat)==lastletter,27]=\ntrans.mat[rownames(trans.mat)==lastletter,27]+1\nlastletter=\"\"\n}\n}\n}\ncurletter=\"\"\nif (lastletter!=\"\") {\ntrans.mat[rownames(trans.mat)==lastletter,27]=\ntrans.mat[rownames(trans.mat)==lastletter,27]+1\n}\nlastletter=\"\"\n}\n\u0026nbsp;\ntrans.prob.mat=sweep(trans.mat+1,1,rowSums(trans.mat+1),FUN=\"/\")Created by Pretty R at inside-R.org\nI like to visualize my data to make sure everything looks correct. Below is a plot of the transition matrix.\u0026nbsp; The blank space corresponds to non-letter character. A lot of insights can be garnered from this plot. The Q-U relationship pops out. T is the most likely letter to start a word. E is a popular letter to follow many others. Y is likely to end a word.\nggplot(melt(trans.prob.mat),aes(Var2,Var1))+geom_tile(aes(fill=value))+\nscale_fill_gradient(low=\"white\",high=\"black\",limits=c(0,1))+\nlabs(x=\"Probability of Second Letter\",y=\"Conditioning on First Letter\",fill=\"Prob\")+\nscale_y_discrete(limits = rev(levels(melt(trans.prob.mat)$Var1)))+\ncoord_equal()Created by Pretty R at inside-R.org\n\nThe desired solution will be the one that gives the highest likelihood. This is an NP-hard problem so the only way to find the solution is to try all 26! combinations of mappings, which is infeasible, and report the one with the highest likelihood.\nWith the MCMC approach you start with a random mapping of letters. Next you propose a new mapping by randomly switching 2 of the characters in the mapping. So if A mapped to G and L mapped to Z and you switched those two, A would map to Z and L would map to G. With this new mapping, you measure the likelihood and divide it by the likelihood of the previous mapping. If this ratio is greater than 1, then move to this new mapping. If the ratio is less than 1, meaning the new mapping is less likely, then move to this new mapping with a probability equal to the ratio. (Practically, this is done by drawing a random uniform number between 0 and 1 and moving to the proposed mapping if the uniform number is less than the ratio.) Repeat this process until you think you have found a solution.\nTo do this, I first wrote a few functions. One to decode the coded text based on the mapping. The other was to calculate the log likelihood of the decoded text.\ndecode \u0026lt;- function(mapping,coded) {\ncoded=toupper(coded)\ndecoded=coded\nfor (i in 1:nchar(coded)) {\nif (substring(coded,i,i) %in% toupper(letters)) {\nsubstring(decoded,i,i)=toupper(letters[mapping==substring(coded,i,i)])\n}\n}\ndecoded\n}\n\u0026nbsp;\n\u0026nbsp;\nlog.prob \u0026lt;- function(mapping,decoded) {\nlogprob=0\n\u0026nbsp;\nlastletter=\"\"\nfor (i in 1:nchar(decoded)) {\ncurletter=substring(decoded,i,i)\nif (curletter %in% toupper(letters)) {\nlogprob=logprob+log(trans.prob.mat[rownames(trans.mat)==lastletter,\ncolnames(trans.mat)==curletter])\nlastletter=curletter\n} else {\nif (lastletter!=\"\") {\nlogprob=logprob+log(trans.prob.mat[rownames(trans.mat)==lastletter,27])\nlastletter=\"\"\n}\n}\n}\n\u0026nbsp;\nif (lastletter!=\"\") {\nlogprob=logprob+log(trans.prob.mat[rownames(trans.mat)==lastletter,27])\nlastletter=\"\"\n}\nlogprob\n}Created by Pretty R at inside-R.org\nTo show how this works, I will use the same Shakespeare text that is used in the MCMC Revolution paper. I let it run until it tries out 2000 different mappings (not the same as 2000 iterations, because rejected proposals are not counted). Along the way I am keeping track of the most likely mapping visited, and that will be the final estimate. It should be noted that I am only considering the mapping of letters and it is assumed that the special characters are known. For example spaces and periods are assumed to be the same.\ncorrectTxt=\"ENTER HAMLET HAM TO BE OR NOT TO BE THAT IS THE QUESTION WHETHER TIS NOBLER IN THE MIND TO SUFFER THE SLINGS AND ARROWS OF OUTRAGEOUS FORTUNE OR TO TAKE ARMS AGAINST A SEA OF TROUBLES AND BY OPPOSING END\"\ncoded=decode(sample(toupper(letters)),correctTxt) # randomly scramble the text\n\u0026nbsp;\nmapping=sample(toupper(letters)) # initialize a random mapping\ni=1\niters=2000\ncur.decode=decode(mapping,coded)\ncur.loglike=log.prob(mapping,cur.decode)\nmax.loglike=cur.loglike\nmax.decode=cur.decode\nwhile (i\u0026lt;=iters) {\nproposal=sample(1:26,2) # select 2 letters to switch\nprop.mapping=mapping\nprop.mapping[proposal[1]]=mapping[proposal[2]]\nprop.mapping[proposal[2]]=mapping[proposal[1]]\n\u0026nbsp;\nprop.decode=decode(prop.mapping,coded)\nprop.loglike=log.prob(prop.mapping,prop.decode)\n\u0026nbsp;\nif (runif(1)\u0026lt;exp(prop.loglike-cur.loglike)) {\nmapping=prop.mapping\ncur.decode=prop.decode\ncur.loglike=prop.loglike\n\u0026nbsp;\nif (cur.loglike\u0026gt;max.loglike) {\nmax.loglike=cur.loglike\nmax.decode=cur.decode\n}\n\u0026nbsp;\ncat(i,cur.decode,\"\\n\")\ni=i+1\n}\n}Created by Pretty R at inside-R.org\nThe output of this example is below. You can see that it comes close but it doesn't quite find the correct mapping. I attribute this to the fact that the text I was trying to decode only had 203 characters. In the paper mentioned above, they decoded the whole play. They further say that their methods work just as well if you randomly sample a text snippet 2000 characters long. Obviously my example had well less than this. This is also a problem in trying to solve a cryptogram because those are even smaller than the Hamlet example I used. So unfortunately this simple method cannot be used for that. (Note that I ran the MCMC chain a several times, using different random starting points, until it came reasonably close to the solution. This is something that the authors also suggested doing.)\n\nI want to also note that I originally used Alice and Wonderland as the reference text. It had more trouble decoding since this book is much smaller than War and Peace, and therefore the transition probabilities were not as good.\nThe MCMC method is a greedy approach in that you are moving to any mapping that increases the likelihood. Greedy methods are not optimal in general because they can get stuck at local modes, especially in high dimensional problems. MCMC avoids this be moving to less likely mappings with some probability, which ensures that it will find the correct solution with enough time.\rComments\rVivek\rAndrew,\nThis work is cool!\nI wrote my own substitution cipher decoder using the algorithm described in Stephen Connor\u0026#39;s dissertation as the basis, but I occasionally glanced at your work as well.\nMy code doesn\u0026#39;t work as awesomely as yours, and it is also much slower (possibly because I use 76 versus the 27 characters, reachChar versus readLine), and I use different functions.\nBut I cited your work and copied your idea for the melt matrix, which is ingenious.\nThanks so much!\nVivek\r\rAndrew Landgraf\rThanks, Fernando. I knew I could speed it up by treating it as a vector of letters (or integers), but didn\u0026#39;t take the time to figure it out.\r\rFernando\rNice work, but the code to get the transition matrix is VERY slow. I manage to make it faster doing the following:\n1- get the vector of words (using scan() for example).\n2- For each word, use strsplit(word, NULL)[[1]] and iterate\nover the characters to populate the matrix.\r\rAndrew\rI have had that issue working on different computers. I think older versions of reshape2 use X# and newer versions use Var#. Or maybe it\u0026#39;s a difference between using the package reshape and reshape2?\r\rPablik\rGreat work! Sometimes I use Persi\u0026#39;s example for teaching MCMC,\nthere are some old matlab code from the original consultancy work,\nbut I never tryed to replicate the example.\nI got an error at:\nggplot(melt(trans.prob.mat),aes(Var2,Var1))+...\nthe melt() function uses X1 and X2:\n\u0026gt; tmp \u0026lt;- melt(trans.prob.mat)\n\u0026gt; head(tmp)\nX1 X2 value\n1 A A 3.893588e-05\n2 B A 9.252956e-02\n...\nCheers,\nPablo\n\r\r\r","date":1358899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1358899200,"objectID":"affd1969cff29f6371d1d5e539575c0d","permalink":"/2013/01/23/text-decryption-using-mcmc/","publishdate":"2013-01-23T00:00:00Z","relpermalink":"/2013/01/23/text-decryption-using-mcmc/","section":"post","summary":"The famous probabilist and statistician Persi Diaconis wrote an article not too long ago about the \"Markov chain Monte Carlo (MCMC) Revolution.\" The paper describes how we are able to solve a diverse set of problems with MCMC. The first example he gives is a text decryption problem solved with a simple Metropolis Hastings sampler.\nI was always stumped by those cryptograms in the newspaper and thought it would be pretty cool if I could crack them with statistics.","tags":null,"title":"Text Decryption Using MCMC","type":"post"},{"authors":null,"categories":["Deep Learning","ggplot","Visualization","machine learning","collaborative filtering","R"],"content":"\rRestricted Boltzmann Machines (RBMs) are an unsupervised learning method (like principal components). An RBM is a probabilistic and undirected graphical model. They are becoming more popular in machine learning due to recent success in training them with contrastive divergence. They have been proven useful in collaborative filtering, being one of the most successful methods in the Netflix challenge (paper). Furthermore, they have been tantamount to training deep learning models, which appear to be the best current models for image and digit recognition.\nI do not want to go into too much detail, but I would like to give a high level overview of RBMs. Edwin Chen has an introduction that is much better. The usual version of an RBM is a probabilistic model for binary vectors. An image can be represented as a binary vector if each pixel that is dark enough is represented as a 1 and the non-dark pixels are 0's. In addition to the visible binary vector, it is assumed that there is a hidden binary vector, and each element of the hidden unit is connected to each unit of the visible unit by symmetric weights. The weights are represented by the matrix W, where the i,jth component is the weight between hidden unit i and visible unit j. It is important that there are no connections between hidden units or between visible units. The probability that visible unit j is 1 is the inverse logistic function of the sum of the weights connected to visible unit j, in which the hidden units are 1. In math notation (where σ is the inverse logistic, or sigmoid, function):\n\nPr(vj=1|h,W)=σ(∑ihiWi,j).\nThe weights are symmetric, so\n\nPr(hi=1|v,W)=σ(∑jvjWi,j).As you can see, the model is defined by its conditional probabilities. The task is to find the weight matrix W that best explains the visible data for a given number of hidden units.\nI have been taking Geoff Hinton's coursera course on neural networks, which I would recommend to anyone interested. One of the programming assignments was to fill in code to write an RBM in Matlab/Octave. I have since tried to find a version for R, but have not had any luck, so I decided to translate the code myself. Below is the code to calculate the weight matrix. It is fairly simple and I only use contrastive divergence 1. The input data is p×n instead of the usual transpose.\n# rbm_w is a matrix of size \u0026lt;number of hidden units\u0026gt; by \u0026lt;number of visible units\u0026gt;\n# visible_state is matrix of size \u0026lt;number of visible units\u0026gt; by \u0026lt;number of data cases\u0026gt;\n# hidden_state is a binary matrix of size \u0026lt;number of hidden units\u0026gt; by \u0026lt;number of data cases\u0026gt;\n\u0026nbsp;\nvisible_state_to_hidden_probabilities \u0026lt;- function(rbm_w, visible_state) {\n1/(1+exp(-rbm_w %*% visible_state))\n}\n\u0026nbsp;\nhidden_state_to_visible_probabilities \u0026lt;- function(rbm_w, hidden_state) {\n1/(1+exp(-t(rbm_w) %*% hidden_state))\n}\n\u0026nbsp;\nconfiguration_goodness \u0026lt;- function(rbm_w, visible_state, hidden_state) {\nout=0\nfor (i in 1:dim(visible_state)[2]) {\nout=out+t(hidden_state[,i]) %*% rbm_w %*% visible_state[,i]\n}\nout/dim(visible_state)[2]\n}\n\u0026nbsp;\nconfiguration_goodness_gradient \u0026lt;- function(visible_state, hidden_state) {\nhidden_state %*% t(visible_state)/dim(visible_state)[2]\n}\n\u0026nbsp;\nsample_bernoulli \u0026lt;- function(mat) {\ndims=dim(mat)\nmatrix(rbinom(prod(dims),size=1,prob=c(mat)),dims[1],dims[2])\n}\n\u0026nbsp;\ncd1 \u0026lt;- function(rbm_w, visible_data) {\nvisible_data = sample_bernoulli(visible_data)\nH0=sample_bernoulli(visible_state_to_hidden_probabilities(rbm_w, visible_data))\nvh0=configuration_goodness_gradient(visible_data, H0)\nV1=sample_bernoulli(hidden_state_to_visible_probabilities(rbm_w, H0))\nH1=visible_state_to_hidden_probabilities(rbm_w, V1)\nvh1=configuration_goodness_gradient(V1, H1)\nvh0-vh1\n}\n\u0026nbsp;\nrbm \u0026lt;- function(num_hidden, training_data, learning_rate, n_iterations, mini_batch_size=100, momentum=0.9, quiet=FALSE) {\n# This trains a model that's defined by a single matrix of weights.\n# \u0026lt;num_hidden\u0026gt; is the number of hidden units\n# cd1 is a function that takes parameters \u0026lt;model\u0026gt; and \u0026lt;data\u0026gt; and returns the gradient (or approximate gradient in the case of CD-1) of the function that we're maximizing. Note the contrast with the loss function that we saw in PA3, which we were minimizing. The returned gradient is an array of the same shape as the provided \u0026lt;model\u0026gt; parameter.\n# This uses mini-batches no weight decay and no early stopping.\n# This returns the matrix of weights of the trained model.\nn=dim(training_data)[2]\np=dim(training_data)[1]\nif (n %% mini_batch_size != 0) {\nstop(\"the number of test cases must be divisable by the mini_batch_size\")\n}\nmodel = (matrix(runif(num_hidden*p),num_hidden,p) * 2 - 1) * 0.1\nmomentum_speed = matrix(0,num_hidden,p)\n\u0026nbsp;\nstart_of_next_mini_batch = 1;\nfor (iteration_number in 1:n_iterations) {\nif (!quiet) {cat(\"Iter\",iteration_number,\"\\n\")}\nmini_batch = training_data[, start_of_next_mini_batch:(start_of_next_mini_batch + mini_batch_size - 1)]\nstart_of_next_mini_batch = (start_of_next_mini_batch + mini_batch_size) %% n\ngradient = cd1(model, mini_batch)\nmomentum_speed = momentum * momentum_speed + gradient\nmodel = model + momentum_speed * learning_rate\n}\nreturn(model)\n}Created by Pretty R at inside-R.org\nI loaded the hand written digit data that was given in the class. To train the RBM, use the syntax below.\nweights=rbm(num_hidden=30, training_data=train, learning_rate=.09, n_iterations=5000,\nmini_batch_size=100, momentum=0.9)Created by Pretty R at inside-R.org\nAfter training the weights, I can visualize them. Below is a plot of strength of the weights going to each pixel. Each facet displays the weights going to/coming from one of the hidden units. I trained 30 units so that it would be easy to show them all on one plot. You can see that most of the hidden units correspond strongly to one digit or another. I think this means it is finding the joint structure of all of the pixels and that pixels representing those numbers are darkened together often.\nlibrary(ggplot2)\nlibrary(reshape2)\nmw=melt(weights); mw$Var3=floor((mw$Var2-1)/16)+1; mw$Var2=(mw$Var2-1)%%16 + 1; mw$Var3=17-mw$Var3;\nggplot(data=mw)+geom_tile(aes(Var2,Var3,fill=value))+facet_wrap(~Var1,nrow=5)+\nscale_fill_continuous(low='white',high='black')+coord_equal()+\nlabs(x=NULL,y=NULL,title=\"Visualization of Weights\")+\ntheme(legend.position=\"none\")Created by Pretty R at inside-R.org\n\rComments\rAndrew Landgraf\rThere are no biases in this implementation. You could make v_1=1, but there still wouldn\u0026#39;t be biases in the hidden units.\r\rsidharth\rwhere are the biases ? have you included them in the weights ?\r\rAndrew Landgraf\rThe weights are randomly initialized, so any 2 runs will give different results. However, if your figure looks similar to mine, in that it looks like the weights represent numbers, I would say it is working correctly.\r\rXiaolin Gui\rThank you for sharing your code.I found the output figure had different with yours when I do the experiment. I have load the MNIST image data successfully. And the data\u0026#39;s format is 784 ×60000. The pixels have been binarization. \u0026gt;= 127——\u0026gt;1.\nI hope you give me some advice.\nemail:guixiaolinde@gmail.com \r\rAnonymous\rI have managed to import the training data (from the Coursera website) into R:\nlibrary(R.matlab)\ndat = readMat(\u0026quot;data_set.mat\u0026quot;)\n......\nweights=rbm(num_hidden=30, training_data= dat$data[,,1]$training[[1]], learning_rate=.09, n_iterations=5000,\nmini_batch_size=100, momentum=0.9)\n\r\rKent Johnson\rR code for assignment 4 was posted here:\nhttps://class.coursera.org/neuralnets-2012-001/forum/thread?thread_id=87\u0026amp;post_id=5105\nI used it as the basis for a work project.\r\rZachary Mayer\rI believe there\u0026#39;s also a kaggle competition using that data right now: http://www.kaggle.com/c/digit-recognizer\r\rAndrew\rI don\u0026#39;t want to post it here because of possible copyright violations, but you can download it in Octave format from the Coursera site (in programming assignment 4) or you can find it at this webpage: http://yann.lecun.com/exdb/mnist/\r\rAnonymous\rDo you have a sample of training data?\r\r\r","date":1358121600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1358121600,"objectID":"ea1fff6398cc3bdeb46f89f4d45a50fc","permalink":"/2013/01/14/restricted-boltzmann-machines-in-r/","publishdate":"2013-01-14T00:00:00Z","relpermalink":"/2013/01/14/restricted-boltzmann-machines-in-r/","section":"post","summary":"Restricted Boltzmann Machines (RBMs) are an unsupervised learning method (like principal components). An RBM is a probabilistic and undirected graphical model. They are becoming more popular in machine learning due to recent success in training them with contrastive divergence. They have been proven useful in collaborative filtering, being one of the most successful methods in the Netflix challenge (paper). Furthermore, they have been tantamount to training deep learning models, which appear to be the best current models for image and digit recognition.","tags":null,"title":"Restricted Boltzmann Machines in R","type":"post"},{"authors":null,"categories":["statistics","Hall of Fame","Visualization","baseball","Factor Analysis","R"],"content":"\r Factor Analysis of Baseball's Hall of Fame Voters body, td { font-family: sans-serif; background-color: white; font-size: 12px; margin: 8px; } tt, code, pre { font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace; } h1 { font-size:2.2em; } h2 { font-size:1.8em; } h3 { font-size:1.4em; } h4 { font-size:1.0em; } h5 { font-size:0.9em; } h6 { font-size:0.8em; } a:visited { color: rgb(50%, 0%, 50%); } pre { margin-top: 0; max-width: 95%; border: 1px solid #ccc; white-space: pre-wrap; } pre code { display: block; padding: 0.5em; } code.r, code.cpp { background-color: #F8F8F8; } table, td, th { border: none; } blockquote { color:#666666; margin:0; padding-left: 1em; border-left: 0.5em #EEE solid; } hr { height: 0px; border-bottom: none; border-top-width: thin; border-top-style: dotted; border-top-color: #999999; } @media print { * { background: transparent !important; color: black !important; filter:none !important; -ms-filter: none !important; } body { font-size:12pt; max-width:100%; } a, a:visited { text-decoration: underline; } hr { visibility: hidden; page-break-before: always; } pre, blockquote { padding-right: 1em; page-break-inside: avoid; } tr, img { page-break-inside: avoid; } img { max-width: 100% !important; } @page :left { margin: 15mm 20mm 15mm 10mm; } @page :right { margin: 15mm 10mm 15mm 20mm; } p, h2, h3 { orphans: 3; widows: 3; } h2, h3 { page-break-after: avoid; } }  pre .operator, pre .paren { color: rgb(104, 118, 135) } pre .literal { color: rgb(88, 72, 246) } pre .number { color: rgb(0, 0, 205); } pre .comment { color: rgb(76, 136, 107); } pre .keyword { color: rgb(0, 0, 255); } pre .identifier { color: rgb(0, 0, 0); } pre .string { color: rgb(3, 106, 7); }  var hljs=new function(){function m(p){return p.replace(/\u0026/gm,\"\u0026amp;\").replace(/\"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event==\"start\"){z+=t(v.node);s.push(v.node)}else{if(v.event==\"stop\"){var p,r=s.length;do{r--;p=s[r];z+=(\"\")}while(p!=v.node);s.splice(r,1);while(r'+M[0]+\"\"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL\u0026\u0026e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'':\"\";if(M.rB){y+=L;M.buffer=\"\"}else{if(M.eB){y+=m(r)+L;M.buffer=\"\"}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?\"\":\"\";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L1){O=D[D.length-2].cN?\"\":\"\";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer=\"\";if(r.starts){I(r.starts,\"\")}return R.rE}if(w(M,R)){throw\"Illegal\"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y=\"\";try{var s,u=0;E.dM.buffer=\"\";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length1){throw\"Illegal\"}return{r:A,keyword_count:x,value:y}}catch(H){if(H==\"Illegal\"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.rr.keyword_count+r.r){r=s}if(s.keyword_count+s.rp.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((]+|\\t)+)/gm,function(t,w,v,u){return w.replace(/\\t/g,q)})}if(p){r=r.replace(/\\n/g,\"\n\")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement(\"pre\");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match(\"(\\\\s|^)(language-)?\"+v+\"(\\\\s|$)\")){u=u?(u+\" \"+v):v}if(/MSIE [678]/.test(navigator.userAgent)\u0026\u0026t.tagName==\"CODE\"\u0026\u0026t.parentNode.tagName==\"PRE\"){s=t.parentNode;var p=document.createElement(\"div\");p.innerHTML=\"\"+y.value+\"\";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName(\"pre\");for(var p=0;p|=||=||=|\\\\?|\\\\[|\\\\{|\\\\(|\\\\^|\\\\^=|\\\\||\\\\|=|\\\\|\\\\||~\";this.ER=\"(?![\\\\s\\\\S])\";this.BE={b:\"\\\\\\\\.\",r:0};this.ASM={cN:\"string\",b:\"'\",e:\"'\",i:\"\\\\n\",c:[this.BE],r:0};this.QSM={cN:\"string\",b:'\"',e:'\"',i:\"\\\\n\",c:[this.BE],r:0};this.CLCM={cN:\"comment\",b:\"//\",e:\"$\"};this.CBLCLM={cN:\"comment\",b:\"/\\\\*\",e:\"\\\\*/\"};this.HCM={cN:\"comment\",b:\"#\",e:\"$\"};this.NM={cN:\"number\",b:this.NR,r:0};this.CNM={cN:\"number\",b:this.CNR,r:0};this.BNM={cN:\"number\",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{\"false\":1,\"int\":1,\"float\":1,\"while\":1,\"private\":1,\"char\":1,\"catch\":1,\"export\":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,\"const\":1,struct:1,\"for\":1,static_cast:2,union:1,namespace:1,unsigned:1,\"long\":1,\"throw\":1,\"volatile\":2,\"static\":1,\"protected\":1,bool:1,template:1,mutable:1,\"if\":1,\"public\":1,friend:2,\"do\":1,\"return\":1,\"goto\":1,auto:1,\"void\":2,\"enum\":1,\"else\":1,\"break\":1,\"new\":1,extern:1,using:1,\"true\":1,\"class\":1,asm:1,\"case\":1,typeid:1,\"short\":1,reinterpret_cast:2,\"default\":1,\"double\":1,register:1,explicit:1,signed:1,typename:1,\"try\":1,\"this\":1,\"switch\":1,\"continue\":1,wchar_t:1,inline:1,\"delete\":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:\"\",k:a,r:10,c:[\"self\"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:\"number\",b:\"\\\\b0[xX][0-9a-fA-F]+[Li]?\\\\b\",e:hljs.IMMEDIATE_RE,r:0},{cN:\"number\",b:\"\\\\b\\\\d+(?:[eE][+\\\\-]?\\\\d*)?L\\\\b\",e:hljs.IMMEDIATE_RE,r:0},{cN:\"number\",b:\"\\\\b\\\\d+\\\\.(?!\\\\d)(?:i\\\\b)?\",e:hljs.IMMEDIATE_RE,r:1},{cN:\"number\",b:\"\\\\b\\\\d+(?:\\\\.\\\\d*)?(?:[eE][+\\\\-]?\\\\d*)?i?\\\\b\",e:hljs.IMMEDIATE_RE,r:0},{cN:\"number\",b:\"\\\\.\\\\d+(?:[eE][+\\\\-]?\\\\d*)?i?\\\\b\",e:hljs.IMMEDIATE_RE,r:1},{cN:\"keyword\",b:\"(?:tryCatch|library|setGeneric|setGroupGeneric)\\\\b\",e:hljs.IMMEDIATE_RE,r:10},{cN:\"keyword\",b:\"\\\\.\\\\.\\\\.\",e:hljs.IMMEDIATE_RE,r:10},{cN:\"keyword\",b:\"\\\\.\\\\.\\\\d+(?![\\\\w.])\",e:hljs.IMMEDIATE_RE,r:10},{cN:\"keyword\",b:\"\\\\b(?:function)\",e:hljs.IMMEDIATE_RE,r:2},{cN:\"keyword\",b:\"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\\\b\",e:hljs.IMMEDIATE_RE,r:1},{cN:\"literal\",b:\"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\\\b\",e:hljs.IMMEDIATE_RE,r:10},{cN:\"literal\",b:\"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\\\b\",e:hljs.IMMEDIATE_RE,r:1},{cN:\"identifier\",b:\"[a-zA-Z.][a-zA-Z0-9._]*\\\\b\",e:hljs.IMMEDIATE_RE,r:0},{cN:\"operator\",b:\"|=||Recently, Nate Silver wrote a post which analyzed how voters who voted for and against Barry Bonds for Baseball's Hall of Fame differed. Not surprisingly, those who voted for Bonds were more likely to vote for other suspected steroids users (like Roger Clemens). This got me thinking that this would make an interesting case study for factor analysis to see if there are latent factors that drive hall of fame voting.\nThe Twitter user @leokitty has kept track of all the known ballots of the voters in a spreadsheet. The spreadsheet is a matrix that has one row for each voter and one column for each player being voted upon. (Players need 75% of the vote to make it to the hall of fame.) I removed all players that had no votes and all voters that had given a partial ballot.\n(This matrix either has a 1 or a 0 in each entry, corresponding to whether a voter voted for the player or not. Note that this kind of matrix is similar to the data that is analyzed in information retrieval. I will be decomposing the (centered) matrix using singular value decomposition to run the factor analysis. This is the same technique used for latent semantic indexing in information retrieval.)\nStarting with the analysis, there is a drop off of the variance after the first 2 factors, which means it might make sense to look only at the first 2 (which is good because I was only planning on doing so).\nvotes = read.csv(\"HOF votes.csv\", row.names = 1, header = TRUE)\npca = princomp(votes)\nscreeplot(pca, type = \"l\", main = \"Scree Plot\")\n\nLooking at the loadings, it appears that the first principal component corresponds strongly to steroid users, which Bonds and Clemens having large negative values and other suspected steroid users being on the negative end. The players on the positive end have no steroid suspicions.\ndotchart(sort(pca$loadings[, 1]), main = \"First Principal Component\")\nThe second component isn't as easy to decipher. The players at the negative end seem to players that are preferred by analytically minded analysts (think Moneyball). Raines, Trammell, and Martinez have more support among this group of voters. Morris, however, has less support among these voters and he isn't that far separated from them.\nThere also may be some secondary steroid association in the component as well separating players who have proof of steroid use versus those which have no proof but “look like” they took steroids. For example, there is no hard evidence that Bagwell or Piazza took steroids, but they were very muscular and hit a lot of home runs, so they are believed to have taken steroids. There is some sort of evidence the top five players of this component did take steroids.\ndotchart(sort(pca$loadings[, 2]), main = \"Second Principal Component\")\n\nProjecting the votes onto two dimensions, we can look at how the voters for Bonds and Clemens split up. You can see there is a strong horizontal split between the voters for and against Bonds/Clemens. There are also 3 voters that voted for Bonds, but not Clemens.\nggplot(data.frame(cbind(pca$scores[, 1:2], votes))) + geom_point(aes(Comp.1, Comp.2, colour = as.factor(Barry.Bonds), shape = as.factor(Roger.Clemens)), size = 4) + coord_equal() + labs(colour = \"Bonds\", shape = \"Clemens\")\n\nSimilarly, I can look at how the voters split up on the issue of steroids by looking at both Bonds and Bagwell. The voters in the upper left do not care about steroid use, but believe that Bagwell wasn't good enough to make it to the hall of fame. The voters in the lower right do care about steroid use, but believe that Bagwell was innocent of any wrongdoing.\nggplot(data.frame(cbind(pca$scores[, 1:2], votes))) + geom_point(aes(Comp.1, Comp.2, colour = as.factor(paste(Roger.Clemens, \"/\", Jeff.Bagwell))), size = 4) + geom_hline(aes(0), size = 0.2) + geom_vline(aes(0), size = 0.2) + coord_equal() + labs(colour = \"Bonds / Bagwell\")\n\nWe can also look at a similar plot with Schilling instead of Bagwell. The separation here appears to be stronger.\nggplot(data.frame(cbind(pca$scores[, 1:2], votes))) + geom_point(aes(Comp.1, Comp.2, colour = as.factor(paste(Barry.Bonds, \"/\", Curt.Schilling))), size = 4) + geom_hline(aes(0), size = 0.2) + geom_vline(aes(0), size = 0.2) + coord_equal() + labs(colour = \"Bonds / Schilling\")\n\nFinally, we can look at a biplot (using code from here).\nPCbiplot \u0026lt;- function(PC = fit, x = \"PC1\", y = \"PC2\") {\n# PC being a prcomp object\nlibrary(grid)\ndata \u0026lt;- data.frame(obsnames = row.names(PC$x), PC$x)\nplot \u0026lt;- ggplot(data, aes_string(x = x, y = y)) + geom_text(alpha = 0.4, size = 3, aes(label = obsnames))\nplot \u0026lt;- plot + geom_hline(aes(0), size = 0.2) + geom_vline(aes(0), size = 0.2)\ndatapc \u0026lt;- data.frame(varnames = rownames(PC$rotation), PC$rotation)\nmult \u0026lt;- min((max(data[, y]) - min(data[, y])/(max(datapc[, y]) - min(datapc[, y]))), (max(data[, x]) - min(data[, x])/(max(datapc[, x]) - min(datapc[, x]))))\ndatapc \u0026lt;- transform(datapc, v1 = 0.7 * mult * (get(x)), v2 = 0.7 * mult * (get(y)))\nplot \u0026lt;- plot + coord_equal() + geom_text(data = datapc, aes(x = v1, y = v2, label = varnames), size = 5, vjust = 1, color = \"red\")\nplot \u0026lt;- plot + geom_segment(data = datapc, aes(x = 0, y = 0, xend = v1, yend = v2), arrow = arrow(length = unit(0.2, \"cm\")), alpha = 0.75, color = \"red\")\nplot\n}\nfit \u0026lt;- prcomp(votes, scale = F)\nPCbiplot(fit)\nI could have also attempted to rotate the factors to make them more interpretable, but they appeared to have easy interpretation as is. Since we were looking at 2-d plots, rotation would not have made a difference in interpreting the plots. It is also common to use a likelihood approach to estimate factors. I chose to use the principal component method because the data are definitely not normal (being 0's and 1's).\r","date":1357689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1357689600,"objectID":"084732c2e4c6330bb3dcec5e5a96e17d","permalink":"/2013/01/09/factor-analysis-of-baseballs-hall-of-fame-voters/","publishdate":"2013-01-09T00:00:00Z","relpermalink":"/2013/01/09/factor-analysis-of-baseballs-hall-of-fame-voters/","section":"post","summary":"Factor Analysis of Baseball's Hall of Fame Voters body, td { font-family: sans-serif; background-color: white; font-size: 12px; margin: 8px; } tt, code, pre { font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace; } h1 { font-size:2.2em; } h2 { font-size:1.8em; } h3 { font-size:1.4em; } h4 { font-size:1.0em; } h5 { font-size:0.9em; } h6 { font-size:0.8em; } a:visited { color: rgb(50%, 0%, 50%); } pre { margin-top: 0; max-width: 95%; border: 1px solid #ccc; white-space: pre-wrap; } pre code { display: block; padding: 0.","tags":null,"title":"Factor Analysis of Baseball's Hall of Fame Voters","type":"post"},{"authors":null,"categories":["Election","ggplot","Visualization","Polls","R"],"content":"\rWith the election nearly upon us, I wanted to share an easy way I just found to download polling data and graph a few with ggplot2. dlinzer at github created a function to download poll data from the Huffington Post's Pollster API.\nThe default is to download national tracking polls from the presidential election. After sourcing the function, I load the required packages, download the data, and make the plot.\nlibrary(XML)\nlibrary(reshape)\nlibrary(ggplot2); theme_set(theme_bw())\n\u0026nbsp;\ndat \u0026lt;- pollstR(pages=20)\nggplot(dat,aes(end.date,Obama/(Obama+Romney)))+geom_point(alpha=.5)+geom_smooth(aes(weight=sqrt(N)))+geom_hline(aes(yintercept=0.5),lty=2,size=1)+\nlabs(title=\"Proportion of Vote for Obama\",x=\"Last Date of Poll\",y=NULL)Created by Pretty R at inside-R.org\nI have used transparency so that you can see when there are many polls on top of each other. You can see that Obama's lead decreased substantially after the first debate but has crawled back up since then. Of course, I am treating all polls as equal (although I am weighting by sample size) when the truth is that some polls are better than others and some are biased.\nTo have some more fun, I will show what some of the data from swing states look like. The code below loops through the swing states and downloads the polls. Then it plots the polls for each state in different facets.\nswing.states=c(\"ohio\",\"florida\",\"virginia\",\"colorado\",\"nevada\",\"north-carolina\")\nfor (s in swing.states) {\nprint(s)\ndat.state \u0026lt;- pollstR(chart=paste(\"2012-\",s,\"-president-romney-vs-obama\",sep=\"\"),pages=\"all\")\ndat.state=subset(dat.state,select=c(\"id\",\"pollster\",\"start.date\",\"end.date\",\"method\",\"N\",\"Obama\",\"Romney\"))\ndat.state$State=s\n\u0026nbsp;\nif (s==\"ohio\") {\ndat=dat.state\n} else {\ndat=rbind(dat,dat.state)\n}\n}\n\u0026nbsp;\nlibrary(lubridate)\ndat$end.date=ymd(as.character(dat$end.date))\nggplot(dat,aes(end.date,Obama/(Obama+Romney)))+geom_point(alpha=.5)+geom_smooth(aes(weight=sqrt(N)))+geom_hline(aes(yintercept=0.5),lty=2,size=1)+\nlabs(title=\"Proportion of Vote for Obama\",x=\"Last Date of Poll\",y=NULL)+facet_wrap(~State)+xlim(c(mdy(\"8/1/2012\"),mdy(\"11/6/2012\")))Created by Pretty R at inside-R.org\nUnfortunately the x-axis didn't show up very well, but it starts at August 1. There have been quite a few polls in Ohio and Florida, haven't there? The state polls did not have nearly the same shift that the national poll did in reaction to the first debate. The state with the largest bump is Colorado, where the debate was held.\n\nBy just looking at the tracking polls, I think you would make the same conclusions that Nate Silver has with his fancy model. Ohio, Virginia, Nevada, and Colorado favor Obama. North Carolina favors Romney and Florida just barely tips toward Romney as well.\nFinally, here are just the smoothed running means, all on one plot. You can see that There was also a first debate effect in Ohio.\nggplot(dat,aes(end.date,Obama/(Obama+Romney)))+geom_smooth(aes(colour=State,weight=sqrt(N)),se=FALSE,size=2)+geom_hline(aes(yintercept=0.5),lty=2,size=1)+\nlabs(title=\"Proportion of Vote for Obama\",x=\"Last Date of Poll\",y=NULL)+xlim(c(mdy(\"8/1/2012\"),mdy(\"11/6/2012\")))Created by Pretty R at inside-R.org\n\rComments\rAndrew Clark\rInteresting work. Thanks for the tip on the API/github. I have had a look at the GOP campaign - surely one of the strangest in history\nhttp://wp.me/p17axt-jK\r\rAndrew\rThanks for the tip. To add more information to the national tracker plot, you can make the of the points relative to the number polled. Also facet it by the type of poll. Like this:\nggplot(dat,aes(end.date,Obama/(Obama+Romney)))+\ngeom_point(aes(size=sqrt(N)),alpha=.5)+\ngeom_smooth(aes(weight=sqrt(N)))+\ngeom_hline(aes(yintercept=0.5),lty=2,size=1)+\nlabs(title=\u0026quot;Proportion of Vote for Obama\u0026quot;,x=\u0026quot;Last Date of Poll\u0026quot;,y=NULL)+\nfacet_wrap(~method)+\ntheme(axis.text.x=element_text(angle=-90))\r\rTony Hirst\rYou can tidy up the x-axis labels by rotating them, eg using something like:\n+theme(axis.text.x=element_text(angle=-90))\r\rTony Hirst\rThis comment has been removed by the author.\r\r\r","date":1352073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1352073600,"objectID":"ddb3db9c76227ed8132728593b116195","permalink":"/2012/11/05/quick-post-about-getting-and-plotting-polls-in-r/","publishdate":"2012-11-05T00:00:00Z","relpermalink":"/2012/11/05/quick-post-about-getting-and-plotting-polls-in-r/","section":"post","summary":"With the election nearly upon us, I wanted to share an easy way I just found to download polling data and graph a few with ggplot2. dlinzer at github created a function to download poll data from the Huffington Post's Pollster API.\nThe default is to download national tracking polls from the presidential election. After sourcing the function, I load the required packages, download the data, and make the plot.","tags":null,"title":"Quick Post About Getting and Plotting Polls in R","type":"post"},{"authors":null,"categories":["GAM","ggplot","Visualization","optimization","tabu search","R"],"content":"\rFinding the best subset of variables for a regression is a very common task in statistics and machine learning. There are statistical methods based on asymptotic normal theory that can help you decide whether to add or remove a variable at a time. The problem with this is that it is a greedy approach and you can easily get stuck in a local mode. Another approach is to look at all possible subsets of the variables and see which one maximizes an objective function (accuracy on a test set, for example).\nHeuristics are required if the number of variables, p, gets large (\u0026gt;40) because the number of possible subsets is equal to 2^p, excluding interactions. One method that works well is called tabu search (TS). It is a more general optimization method, but in the case of finding the best subset, it is similar to stepwise methods. At any point, it will try to improve the objective function the most by adding or removing one variable. One difference is that TS will not add or remove variables that have been recently added or removed. This avoids the optimization from getting stuck at a local maximum. There are more details to the algorithm that you can read up on if you would like.\nThere is a tabuSearchpackage in R that implements this algorithm. I wanted to find a best subset regression using generalized additive models (GAMs) of the package mgcv. An issue arose, because you need to specify which terms are smooth and which are not in a formula to use mgcv. The general form of the formula looks like:\ngam(y ~ s(x1) + s(x2) + x3, data=train)\nwhere x1 and x2 are smooth terms and x3 is treated like it is in a linear model.\nMy data set has a combination of continuous variables, which I want to treat as smooth, and categorical variables, which I want to treat as factors. For each variable in the subset, I need to identify whether it is a factor or not, and then creating a string of the variable with or without s(). For example, th is a vector of 0's and 1's which indicate whether each variable (column) is in the regression. I used the housing data set from UCI ML repository. With 13 explanatory variables, there are 8192 possible main effects models. I am predicting MEDV, so I start the formula string with \"MEDV ~\". Then I loop through each element of th. If it is 1 then I want to add it to the formula. I check if it is a factor and if so I just add the name of the variable plus a \"+\". If it is continuous, I add the column name with \"s()\" around it. Finally I convert the string to a formula using formula(). I can plug in this formula into the gam function. num.cols=sum(th)\nfo.str=\"MEDV ~\"\ncum.cols=0\nfor (i in 1:length(th)) {\nif (th[i]\u0026gt;0) {\nif (is.factor(train[,i])) {\nfo.str=paste(fo.str,colnames(train)[i],sep=\" \")\n} else {\nfo.str=paste(fo.str,\" s(\",colnames(train)[i],\")\",sep=\"\")\n}\ncum.cols=cum.cols+1\nif (cum.cols\u0026lt;num.cols) {\nfo.str=paste(fo.str,\"+\")\n}\n}\n}\nfo=as.formula(fo.str)Created by Pretty R at inside-R.org\nFor evaluating the subsets, I split the data into training, validation, and testing. I trained the subset on the training data set and measured the R-squared on the prediction of the validation set. The full code can be found below.\nlibrary(mgcv)\nlibrary(tabuSearch)\n# http://archive.ics.uci.edu/ml/datasets/Housing\nhousing=read.table(\"http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\")\ncolnames(housing)=c(\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\")\n\u0026nbsp;\nhousing$CHAS=as.factor(housing$CHAS)\nhousing$RAD=as.factor(housing$RAD) # Changed to factor bc only 9 unique values\nsummary(housing)\n\u0026nbsp;\nset.seed(20120823)\ncv=sample(nrow(housing))\ntrain=housing[cv[1:300],]\nvalid=housing[cv[301:400],]\ntest=housing[cv[401:nrow(housing)],]\n\u0026nbsp;\nssto=sum((valid$MEDV-mean(valid$MEDV))^2)\nevaluate \u0026lt;- function(th){ num.cols=sum(th)\nif (num.cols == 0) return(0)\nfo.str=\"MEDV ~\"\ncum.cols=0\nfor (i in 1:length(th)) {\nif (th[i]\u0026gt;0) {\nif (is.factor(train[,i])) {\nfo.str=paste(fo.str,colnames(train)[i],sep=\" \")\n} else {\nfo.str=paste(fo.str,\" s(\",colnames(train)[i],\")\",sep=\"\")\n}\ncum.cols=cum.cols+1\nif (cum.cols\u0026lt;num.cols) {\nfo.str=paste(fo.str,\"+\")\n}\n}\n}\n# colnames(train)[c(th,0)==1]\nfo=as.formula(fo.str)\ngam1 \u0026lt;- gam(fo,data=train)\npred1 \u0026lt;- predict(gam1,valid,se=FALSE)\nsse \u0026lt;- sum((pred1-valid$MEDV)^2,na.rm=TRUE)\nreturn(1-sse/ssto)\n}\n\u0026nbsp;\nres \u0026lt;- tabuSearch(size = ncol(train)-1, iters = 20, objFunc = evaluate, listSize = 5,\nconfig = rbinom(ncol(train)-1,1,.5), nRestarts = 4,verbose=TRUE)\nIt was able to find a subset with a 0.8678 R-squared on the validation set (and 0.8349 on the test set). The formula found was:\n\nMEDV ~ s(CRIM) + s(INDUS) + s(NOX) + s(RM) + s(DIS) + RAD + s(TAX) + s(PTRATIO) + s(LSTAT).\nVisualizing the resultsThe tabu search gave me a subset which it thinks is best. But I would like to get a better handle on how it derived it, or if there were lots of models with similar quality, but different variables. Or if there were variables that were always in the top performing models. I created a heat plot that showed whether or not a variable was in the regression or not at each iteration.\nBelow you can see which variables were included in each iteration. There are a few variables that seem to be more important because they are included in almost every iteration, like LSAT, PTRATIO,and RM. But this doesn't tell me which iterations were the best.\n\nIn the chart below, I shaded each region by the ranking of model in that iteration. The higher ranking mean it did better. It is not easy, but we can glean a little more information from this chart. The models with RAD and DIS do significantly better than the models without them, even though they are not in every iteration. Further, the models with AGE seem a bit worse than those without it.\n\nThe R code to make these plots is below.\nlibrary(reshape)\nlibrary(ggplot2); theme_set(theme_bw())\n\u0026nbsp;\ntabu.df=data.frame(res$configKeep)\ncolnames(tabu.df)=colnames(train)[1:(ncol(train)-1)]\ntabu.df$Iteration=1:nrow(tabu.df)\ntabu.df$RSquared=res$eUtilityKeep\ntabu.df$Rank=rank(tabu.df$RSquared)\ntabu.melt=melt(tabu.df,id=c(\"Iteration\",\"RSquared\",\"Rank\"))\ntabu.melt$RSquared=ifelse(tabu.melt$value==1,tabu.melt$RSquared,0)\ntabu.melt$Rank=ifelse(tabu.melt$value==1,tabu.melt$Rank,0)\n(pHeat01 \u0026lt;- ggplot(tabu.melt, aes(Iteration,variable)) + geom_tile(aes(fill = value)) +\nscale_fill_gradient(low = \"white\", high = \"steelblue\",guide=FALSE))\n(pHeatRank \u0026lt;- ggplot(tabu.melt, aes(Iteration,variable)) + geom_tile(aes(fill = Rank)) +\nscale_fill_gradient(low = \"white\", high = \"steelblue\"))Created by Pretty R at inside-R.org\rComments\ranonymous\rGood post!\nNice approach with the heat map to go beyond just accepting the optimal solution. I agree that variables seem to be important because they are present in every iteration, but this could perhaps also mean that the tabu search was trapped in a subregion - perhaps the search move operator which only adds or deletes a single variable is not powerful enough to escape this region. Some possible remedies for exploring a larger part of the search landscape (if indeed this is a problem) might be:\n- Multiple tabu searches, each with a randomized set of initial variables in the model\n- Other move operators. How about finding correlation clusters among the variables, then let a move add or delete the entire cluster? Both move operators can be used in the search, using some metaheuristics to guide them.\n- Use metaheuristic techniques like \u0026quot;ruin and recreate\u0026quot; during the tabu search; occasionally, \u0026quot;ruin\u0026quot; the model by removing a random subset of the variables.\nOne more thought. With the tabu search already in place for regression, it wouldn´t be too hard to explore some nonlinear models. You could create temp variables that are products of the original variables, then let the tabu search add or delete these as well. Do you think such an approach would have any merit in this case?\n\r\rAndrew\r@Anonymous I would also like to see how this does compared to the more standard methods. It will take a little effort to implement in this example, though, because these variable selection techniques are not implemented for GAMs (as far as I know). http://www.statmethods.net/stats/regression.html has a nice summary of different methods available for linear models. You are right that the whole point of using a TS is that it will be better than the other methods, so it would be good to have validation. Maybe I could do a comparison for linear models...\nI described the meat of the TS algorithm in the post. If you want more info, I found this reference informative http://www.math.ntua.gr/~fouskakis/Publications/4.pdf \r\rAndrew\r@anspiess That is probably a good idea. But one reason to use Tabu Search is that it will speed up the time to find the best model. Running it many times will defeat this purpose. Running time for the parameters used was long already because GAMs take longer to train than LMs (~ 9 mins). In terms of better understanding TS, your idea would be useful. For what its worth, I ran it again with a different CV split and the chosen model was the same except it added s(ZN) and removed s(PTRATIO).\r\ranspiess\rMight it be feasible to sample training and test set a few hundred times to see how stable variable selection is?\r\rAnonymous\rI wonder how this method is better than using F test, t test or information criterion? did you compare tabusearch to other methods (for linear models)? maybe it its faster than standard methods? I would be grateful for answers:) if it its possible maybe you can describe this algorithm more closely.\r\r\r","date":1345766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1345766400,"objectID":"1fa0ee8cf971a3b191985e69a5519e96","permalink":"/2012/08/24/finding-the-best-subset-of-a-gam-using-tabu-search-and-visualizing-it-in-r/","publishdate":"2012-08-24T00:00:00Z","relpermalink":"/2012/08/24/finding-the-best-subset-of-a-gam-using-tabu-search-and-visualizing-it-in-r/","section":"post","summary":"Finding the best subset of variables for a regression is a very common task in statistics and machine learning. There are statistical methods based on asymptotic normal theory that can help you decide whether to add or remove a variable at a time. The problem with this is that it is a greedy approach and you can easily get stuck in a local mode. Another approach is to look at all possible subsets of the variables and see which one maximizes an objective function (accuracy on a test set, for example).","tags":null,"title":"Finding the Best Subset of a GAM using Tabu Search and Visualizing It in R","type":"post"},{"authors":null,"categories":["libFM","baseball","collaborative filtering","Matrix Factorization"],"content":"\rIntroduction Matrix factorization has been proven to be one of the best ways to do collaborative filtering. The most common example of collaborative filtering is to predict how much a viewer will like a movie. The power of matrix factorization was a key development of the Netflix Prize (see http://www2.research.att.com/~volinsky/papers/ieeecomputer.pdf).\nUsing the movie rating example, the idea is that there are some underlying features of the movie and underlying attributes of the user that interact to determine if the user will like the movie. So if the user typically likes comedies, and the movie is a comedy, these will interact and the user will like the movie. The best part is that in order to determine these features, we do not need to use any background information on the movie or the user. For example, we don't need someone to manually code whether each movie is a comedy or not. Matrix factorization just needs a set of historical ratings, which is usually easily available.\nUsing only a few features, we could probably interpret what each of them mean (comedy, drama, foreign, blockbuster, ...). They found in the Netflix competition that increasing the number of underlying features that make up a user and a movie almost always improved accuracy (they used up to 1500 features). With that many features, they can no longer be interpreted as in the example.\nBaseball DataI find this method interesting, and I wanted to apply it to some real data. One idea I had is predicting whether the result of a pitcher-batter match-up in baseball. Usually in baseball, the analysts cry \"small sample size\" if someone reports that a pitcher is particularly good/bad versus a batter. In order to get an accurate assessment that does not suffer from random variation, we need 100 or more at bats, which usually takes a few years at least.\nIt seems that matrix factorization might be a natural fit, because we are not predicting the specific interaction of the batter/pitcher. Instead, we are estimating some underlying features of the batter and the pitcher. We can estimate these features from all of the match-ups that the batter and pitcher have been in -- not just the ones against each other.\nOne example of a feature that I hoped to extract is batter and pitcher handedness. It is well known that batters fair poorly against pitchers of the same handedness and do better against pitchers of the opposite handedness. Other examples might relate to whether the pitcher can throw fast and how well the batter does against fast pitches; similarly with breaking balls, etc.\nIn the Netflix competition, each user rates movies on a 1 to 5 star scale, and rates the movie only once. The outcome of a match-up is different as there is no obvious \"rating\" of the outcome. I used improvement in expected number of runs scored of the outcome of a plate appearance as the \"rating\" to be predicted (from here). So a single is worth 0.474 runs and an out is worth -0.299 runs. Also, in baseball, there will be many match-ups between the batter and pitcher. I could have averaged the outcomes, but I left the multiple records in the data as is.\nI used 2010 data and removed all pitchers that were hitting and batters that were pitching. According to my data, there were 65,128 unique batter-pitcher match-ups (with about 173,000 plate appearances). I used 5,000 match-ups as validation, 5,000 as testing, and the remaining as training.\nModel FittingThe first model I used as a baseline just included \"biases\" for the batter and pitcher, as well as an overall league average bias. In the baseline, there are no factors or interactions. So the result of the match-up between a batter i and pitcher j would be estimated as the overall average bias plus the batter i bias plus the pitcher j bias. In math terms:\n\nHigher values for the batter bias mean the batter is better better than average and lower values for the pitcher bias means the pitcher is better than average.\nSo as to not overfit the data, I used L2 regularization on the batter and pitcher biases. For this and future estimation, I used the validation data to determine the best penalty and I used a gradient descent method to solve for the parameters.\nAdding in the factor interaction, the result of the match-up between a batter i and pitcher j is\n\nwhere  is a vector of offensive features for batter i and  is a set of defensive features for pitcher j. The two vectors must be of the same length which we can determine. Again, I used L2 regularization on the feature vectors.\nResultsI fit the matrix factorization model using 0, 1, 2, 5, and 10 features. The model with 0 features corresponds to the baseline model. As stated above, I was hoping to see that handedness would show up as one of the first features.\nI fit the models with the training set, tuned the regularization parameters with the validation set, and finally I am comparing the models with the test set. Unfortunately, the baseline model performed the best of all and the performance degraded as more features were added. A plot of the test mean squared errors is below. The difference between the models is small, but I believe that is because the regularization parameter is shrinking the factor vectors to 0.\nA further baseline model could have been used -- an overall mean with no biases. The MSE for all but the 10-factor model were better than this average. So the biases help, but unfortunately the factorization did not.\nI compared the factor vectors of the pitchers by their handedness and I found no difference between righties and lefties.\nSummaryThe outcome of this analysis was not what I hoped for, but I still learned something about matrix factorization and its implementation. I think the moral of the story is that it is hard to predict batter-pitcher match-ups, probably because there is a lot of variation in every at bat. Matrix factorization has been successfully applied in situations where the data is much more sparse than this application, so this difference may be a reason for its failure. I plan to try this again with the libFM software to see if I get the same results.\nUpdate:\nI implemented this in libFM and basically got the same results, so I guess I was doing it correctly. If I work on this further, I wonder if adding attributes about the pitcher explicitly would help. I would include an indicator for righty/lefty or maybe home/away.\r","date":1344556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1344556800,"objectID":"c1979b7e9270b245aab64ce422af4acd","permalink":"/2012/08/10/a-matrix-factorization-model-for-hitter/pitcher-matchups/","publishdate":"2012-08-10T00:00:00Z","relpermalink":"/2012/08/10/a-matrix-factorization-model-for-hitter/pitcher-matchups/","section":"post","summary":"Introduction Matrix factorization has been proven to be one of the best ways to do collaborative filtering. The most common example of collaborative filtering is to predict how much a viewer will like a movie. The power of matrix factorization was a key development of the Netflix Prize (see http://www2.research.att.com/~volinsky/papers/ieeecomputer.pdf).\nUsing the movie rating example, the idea is that there are some underlying features of the movie and underlying attributes of the user that interact to determine if the user will like the movie.","tags":null,"title":"A Matrix Factorization Model for Hitter/Pitcher Matchups","type":"post"},{"authors":null,"categories":["data","MATLAB"],"content":"\rI have been toying around with Kaggle's Million Song Dataset Challenge recently because I have some interest in collaborative filtering (using matrix factorization). I haven't made much progress with the competition (all 3 of my submissions are below the baseline), but I have learned a few things about dealing with large amounts of data.\nThe goal of the competition is to predict the 500 most likely songs each of 110,000 users will listen to next. As the name implies, there are 1,000,000 songs in the full dataset. To simplify things, I decided to concentrate on the most popular songs. I created a 110,000 x 2,000 matrix of 0's and 1's. Row i, column j is 1 if user i had listened to song j (the jth most popular song) and 0 if user i had not. As you can imagine, there are a lot more 0's than 1's in this matrix. The first few rows and columns look like this:\n0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 ...0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ......\nThis matrix was about 430 Mb and took a while to load into MATLAB. So I wisened up and created a sparse matrix. A sparse matrix realizes that most of the values are 0's and does not record them. Instead, it lists the locations of the non-zero elements and what the value is. For example, this is what the first few rows of the sparse matrix looks like:\n1 3 1\n1 7 1\n1 10 1\n1 13 1\n1 82 1\n1 717 1\n2 1111 1\n2 2972 1\n2 3516 1...\nThe first column is the row number, the second is the column number, and the third is the value at that location. In this application, all the values are 1. For this matrix, I used the 50,000 most popular songs (instead of just 2,000), and the size was much smaller -- just 17 Mb.\nIt is easy to load the sparse matrix into MATLAB with the spconvert command, and many of MATLAB's functions (like singular value decomposition) are optimized for sparse matrices.\r","date":1342742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1342742400,"objectID":"5d15abd5c0e32e8c758a8e349744b48e","permalink":"/2012/07/20/the-magical-sparse-matrix/","publishdate":"2012-07-20T00:00:00Z","relpermalink":"/2012/07/20/the-magical-sparse-matrix/","section":"post","summary":"I have been toying around with Kaggle's Million Song Dataset Challenge recently because I have some interest in collaborative filtering (using matrix factorization). I haven't made much progress with the competition (all 3 of my submissions are below the baseline), but I have learned a few things about dealing with large amounts of data.\nThe goal of the competition is to predict the 500 most likely songs each of 110,000 users will listen to next.","tags":null,"title":"The Magical Sparse Matrix","type":"post"},{"authors":null,"categories":["statistics","Random forest","R"],"content":"\rRandom forests ™ are great. They are one of the best \"black-box\" supervised learning methods. If you have lots of data and lots of predictor variables, you can do worse than random forests. They can deal with messy, real data. If there are lots of extraneous predictors, it has no problem. It automatically does a good job of finding interactions as well. There are no assumptions that the response has a linear (or even smooth) relationship with the predictors.\nAs cited in the Wikipedia article, they do lack some interpretability. But what they lack in interpretation, they more than make up for in prediction power, which I believe is much more important than interpretation in most cases. Even though you cannot easily tell how one variable affects the prediction, you can easily create a partial dependence plot which shows how the response will change as you change the predictor. You can also do this for two variables at once to see the interaction of the two.\nAlso helping in the interpretation is that they can output a list of predictor variables that they believe to be important in predicting the outcome. If nothing else, you can subset the data to only include the most \"important\" variables, and use that with another model. The randomForest package in R has two measures of importance. One is \"total decrease in node impurities from splitting on the variable, averaged over all trees.\" I do not know much about this one, and will not talk about it further. The other is based on a permutation test. The idea is that if the variable is not important (the null hypothesis), then rearranging the values of that variable will not degrade prediction accuracy. Random forests use out-of-bag (OOB) samples to measure prediction accuracy.\nIn my experience, it does a pretty good job of finding the most important predictors, but it has issues with correlated predictors. For example, I was working on a problem where I was predicting the price that electricity trades. One feature that I knew would be very important was the amount of electricity being used at that same time. But I thought there might also be a relationship between price and the electricity being used a few hours before and after. When I ran the random forest with these variables, the electricity used 1 hour after was found to be more important than the electricity used at the same time. When including the 1 hour after electricity use instead of the current hour electricity use, the cross validation (CV) error increased. Using both did not significantly change the CV error compared to using just current hour. Because the electricity used at the current hour and the hour after are very correlated, it had trouble telling which one was more important. In truth, given the electricity use at the current hour, the electricity use at the hour after did not improve the predictive accuracy.\nWhy does the importance measure give more weight correlated predictors? Strobl et al. give some intuition behind what is happening and propose a solution. Basically, the permutation test is ill-posed. The permutation test is testing that the variable is independent of the response as well as all other predictors. Since the correlated predictors are obviously not independent, we get high importance scores. They propose a permutation test where you condition on the correlated predictors. This is a little tricky when the correlated predictors are continuous, but you can read the paper for more details.\nAnother way to think of it is that, since each split only considers a subset of the possible variables, a variable that is correlated with an \"important\" variable may be considered without the \"important\" variable. This would cause the correlated variable to be selected for the split. The correlated value does hold some predictive value, but only because of the truly important variable, so it is understandable why this procedure would consider it important.\nI ran a simulation experiment (similar to the one in the paper) to demonstrate the issue. I simulated 5 predictor variables. Only the first one is related to the response, but the second one has a correlation of about 0.7 with the first one. Luckily, Strobl et al. included their version of importance in the package party in R. I compare variable importance from the randomForest package and the importance with and without taking correlated predictors into account from the partypackage.\n# simulate the data\nx1=rnorm(1000)\nx2=rnorm(1000,x1,1)\ny=2*x1+rnorm(1000,0,.5)\ndf=data.frame(y,x1,x2,x3=rnorm(1000),x4=rnorm(1000),x5=rnorm(1000))\n# run the randomForest implementation\nlibrary(randomForest)\nrf1 \u0026lt;- randomForest(y~., data=df, mtry=2, ntree=50, importance=TRUE)\nimportance(rf1,type=1)\n# run the party implementation\nlibrary(party)\ncf1 \u0026lt;- cforest(y~.,data=df,control=cforest_unbiased(mtry=2,ntree=50))\nvarimp(cf1)\nvarimp(cf1,conditional=TRUE)\n\nFor the randomForest, the ratio of importance of the the first and second variable is 4.53. For party without accounting for correlation it is 7.35. And accounting for correlation, it is 369.5. The higher ratios are better because it means that the importance of the first variable is more prominent. party's implementation is clearly doing the job.\nThere is a downside. It takes much longer to calculate importance with correlated predictors than without. For the party package in this example, it took 0.39 seconds to run without and 204.34 seconds with. I could not even run the correlated importance on the electricity price example. There might be a research opportunity to get a quicker approximation.\nPossibly up next: confidence limits for random forest predictions.\rComments\rAbhijeet Patil\rhi,m new to random forest.. i ve some doubts..\n1)\u0026quot;we need to choose m number of variables randomly for each node..\u0026quot;-can u pleas explain it..\nOR\n2)\u0026quot;take 1 bootstrap sample,choose some variables and create a decision tree\u0026quot;-is it correct??\n\r\rAnonymous\rCould you tell me what the numbers mean when ctree and randonForest return variable importance numbers please? I can get the output but I don\u0026#39;t know how to interpret them.\nThan you very much.\r\rAnonymous\rHI Andrew, When i tried to calculate the variable importance, I get the error \u0026quot;Error in model.matrix.default(as.formula(f), data = blocks) : allocMatrix: too many elements specified\u0026quot;\nI\u0026#39;m not sure how to deal with it because I have 2 factor levels which I\u0026#39;m predicting with 23 continuous variable predictors (about 2200 data points. I know it\u0026#39;s a lot to work with, but not sure how to get the correct VarImp values with this large data set.\nI tried increasing the threshold, but it didn\u0026#39;t help. fit.varimp=varimp(fit.cf,threshold=0.8,conditional=TRUE)\nDo you have any suggestions? Thanks.\r\rAndrew Landgraf\rUnfortunately, I didn\u0026#39;t set the seed. #nexttime\r\rAnonymous\ryou should provide the seed to reproduce your example\r\rAndrew\rIt should be able to give you a solution. I have no idea what ramifications this would have on accuracy or variable importance.\r\rAnonymous\rI have microarray data on 30 entries and thought of using RF to identify important response genes. I understand that this is a N\u0026lt;\u0026lt;p type of problems, but will \u0026#39;party\u0026#39; be able to come up with a solution?\r\rAndrew\rThe simple answer is using the partialPlot function in the randomForest package for R (http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=randomForest:partialPlot). What it is doing isn\u0026#39;t all that complicated. Say you want a partial dependence plot for the variable X_1. For each value of X_1=x that you want to plot, you take the average of the prediction with X_1=x and the other explanatory variables equal to the n values that they are in the data set. You are trying to average out the other variables.\r\rnanounanue\rGreat explanation!\nI have a question for you: In your text you said:\n\u0026quot;\u0026quot;\u0026quot;\n... Even though you cannot easily tell how one variable affects the prediction, you can easily create a partial dependence plot which shows how the response will change as you change the predictor. You can also do this for two variables at once to see the interaction of the two.\n\u0026quot;\u0026quot;\u0026quot;\nCould you provide, please, an example of how build the partial dependence plot?\nThank you for the post and thanks in advance\r\rAnonymous\rGreat post! Thank you.\r\r\r","date":1342656000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1342656000,"objectID":"86f120b5efa4e5f23b3f1eb00fc40b31","permalink":"/2012/07/19/random-forest-variable-importance/","publishdate":"2012-07-19T00:00:00Z","relpermalink":"/2012/07/19/random-forest-variable-importance/","section":"post","summary":"Random forests ™ are great. They are one of the best \"black-box\" supervised learning methods. If you have lots of data and lots of predictor variables, you can do worse than random forests. They can deal with messy, real data. If there are lots of extraneous predictors, it has no problem. It automatically does a good job of finding interactions as well. There are no assumptions that the response has a linear (or even smooth) relationship with the predictors.","tags":null,"title":"Random Forest Variable Importance","type":"post"},{"authors":null,"categories":["MATLAB","R"],"content":"\rForgive me if you are already aware of this, but I found it quite alarming. I know that most code is interpreted by the computer in binary and we input in decimal, so problems can arise in conversion and with floating point. But the example I have below is so simple that it really surprised me.\nI was converting a function from R into MATLAB so that a colleague could use it. I tested it out on the same data and got slightly different results. Digging into the problem, the difference was due to the fact that R was rounding 4.5 to 4 and MATLAB was rounding it to 5. I thought the \"4.5\" must have really been \"4.49999...\". But that was not so.\nFor example, this is the result of the round function for a few numbers.\n\u0026gt; round(0.5,0)\n[1] 0\n\u0026gt; round(1.5,0)\n[1] 2\n\u0026gt; round(2.5,0)\n[1] 2\n\u0026gt; round(3.5,0)\n[1] 4\n\u0026gt; round(4.5,0)\n[1] 4\n\u0026gt; round(5.5,0)\n[1] 6\n\u0026gt; round(6.5,0)\n[1] 6\n\nDo you see a pattern?\nI tried this on versions 2.13.1 and 2.14.0. I ran the same with MATLAB and it gave the expected results. I am not any kind of expert on computer sciences, so I was not sure why this is happening. Converting any decimal number that ends in .5 into binary results in a finite length binary number. For example, 4.5 is 100.1 in binary. Because of this, I wouldn't think the error would be due to floating points, but I really don't know.\nLooking at the documentation for round, I found the reason. It states in the notes, \"Note that for rounding off a 5, the IEC 60559 standard is expected to be used, ‘go to the even digit’.\" It is a little comforting knowing that there is a logic behind it and that R is abiding to some standard. But why isn't MATLAB abiding by the same standard? Also, I think most people expect numbers ending in .5 to round up, not the nearest even digit.\rComments\rAnalytic Bastard\rkudos Blaise\r\rAnonymous\rAndrew wrote \u0026quot;Also, I think most people expect numbers ending in .5 to round up (not the nearest even digit)\u0026quot;. This kind of rounding is in German called \u0026quot;kaufmännische Rundung\u0026quot; (rounding in commerce). For this purpose I use the following function:\n#Definition of a function for \u0026quot;rounding in commerce\u0026quot;\ncround = function(x,n){\nvorz = sign(x)\nz = abs(x)*10^n\nz = z + 0.5\nz = trunc(z)\nz = z/10^n\nz*vorz\n}\n# Example\n\u0026gt; round(seq(0.5,6.5,1),0)\n[1] 0 2 2 4 4 6 6\n\u0026gt; cround(seq(0.5,6.5,1),0)\n[1] 1 2 3 4 5 6 7\r\rcellocgw\rThis \u0026quot;round to even\u0026quot; approach has been accepted by just about everyone (except matlab, and no surprise, except Msoft Excel). Sadly, the flame wars over \u0026quot;round to even\u0026quot; vs. \u0026quot;round up\u0026quot; continue, rather the way people argue about \u0026quot;0.999... != 1\u0026quot;\nPS: @a Tom: I\u0026#39;m highly skeptical of your claim about 2.46--\u0026gt;3. Do you have a citation?\r\ra Tom\rI\u0026#39;m ever amazed that something so seemingly basic can have so many different approaches.\nI understand that in many middle east countries they start with the far right digit and round up or down, so 2.46 is rounded to 3!\r\rBlaise\rThis is discussed in Don Knuth\u0026#39;s 1973 classic Seminumerical Algorithms. He gives the following example of what can happen when 5s are always rounded upwards. Suppose u = 10000000 and v = 0.5555556. Then u + v = 1.5555556. If we subtract v from this result we get u\u0026#39; = 1.0000001. Adding and then subtracting v from u\u0026#39; and we get 1.0000002 and if we do it again we get 1.0000003 and so on. He says \u0026quot;This phenomenon, called drift, will not occur when we use a stable rounding rule based on the parity of the least significant digit.\u0026quot;\r\rAnonymous\rI was the #2 anonymous poster. Echoing Ben, I think that for ease of teaching, the \u0026quot;round 5 up\u0026quot; method is taught to children (and adults?) below the university level, and only if you go on for advance work is the more complicated method taught.\nCan you imagine trying to teach a 10 or 12 year old the IEC 60559 standard? Unfortunately, this is the method most adults are used to...\nI agree, it is a little troubling that Matlab doesn\u0026#39;t abide by the standard. Yet another reason to stick with R!\r\rBen Bolker\rWikipedia ( http://en.wikipedia.org/wiki/Rounding#Round_half_to_even ) says of round-to-even:\nThis method also treats positive and negative values symmetrically, and therefore is free of overall bias if the original numbers are positive or negative with equal probability. In addition, for most reasonable distributions of y values, the expected (average) value of the rounded numbers is essentially the same as that of the original numbers, even if the latter are all positive (or all negative). However, this rule will still introduce a positive bias for even numbers (including zero), and a negative bias for the odd ones.\nSo round-to-even seems to have *slightly* better numerical properties than \u0026quot;round ties away from zero\u0026quot;, which is what is (I think) most often taught, because it\u0026#39;s easier to understand. http://www.mathworks.com/matlabcentral/fileexchange/6752 gives a MATLAB function for \u0026quot;round to even\u0026quot;.\nIf I had to guess I would predict that in borderline cases (which this certainly is) MATLAB would favor \u0026quot;do what will lead to happier users\u0026quot; and R would favor \u0026quot;do what is thought to be the best numerical practice\u0026quot;.\r\rAnonymous\rHi,\nI\u0026#39;m not sure I understand what you mean by \u0026quot;expected results\u0026quot;?\nRegarding rounding, I was taught to round numbers ending in \u0026quot;1, 2, 3, and 4\u0026quot; *down*, and numbers that ended in \u0026quot;6, 7, 8, 9\u0026quot; *up*. Then, specifically regarding \u0026quot;5\u0026quot;, if the preceding digit is odd, round up and if the preceding digit is even, to round down. As you can see, this will then result in 50% of the numbers being rounded up, and 50% rounded down. If you round *down* on \u0026quot;1, 2, 3, 4\u0026quot; and round up on \u0026quot;5, 6, 7, 8, 9\u0026quot; you are rounding up 5/9th\u0026#39;s of the time, and so introducing a bias.\nIt sounds like R is handling it the way I would. Is that what you were wondering about?\r\rAnonymous\rTo learn something about how computers handle numbers, especially as it relates to statistics and econometrics:\nB. D. McCullough and H. D. Vinod\n\u0026quot;The Numerical Reliability of Econometric Software,\u0026quot;\nJournal of Economic Literature 37(2), 633-665, 1999 A temporary link is available here:\nhttp://www.pages.drexel.edu/~bdm25/jel.pdf\r\r\r","date":1339718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1339718400,"objectID":"144182d1b25e7ac46fdfa3a537a71bf2","permalink":"/2012/06/15/rounding-in-r/","publishdate":"2012-06-15T00:00:00Z","relpermalink":"/2012/06/15/rounding-in-r/","section":"post","summary":"Forgive me if you are already aware of this, but I found it quite alarming. I know that most code is interpreted by the computer in binary and we input in decimal, so problems can arise in conversion and with floating point. But the example I have below is so simple that it really surprised me.\nI was converting a function from R into MATLAB so that a colleague could use it.","tags":null,"title":"Rounding in R","type":"post"},{"authors":null,"categories":["GAM","Visualization","baseball","R"],"content":"\rI was having some fun with PITCHf/x data and generalize additive models. PITCHf/x keeps track of the trajectory, path, location of every pitch in the MLB. It is pretty accurate and opens up baseball to more analyses than ever before. Generalized additive models (GAMs) are statistical models that put minimal assumptions on the type of model you are fitting. Traditional statistical models are linear, in that they assume that the response variable you are modelling is a linear function of the explanatory variables. GAMs just assumes that the relationship is \"smooth.\" Here is a good example of a relationship that may have traditionally been modeled as linear, but it is a much better assumption that the relationship is smooth.\nI fit a GAM to PITCHf/x data. The response is whether or not Ichiro swung. The explanatory variables are pitch location on the x, pitch location on the z, and the day of the year. Obviously, we expect the probability of swinging to change as the pitch is closer or further away from the center of the strike zone. Additionally, I was interested in seeing his swinging propensity changed as the year went on.\n\nYou can see that the probability of swinging is smooth in both location and time. Also, you can see (ever so slightly) that the probability of swinging increased as the year went on. Looking at the splits, you can see that his walk percentage was 28/395 (7.1%) in the first half and 17/337 (5.0%) in the second half. This is in agreement with the swing probability increasing,\nI used the mgcv package in R to run the GAM. I created an image for every day and stitched them together into a movie with ffmpeg. The R code is here.\r","date":1338336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338336000,"objectID":"ac789819f4992cb38af17c3642f65867","permalink":"/2012/05/30/space-time-swing-probability-plot-for-ichiro/","publishdate":"2012-05-30T00:00:00Z","relpermalink":"/2012/05/30/space-time-swing-probability-plot-for-ichiro/","section":"post","summary":"I was having some fun with PITCHf/x data and generalize additive models. PITCHf/x keeps track of the trajectory, path, location of every pitch in the MLB. It is pretty accurate and opens up baseball to more analyses than ever before. Generalized additive models (GAMs) are statistical models that put minimal assumptions on the type of model you are fitting. Traditional statistical models are linear, in that they assume that the response variable you are modelling is a linear function of the explanatory variables.","tags":null,"title":"Space Time Swing Probability Plot for Ichiro","type":"post"},{"authors":null,"categories":["R"],"content":"\rDon't you hate it when you are running a long piece of code and you keep checking the results every 15 minutes, hoping it will finish? There is a better way.\nI got the idea from here. He uses a Python script and the text interface is not free. I thought someone must have already thought of this for R. There is an easy solution. You can send an email fairly easily in R. You can also send text messages as emails. This page gives a few examples of how to send an email in R. Sending a text via email will depend on what carrier you have and I am sure standard text message fees apply.You can only send 20 emails a day with this package, but that probably won't be a big deal.\nIt is basically a one-liner. Here is the code I used:\ninstall.packages(\"mail\",repos=\"http://cran.case.edu/\")\nlibrary(mail)\nsendmail(\"5555555555@txt.att.net\", subject=\"Notification from R\",message=\"MCMC Finished\", password=\"rmail\")\n\nThat's it!\n\r","date":1337904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1337904000,"objectID":"00245a5ac36be9e2e677ec37f9f49743","permalink":"/2012/05/25/sending-a-text-in-r/","publishdate":"2012-05-25T00:00:00Z","relpermalink":"/2012/05/25/sending-a-text-in-r/","section":"post","summary":"Don't you hate it when you are running a long piece of code and you keep checking the results every 15 minutes, hoping it will finish? There is a better way.\nI got the idea from here. He uses a Python script and the text interface is not free. I thought someone must have already thought of this for R. There is an easy solution. You can send an email fairly easily in R.","tags":null,"title":"Sending a Text in R","type":"post"},{"authors":null,"categories":["Visualization","baseball","R"],"content":"\rRecently, Chris Perez, the closer for the Indians, displayed some frustration with the fans for not supporting the team. Currently, they have the lowest attendance in the majors -- by a decent margin. The Indians are averaging about 15,000 fans per home game, while the next closest team, the Oakland A's, is averaging 19,000. It seemed like an odd time for Perez to bring this up because they have had attendance in the 29,000s each of the last two home games. So that intrigued me to look into the numbers of what causes attendance to vary.\nI looked at 2011 attendance data for the Cleveland Indians only. I had a strong suspicion that a popular opponent would definitely and weekend games cause attendance to increase. Also, there is usually some press at the beginning of the season that claims no one wants to go to the games because it is too cold for baseball. (There is also more competing entertainment at the beginning of the season.)\nWhat I found to be significant (based on an exploratory approach) are summarized in the graph below. This plot explores the relationship of attendance with 5 other variables. I plotted attendance on the y-axis and the date on the x-axis. I don't expect date to have any effect, but it organizes other aspects well (and you can see opening day had the highest attendance of the year). Instead of plotting points, I plotted the name of the opponent. You can see there are some larger attendances when they are playing the New York Yankees and the Cincinnati Reds, for example. The color of the team name indicates whether they are playing on the weekend or not and the size indicates the temperature. Probably the biggest effect, weekend games outdraw weekday games consistently. The colder temperatures are only in the beginning of the season, and seem to have a noticeable effect (at least for the coldest days).\n\nI also looked into how many games above .500 the team was and how close they were in the division race. Neither of these showed any correlation, at least at the marginal level. This is interesting because the main reason Chris Perez is frustrated is that the team is winning, so the fans should be supporting them. This shows that wining did not make much of a difference within a single year. This should be more prevalent over multiple years.\nSome other information that might be useful is the quality of the opponent or whether the ace of the opposing pitching staff is starting. I only included temperature and not precipitation or any other weather information.\nHere is the basic R code I used: library(ggplot2)\nggplot(data=home.attend,aes(x=Date,y=Attendance,colour=Weekend,label=Opp,size=Temp))+\ngeom_text()+scale_size(to = c(2, 5))+theme_bw()\u0026nbsp;\u0026nbsp;Update (9/21/2013):\nSince attendance is still a hot topic, I created the same plot for the 2013 season so far.\n\n\r","date":1337472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1337472000,"objectID":"87e13eaa30cdfc112e7af7338fdae47f","permalink":"/2012/05/20/cleveland-indians-attendance/","publishdate":"2012-05-20T00:00:00Z","relpermalink":"/2012/05/20/cleveland-indians-attendance/","section":"post","summary":"Recently, Chris Perez, the closer for the Indians, displayed some frustration with the fans for not supporting the team. Currently, they have the lowest attendance in the majors -- by a decent margin. The Indians are averaging about 15,000 fans per home game, while the next closest team, the Oakland A's, is averaging 19,000. It seemed like an odd time for Perez to bring this up because they have had attendance in the 29,000s each of the last two home games.","tags":null,"title":"Cleveland Indians' Attendance","type":"post"},{"authors":null,"categories":["Bayesian","baseball","R"],"content":"\rAfter signing a huge deal with the Angels, Pujols has been having a really bad year. He hasn't hit a home run this year, breaking a career long streak. So I thought it would be a good idea to use some statistics to tell how good or bad we think Pujols will actually be this year.\nComing into the year, he had a career .328/.420/.617 career AVG/OBP/SLG. Through one month, he has a .194/.237/.269. So what do we expect out of Pujols for the rest of the year?\nIn Bayesian statistical terms, we can quantify our prior beliefs about Pujols from his history before this year. Below are histograms and fitted distributions of Pujols' yearly batting lines from 2001 to 2011. His numbers are well above normal and he has been the best player in baseball for a while.\nSo coming into this year, we would expect him to have a batting average between .290 and .370, with .330 being the most likely, for example.\nCombining our prior expectations with the data we have observed from this year, we can get our posterior beliefs. When we do that, we get a posterior expectation that Pujols is a true .312/.379/.451 hitter. The league averages from 2001 to 2011 are .263/.331/.418, so he is still expected to be well above the average player, even with the poor start. If we use data from just this year, we do not have enough data to give us an accurate reflection of how good he is. If we combine the data with our prior beliefs, we get a better indication of what to expect. Below is a table that compares these numbers.\n.nobrtable br { display: none } \n\nAVGOBPSLGLeague.263.331.418Prior.328.420.617This Year.194.237.269Posterior.312.379.451\nFinally, we can also get the whole posterior distribution (not just the expectation). I have plotted the prior and posterior distributions on the same graph. You can see that Pujols' bad month has caused our beliefs about him to decrease quite a bit. Most notable is the slugging percentage, which is likely because of his career-long homerless streak.\n\nWhat Bayesian analysis does is shrinks (or regresses) the data from this year to the prior average. This is\u0026nbsp; the same idea of regressing to the overall mean that is talked about frequently in sabermetric blogs. The difference is that regressing to the mean usually regresses to the average player in the league. With Bayesian analysis, we can regress to our prior expectation about the specific player (Albert Pujols). I believe this approach will give us better results in most cases.\nThe R code to do the analysis can be found here. I used data from Baseball Reference.\rComments\rAndrew Landgraf\rJust a follow up: From May 6 to the end of the season he had a .305/.365/.569 split. The posterior expectation of .312/.379/.451 matches the average and on base percentage well, but his actual slugging is quite a bit higher than the posterior distribution would have predicted.\nNot sure what to make of this except to say he had a really bad start with slugging, which may have been caused more by random chance than the other two. And obviously the distributional assumptions are an approximation to reality.\r\r\r","date":1336176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1336176000,"objectID":"6f1120a8ca7b8141e6f409f402eb8121","permalink":"/2012/05/05/whats-up-with-albert-pujols/","publishdate":"2012-05-05T00:00:00Z","relpermalink":"/2012/05/05/whats-up-with-albert-pujols/","section":"post","summary":"After signing a huge deal with the Angels, Pujols has been having a really bad year. He hasn't hit a home run this year, breaking a career long streak. So I thought it would be a good idea to use some statistics to tell how good or bad we think Pujols will actually be this year.\nComing into the year, he had a career .328/.420/.617 career AVG/OBP/SLG. Through one month, he has a .","tags":null,"title":"What's Up with Albert Pujols?","type":"post"},{"authors":null,"categories":["Visualization","R"],"content":"\rCorrelation matrices are a common way to look at the dependence of a set of variables. When the variables have spatial relationships, the correlation matrix loses some information.\nLets say you have repeated observations, each one being a matrix. For example, you could have yearly observations of health statistics for a spatial grid. Lets say the grid is n by p (n*p variables) and there are m observations of the grid. If we want to get the correlations of each element of the grid, the typical way to do that would be to convert the matrix of variables into a vector of length n*p, and then calculate the correlation matrix of the vector. When you do that, however, it will no longer be obvious which of the variables are on the same row/column or are close to each other. So the typical correlation matrix is not satisfactory.\nWhat I propose is a set of small multiples of correlation matrices. Instead of having an n*p by n*p correlation matrix, we will have an n by p grid of correlation matrices, each correlation matrix representing the correlation with the variable in that position of the matrix. Below is an example.\nThe above example is just random data, so all correlations are spurious. Blue is positive, red is negative, and white is no correlation. You can see in the row 1, column 1 matrix that the 1st row and 1st column is dark blue. This is because this is the correlation with itself. Similarly in all other rows and columns.\nUsing a real example might display the usefulness more clearly. I am on a project estimating elements of a matrix with only the row totals and column totals. I simulated data many times and kept track of the errors. I was interested in how the errors in the different cells are correlated with each other. Below, you can see that the errors in the same row or column are positively correlated with each other, while the errors in other rows and columns are negatively correlated. This pops out at you with the below plot, but would have been difficult to figure out with a typical correlation matrix.\n\nCode:\n# Generate random data for the example\nreps=10\nmat.data=array(0,c(4,5,reps))\nfor (i in 1:reps) {\n\u0026nbsp; mat.data[,,i]=matrix(rmultinom(1,20,rep(1,4*5)/(4*5)),4,5)\n}\nmatrix.cor.plot(mat.data)\n# the function\nmatrix.cor.plot \u0026lt;- function(mat.data) {\n\u0026nbsp; #mat.data should be a nrow by ncol by nrep array\n\u0026nbsp; \u0026nbsp; nrow=dim(mat.data)[1]\n\u0026nbsp; ncol=dim(mat.data)[2]\n#\u0026nbsp;\u0026nbsp; nrep=dim(mat.data)[3]\n\u0026nbsp; \u0026nbsp; par(mfrow=c(nrow,ncol),cex=.75,bty=\"o\",mar=c(1, 1, 1, 1) + 0.1)\n\u0026nbsp; \u0026nbsp; # red is -1, white is 0, blue is +1\n\u0026nbsp; rgb.palette \u0026lt;- colorRampPalette(c(\"red\",\"white\",\"blue\"), space = \"rgb\")\n\u0026nbsp; \u0026nbsp; for (r in 1:nrow) {\n\u0026nbsp;\u0026nbsp;\u0026nbsp; for (c in 1:ncol) {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; cor.mat=matrix(0,nrow,ncol)\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; for (r2 in 1:nrow) { for (c2 in 1:ncol) {cor.mat[r2,c2]=cor(mat.data[r,c,],mat.data[r2,c2,]) } }\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; cor.mat=t(cor.mat)\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; cor.mat=cor.mat[,ncol(cor.mat):1]\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; image(cor.mat,zlim=c(-1,1),col=rgb.palette(120),axes = FALSE,main=paste(\"Row:\",r,\"Col:\",c))\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; box()\n\u0026nbsp;\u0026nbsp;\u0026nbsp; }\n\u0026nbsp; }\n}\r","date":1329436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1329436800,"objectID":"774ff82e0809c7dc0c2c80175dd29ec9","permalink":"/2012/02/17/visualizing-the-correlations-of-a-matrix/","publishdate":"2012-02-17T00:00:00Z","relpermalink":"/2012/02/17/visualizing-the-correlations-of-a-matrix/","section":"post","summary":"Correlation matrices are a common way to look at the dependence of a set of variables. When the variables have spatial relationships, the correlation matrix loses some information.\nLets say you have repeated observations, each one being a matrix. For example, you could have yearly observations of health statistics for a spatial grid. Lets say the grid is n by p (n*p variables) and there are m observations of the grid.","tags":null,"title":"Visualizing the Correlations of a Matrix","type":"post"},{"authors":null,"categories":["image segmentation","spectral clustering","R"],"content":"\rThat title is quite a mouthful. This quarter, I have been reading papers on Spectral Clustering for a reading group. The basic goal of clustering is to find groups of data points that are similar to each other. Also, data points in one group should be dissimilar to data in other clusters. This way you can summarize your data by saying there are a few groups to consider instead of all the points. Clustering is an unsupervised learning method in that there are no \"true\" groups that you are comparing the clusters to.\nThere are many ways to do this, two of the most popular are k-means and hierarchical clustering. Spectral clustering is nice because it gives you as much flexibility as you want to define how pairs of data points are similar or dissimilar. K-means only works well for data that are grouped in elliptically shaped, whereas spectral clustering can theoretically work well for any group. For example, the data in this image is easily clustered by spectral, but would not be by k-means. The flexibility of spectral clustering can also be a burden in that there are an infinite ways to group points.\nThe basic idea (and all the flexibility) behind spectral clustering is that you define the similarity between any two data points however you want, and put them in a matrix. So if you have 100 data points, you will end up with a 100x100 matrix, where the rth row and cth column is the similarity between the rth data point and the cth data point. You can define \"similarity\" any way you want. Popular methods are Euclidean distance, a kernel function of the Euclidean distance, or a k nearest neighbors approach.\nOnce you have the similarity matrix, you need to create a normalized/unnormalized Laplacian matrix, then calculate the eigenvectors and eigenvalues of the the Laplacian. Finally, use the k-means algorithm on the eigenvalues corresponding to the k smallest eigenvectors. This will give you k clusters (something else you need to specify).\nThe other day, someone in my office was working a project of Image Segmentation (a topic I know nothing about) for a machine learning class. I thought this would be a perfect application for spectral clustering because you can define similarity of pixels in terms of both the contrast of the pixel as well as the proximity to nearby pixels. I downloaded a few pictures from the Berkeley Segmentation Dataset Benchmark website.\nOne thing I quickly found was that even these moderately sized pictures were too big to create a similarity matrix for in R. A typical image is 481x321=154401 pixels. So a similarity matrix between all the pixels would be 154401x154401=23 billion elements. R only allows 2^31-1=2.1 billion elements in a matrix. Even if I could create the matrix, it would take forever to calculate the eigenvectors and eigenvalues. [Note: Some people from my department actually tackled this exact problem using sampling methods.]\nSo I had to reduce the size of the image. For this I just created an image of a factor of the original dimension (about 10 to 20 times smaller), and averaged the contrast of all the points that were collapsed. I also experimented with smoothing the image first and then doing the averaging In some cases it helped in some it hurt, I think.\nFor example here is an original picture.\n\nThen I smoothed using the image.smooth function of the fields package.\nThen I reduced the dimension by a factor of 10 and averaged the original pixels. The resulting image is below. You can see there is a decent loss of information in the averaging.\n\nFinally, for the similarity, I only considered pixels that were within 3 horizontally or vertically to be similar (otherwise 0). Also, for those within 3, I used a Gaussian kernel of the difference in contrast with variance equal to 0.01. I chose this number because the variance in the data was about 0.01. Varying both of these parameters wildly affected quality of the results. I also tried using a k nearest neighbors similarity and I did not get any good results. Hence, you can see both the positive and negative of the flexibility.\nAnyway, here are the two clusters (white and black) using the unnormalized Laplacian.\n\nIt looks very good and encouraging for future problems. As stated before, however, I am not sure how to determine the parameters for a generic problem.\nOverlaying the cluster on the original image, you can see the two segments of the image clearly. You can also see the loss in fidelity due to reducing the size of the image.\nHere are a couple of other examples that worked well. With the airplane one, in particular, you can see that the clustering was able to identify an unusual shape. I was not able to get it to work well with more than two clusters, although I only tried one image that was not that easy.\n\nFor posterity, here is the code I used.\n############\n# Import the image \n############\nlibrary(jpeg)\n# http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300/html/images/plain/normal/gray/86016.jpg\nrawimg=readJPEG(\"segment.jpeg\")\nrawimg=t(rawimg)\nrawimg=rawimg[,ncol(rawimg):1]\nimage(rawimg,col = grey((0:12)/12))\n\n############\n# Smooth the image\n############\nlibrary(fields)\nsmoothimg=image.smooth(rawimg,theta=2)\nimage(smoothimg,col = grey((0:12)/12))\n\n############\n# Reduce Size of Image\n############\nolddim=dim(rawimg)\nnewdim=c(round(olddim/10))\nprod(newdim)\u0026gt;2^31\nimg=matrix(NA,newdim[1],newdim[2])\nfor (r in 1:newdim[1]) {\n\u0026nbsp; centerx=(r-1)/newdim[1]*olddim[1]+1\n\u0026nbsp; lowerx=max(1,round(centerx-olddim[1]/newdim[1]/2,0))\n\u0026nbsp; upperx=min(olddim[1],round(centerx+olddim[1]/newdim[1]/2,0))\n\u0026nbsp; for (c in 1:newdim[2]) {\n\u0026nbsp;\u0026nbsp;\u0026nbsp; centery=(c-1)/newdim[2]*olddim[2]+1\n\u0026nbsp;\u0026nbsp;\u0026nbsp; lowery=max(1,round(centery-olddim[2]/newdim[2]/2,0))\n\u0026nbsp;\u0026nbsp;\u0026nbsp; uppery=min(olddim[2],round(centery+olddim[2]/newdim[2]/2,0))\n\u0026nbsp;\u0026nbsp;\u0026nbsp; img[r,c]=mean(smoothimg$z[lowerx:upperx,lowery:uppery])\n\u0026nbsp; }\n}\nimage(img,col = grey((0:12)/12))\n\n############\n# Convert matrix to vector\n############\nimgvec=matrix(NA,prod(dim(img)),3)\ncounter=1\nfor (r in 1:nrow(img)) {\n\u0026nbsp; for (c in 1:ncol(img)) {\n\u0026nbsp;\u0026nbsp;\u0026nbsp; imgvec[counter,1]=r\n\u0026nbsp;\u0026nbsp;\u0026nbsp; imgvec[counter,2]=c\n\u0026nbsp;\u0026nbsp;\u0026nbsp; imgvec[counter,3]=img[r,c]\n\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;\u0026nbsp;\u0026nbsp; counter=counter+1\n\u0026nbsp; }\n}\n\n############\n# Similarity Matrix\n############\npixdiff=2\nsigma2=.01 #var(imgvec[,3])simmatrix=matrix(0,nrow(imgvec),nrow(imgvec))\nfor(r in 1:nrow(imgvec)) {\n\u0026nbsp; cat(r,\"out of\",nrow(imgvec),\"\\n\")\n\u0026nbsp; simmatrix[r,]=ifelse(abs(imgvec[r,1]-imgvec[,1])\u0026lt;=pixdiff \u0026amp; abs(imgvec[r,2]-imgvec[,2])\u0026lt;=pixdiff,exp(-(imgvec[r,3]-imgvec[,3])^2/sigma2),0)\n}\n\u0026nbsp;\n############\n# Weighted and Unweighted Laplacian\n############\nD=diag(rowSums(simmatrix))\nDinv=diag(1/rowSums(simmatrix))\nL=diag(rep(1,nrow(simmatrix)))-Dinv %*% simmatrix\nU=D-simmatrix\n\n############\n# Eigen and k-means\n############\nevL=eigen(L,symmetric=TRUE)\nevU=eigen(U,symmetric=TRUE)\nkmL=kmeans(evL$vectors[,(ncol(simmatrix)-1):(ncol(simmatrix)-0)],centers=2,nstart=5)\nsegmatL=matrix(kmL$cluster-1,newdim[1],newdim[2],byrow=T)\nif(max(segmatL) \u0026amp; sum(segmatL==1)\u0026lt;sum(segmatL==0)) {segmatL=abs(segmatL-1)}\nkmU=kmeans(evU$vectors[,(ncol(simmatrix)-1):(ncol(simmatrix)-0)],centers=2,nstart=5)\nsegmatU=matrix(kmU$cluster-1,newdim[1],newdim[2],byrow=T)\nif(max(segmatU) \u0026amp;sum(segmatU==1)\u0026lt;sum(segmatU==0)) {segmatU=abs(segmatU-1)}\n############\n# Plotting the clusters\n############\nimage(segmatL, col=grey((0:15)/15))\nimage(segmatU, col=grey((0:12)/12))\n\n############\n# Overlaying the original and the clusters\n############\nimage(seq(0,1,length.out=olddim[1]),seq(0,1,length.out=olddim[2]),rawimg,col = grey((0:12)/12),xlim=c(-.1,1.1),ylim=c(-.1,1.1),xlab=\"\",ylab=\"\")\nsegmat=segmatU\nlinecol=2\nlinew=3\nfor(r in 2:newdim[1]) {\n\u0026nbsp; for (c in 2:newdim[2]) {\n\u0026nbsp;\u0026nbsp;\u0026nbsp; if(abs(segmat[r-1,c]-segmat[r,c])\u0026gt;0) {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; xloc=(r-1)/(newdim[1])\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ymin=(c-1)/(newdim[2])\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ymax=(c-0)/(newdim[2])\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; segments(xloc,ymin,xloc,ymax,col=linecol,lwd=linew)\n\u0026nbsp;\u0026nbsp;\u0026nbsp; }\n\u0026nbsp;\u0026nbsp;\u0026nbsp; if(abs(segmat[r,c-1]-segmat[r,c])\u0026gt;0) {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; yloc=(c-1)/(newdim[2])\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; xmin=(r-1)/(newdim[1])\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; xmax=(r-0)/(newdim[1])\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; segments(xmin,yloc,xmax,yloc,col=linecol,lwd=linew)\n\u0026nbsp;\u0026nbsp;\u0026nbsp; }\n\u0026nbsp; }\n}\r","date":1329004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1329004800,"objectID":"40d36694cb10833179094fb9fdca19e0","permalink":"/2012/02/12/unsupervised-image-segmentation-with-spectral-clustering-with-r/","publishdate":"2012-02-12T00:00:00Z","relpermalink":"/2012/02/12/unsupervised-image-segmentation-with-spectral-clustering-with-r/","section":"post","summary":"That title is quite a mouthful. This quarter, I have been reading papers on Spectral Clustering for a reading group. The basic goal of clustering is to find groups of data points that are similar to each other. Also, data points in one group should be dissimilar to data in other clusters. This way you can summarize your data by saying there are a few groups to consider instead of all the points.","tags":null,"title":"Unsupervised Image Segmentation with Spectral Clustering with R","type":"post"},{"authors":null,"categories":["JMP","Visualization"],"content":"\rI am a big fan of SAS's JMP software. It is the first statistical program I learned and I really like how the emphasize visualization. In their most recent update, JMP 9 now has the ability to create maps. I have been itching to test this out for a little while and I came across a map on the internet that I thought would be a good test. It is the percentage of the population of each state that has a passport.\n\u0026nbsp;\nLuckily, the website has the source data so I \"jumped\" right in. It was really easy.\nCopy and paste the data into JMP.\u0026nbsp;Open the Graph Builder under the Graph menu.\u0026nbsp;Drag the State field into the shape area on the lower left corner.You can see the outline of the USA in the map. JMP recognizes that the state field is filled with US state names, so it knows to open the shapefile of the US states.\n\u0026nbsp;\u0026nbsp;\u0026nbsp; 4. Drag the Population with Passport field onto the main map. You can also drag it into the Color area.\n\n\u0026nbsp;\u0026nbsp;\u0026nbsp; 5. Right click on the color and select \"Gradient...\" to customize the colors as you like. I changed it to \"White to Blue\" and checked the \"Reverse Colors\" check box to match the original map.\nBelow is the final result. Very quick and easy with a pretty result.\n\rComments\rXan Gregg\rNice work. You might try the reverse the labels instead of the colors -- I usually like the bigger numbers to be at the top of the legend list. (I also like bigger numbers to be associated with darker colors, but I realize you\u0026#39;re trying to duplicate that feature of the original.)\r\r\r","date":1299715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1299715200,"objectID":"1bc8ae08d4985bc9b82572f4fa55835a","permalink":"/2011/03/10/using-jmp-to-create-a-map/","publishdate":"2011-03-10T00:00:00Z","relpermalink":"/2011/03/10/using-jmp-to-create-a-map/","section":"post","summary":"I am a big fan of SAS's JMP software. It is the first statistical program I learned and I really like how the emphasize visualization. In their most recent update, JMP 9 now has the ability to create maps. I have been itching to test this out for a little while and I came across a map on the internet that I thought would be a good test. It is the percentage of the population of each state that has a passport.","tags":null,"title":"Using JMP to Create a Map","type":"post"},{"authors":null,"categories":["Bayesian","baseball","R"],"content":"\rI guess you could call this On Bayes Percentage. *cough*\nFresh off learning Bayesian techniques in one of my classes last quarter, I thought it would be fun to try to apply the method. I was able to find some examples of Hierarchical Bayes being used to analyze baseball data at Wharton. Setting up the problem\nOn base percentage (OBP) is probably the most important basic offensive statistic in baseball. Getting a reliable estimate of a players true ability to get on base is therefore important. The basic problem is that the sample size we get from one season rarely has enough observations so that we are certain of a player's ability. Even though there are 162 games in a season, there is a possibility that the actual OBP is the result of luck rather than skill. Bayesian analysis will \"regress\" the actual observed OBP to the mean, in that if a player has a small number of plate appearances (PA) it doesn't give them very much weight and the result will be something closer to the overall (MLB) average. On the other hand, if a player has quite a few PAs then it believes that the results are not the result of luck and it gives the observations a lot of weight.\nWe are trying to estimate the \"true\" OBP of each batter. Bayesian analysis assumes that the true OBP is random. Empirical Bayes is a method of figuring out the distribution of \"true\" OBP using the data. OBP is times on base divided by PA. Times on base (X) for each batter is distributed binomial with n=PA and p=true OBP. We further assume that p is distributed Beta with parameters a and b. It follows from this that the marginal distribution of X is distributed according to the distribution:\ngamma(a+b)*gamma(a+x)*gamma(n-x+b)*(n choose x)/(gamma(a)*gamma(b)*gamma(a+b+n))\nwhere gamma is the gamma function.\nWe will estimate the parameters a and b based on the data (X), using its marginal distribution (the \"empirical\" part of Bayes). To do this I found that likelihood of the marginal distribution of all the batters. Then I maximized this likelihood by adjusting the parameters a and b. This is called the ML-II.\nThe Analysis\nI used data for all non-pitchers in 2010. I assume that each player is independent. In doing that, I just have to multiply all the marginals for each player together to get the likelihood. When I do this and maximize it with respect to a and b, I get estimates that a = 83.48291 and b = 174.9038. I think this can be interpreted that prior mean (what we would assume that average OBP of a batter is before seeing him bat) is a/(a+b) = 0.323. This is pretty close to what the overall OBP of the league was (0.330). I think it makes sense that the prior is lower than the league average because batters who do well will get more opportunities and players that do poorly will get fewer. So the league average is biased high. Below is a graph of the prior distribution and the updated posteriors of every batter. You can (sort of) see that the posteriors have tighter distributions than the prior does. (The posterior distribution of each batter in this case is the distribution of OBP after we have observed PA and the actual OBP.)\n\nOne way to see why this Bayesian analysis is useful is to compare the posterior means with the observed OBP. If someone has only a few PAs, their OBP could be very high or very low and this may mislead you into thinking that this batter is very good or bad. However, the posterior mean takes into account the number of PAs. Below is a graph comparing the two. You can see that the range of values for posterior mean is pretty small, especially compare to actual OBP.\n\nHere is a list of the highest posterior mean OBP:\n   Batter Posterior Mean Actual OBP Joey Votto 0.396 0.424 Miguel Cabrera 0.392 0.420 Albert Pujols 0.390 0.414 Justin Morneau 0.388 0.437 Josh Hamilton 0.383 0.411 Prince Fielder 0.380 0.401 Shin-Soo Choo 0.379 0.401 Kevin Youkilis 0.379 0.412 Joe Mauer 0.378 0.402 Adrian Gonzalez 0.374 0.393 Daric Barton 0.374 0.393 Jim Thome 0.373 0.412 Paul Konerko 0.373 0.393 Jason Heyward 0.373 0.393 Matt Holliday 0.371 0.390 Carlos Ruiz 0.371 0.400 Manny Ramirez 0.371 0.409 Billy Butler 0.370 0.388 Jayson Werth 0.370 0.388 Ryan Zimmerman 0.369 0.388 \nAnd here is a list of the lowest posterior mean OBP:\n   Batter Posterior Mean Actual OBP Brandon Wood 0.252 0.175 Pedro Feliz 0.271 0.240 Jeff Mathis 0.276 0.219 Garret Anderson 0.277 0.204 Adam Moore 0.281 0.230 Josh Bell 0.285 0.224 Jose Lopez 0.286 0.270 Peter Bourjos 0.287 0.237 Aaron Hill 0.287 0.271 Tony Abreu 0.288 0.244 Koyie Hill 0.291 0.254 Gerald Laird 0.291 0.263 Drew Butera 0.291 0.237 Jeff Clement 0.291 0.237 Matt Carson 0.291 0.193 Humberto Quintero 0.292 0.262 Wil Nieves 0.292 0.244 Matt Tuiasosopo 0.292 0.234 Luis Montanez 0.292 0.155 Cesar Izturis 0.292 0.277 \nYou can see that all of the posterior means are pulled closer to the overall mean (the good players look worse and the bad players look better). The order changes a little bit but not too much. You can see the effect of sample size (PAs) by comparing Justin Morneau with Joey Votto. Morneau had a higher OBP, but Votto ended up with a higher posterior mean because he had more PAs (Votto had 648 while Morneau had 348). Here are their posterior distributions:\n\nBecause of the additional PAs, you can see that the distribution of Votto is a little tighter than Morneau. We are more sure that Votto is excellent than we are sure that Morneau is excellent.\r","date":1293753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293753600,"objectID":"95d649dc6ec3af09cc2e9bfbcffb14af","permalink":"/2010/12/31/empirical-bayes-estimation-of-on-base-percentage/","publishdate":"2010-12-31T00:00:00Z","relpermalink":"/2010/12/31/empirical-bayes-estimation-of-on-base-percentage/","section":"post","summary":"I guess you could call this On Bayes Percentage. *cough*\nFresh off learning Bayesian techniques in one of my classes last quarter, I thought it would be fun to try to apply the method. I was able to find some examples of Hierarchical Bayes being used to analyze baseball data at Wharton. Setting up the problem\nOn base percentage (OBP) is probably the most important basic offensive statistic in baseball.","tags":null,"title":"Empirical Bayes Estimation of On Base Percentage","type":"post"},{"authors":null,"categories":null,"content":"\rContinuing my series of trying to figure out which team is best to pick for survival football and then ignoring it, I present my week 3 analysis. I used the same method as the past 2 weeks, and didn't make any updates to it since last week. Here we go:\nNE (53%), BAL (20%), WAS (8%), and MIN (7%) are the most common teams picked in the Yahoo! leagues. They are also the top four teams according to my metric in a different order.\nThis week has NE favored by the most that they will be favored by for the rest of the season and by quite a bit so it makes sense that they are ranked at the top. Maybe the biggest difference between this analysis and the Yahoo! distribution is that Baltimore has 20% of Yahoo! but is just fourth best here. I would have to say that my numbers make more sense because Baltimore has three more games with a spread of 10 or higher and couple more with a spread higher than 6. I am surprised that Washington is up on both lists because they are only favored by 4 and playing an away game. Although they only have one more playable game and that is a 6 point favorite.\nI would definitely pick New England this week if perfection is your goal.\r","date":1285027200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1285027200,"objectID":"42425b0d48740e16b162f8c975eda2f1","permalink":"/2010/09/21/week-3-nfl-survival-odds/","publishdate":"2010-09-21T00:00:00Z","relpermalink":"/2010/09/21/week-3-nfl-survival-odds/","section":"post","summary":"Continuing my series of trying to figure out which team is best to pick for survival football and then ignoring it, I present my week 3 analysis. I used the same method as the past 2 weeks, and didn't make any updates to it since last week. Here we go:\nNE (53%), BAL (20%), WAS (8%), and MIN (7%) are the most common teams picked in the Yahoo! leagues. They are also the top four teams according to my metric in a different order.","tags":null,"title":"Week 3 NFL Survival Odds","type":"post"},{"authors":null,"categories":null,"content":"\rSo this is late, but I already did the analysis and I wanted to share my results for posterity. I used the same method as last time to try to evaluate who should be picked in a survival football pick 'em. This method basically only tries to figure out which teams, or combinations of teams, give you the best chance of getting all of your picks correct for the entire year. Obviously you want to pick a team this week that has the best chance of winning. But you do not want to pick a team that also has a lot of future value, where they will be favored by quite a bit in their remaining games. To reiterate, the way I did this is that I randomly picked a teams for the remainder of the season. I then used the Las Vegas point spreads of all the games to give determine the probability of winning each game and the rest of the games. Finally, I compared the average probability of winning every game when picking team X next week to the average probability of winning every game when picking a random team next week. I express this as a ratio - the higher, the better.\nThere are a couple of differences to how I did this last week.\nI modified the sampling of teams so that it does not choose teams on a bye week. This saved a lot of wasted simulations, and helps to make the results more stable. Therefore, I only simulated 100,000 seasons worth of picks compared to 1,000,000 last week.I omitted the team I picked last week from consideration of being picked for the rest of the season. Last week I picked Chicago, so they are not included.\u0026nbsp;The bar charts on the graphic start at zero. I committed a cardinal sin of graphing last week because of Excel's defaults.Here is what I came up with for Week 2:\nWhat this is saying is that you are about 48% more likely to go 16 - 0 for the rest of the year if you choose Green Bay than if you pick a random team. One aspect this method does not address is that your goal is not always to have a perfect season. If the league is small enough, you just want to outlast your opponents. My league only had about 10 people to start. If you looked at Yahoo!'s dashboard, you'd see that 56% of people chose GB this week. If they go down and you didn't pick them, your chances of winning increase dramatically. That is why I didn't choose GB and why I lost.\nI chose DAL, because the spread was high, the percent taking them was low, and they had less future value. It was almost entirely on the recommendation of Vegas Watch. Maybe I should have looked at my ranking and picked Oakland, Atlanta or Cleveland since there was even a lower percentage picking those teams and a higher ranking than Dallas. With hindsight, we now know that Green Bay, Oakland, and Atlanta won and Cleveland and Dallas lost. But there was a strategy to it because 4 of the 9 teams that picked took Green Bay. If they lost and my team won, I would have had a much better chance of winning.\nI will brainstorm about a way to factor the pick distribution into the rankings, but I do not think it is possible the way it is currently set up.\r","date":1284940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1284940800,"objectID":"9e226c680800d687ca6770ea906e293b","permalink":"/2010/09/20/week-2-nfl-survival-odds/","publishdate":"2010-09-20T00:00:00Z","relpermalink":"/2010/09/20/week-2-nfl-survival-odds/","section":"post","summary":"So this is late, but I already did the analysis and I wanted to share my results for posterity. I used the same method as last time to try to evaluate who should be picked in a survival football pick 'em. This method basically only tries to figure out which teams, or combinations of teams, give you the best chance of getting all of your picks correct for the entire year.","tags":null,"title":"Week 2 NFL Survival Odds","type":"post"},{"authors":null,"categories":null,"content":"\rThe NFL season is starting tomorrow night and I am in a survival league this year. If you are not familiar, in a survival league, each week you pick one team to win their game. Once you pick a team, you can no longer pick them for the rest of the season. So you have to balance pick who you think has the best odds of winning this week and what the rest of the team's schedule looks like.\nThere are some sites online that are very helpful for deciding which team to pick this week. One of them is http://www.survivorgrid.com/. I was wondering if there was a way to use this information to automatically decide which team to pick. This is an optimization problem with a lot of moving pieces, so simulation seemed like a natural choice.\nI downloaded the spreads for each game from that website (using some expert Excel skills). From the lines, I was able to discern the probability of winning the game from this paper. (I found out about this method from the book Mathletics.) Basically, I just assume that margin of victory for the game is normally distributed with a mean equal to the spread (meaning the bookies are on average right) and a standard deviation equal to 13.86 points (as found in that paper). I finally calculate the probability that the margin of victory (\u0026gt;0) is in the team's favor. There are obviously a lot of assumptions here (I doubt the spread is normal plus we are dealing with discrete amounts), but at least this gives us a ballpark.\nSo now I have the probability of each team winning each of their remaining games. Next, I simulated. If I wanted to spend more time, I would have used a genetic algorithm or some other AI to try to find the best combination of picks. But I decided to go brute force because I didn't want to spend a lot of time. Here was my process:\nI told R to randomly sample which teams to pick each week (SD for week 1, IND for week 2, etc.)Using the probabilities I created from the spreads, I calculated the probability that this combination of picks goes undefeated the whole season. The probability of going undefeated is the product of winning all of the individual games.I wish there was an easy way, but I had to waste a lot of trials here. Whenever a team had bye and the random sample picked that team on that week, they obviously have a 0% chance of advancing. I wanted to prevent the random sample from selecting teams on a bye but it became a little hairy so I gave up.Do this 1,000,000 times Analyze the picks with the highest probability of going undefeated.\u0026nbsp;Here are the 20 best simulations I came up with. You can see that the even the best have pretty low odds of going the distance.\n\nFinally, I took the average probability of winning it all when each team was selected for the first game. I then compared that to the average overall probability of winning it all. This should give you a good idea of what team to select this week.\n\nThis is probably the best way to use this data rather than looking at the best combination of picks since the lines for the rest of the year will see a significant change as they get closer. This also agrees with Vegas Watch's analysis, which I like to see.\rComments\rAndrew\rI was thinking the same thing. I cannot think of an easy application to the way I have set up the problem though. I am currently trying to find the best probability of getting them all correct, so I really don\u0026#39;t care who other people are selecting.\nI would need to formulate the problem differently to account for that. I think I would also need to have an idea of what teams people have picked in the past and what strategy they will use in the future. Then I could run a simulation and see which team gives the highest probability of finishing in the top x%.\r\rVegas Watch\rInteresting approach. It seems to me like this system would do a great job of taking into account probability of advancing to next week while saving as much future value as possible, but it doesn\u0026#39;t factor in what % of people are picking each team, does it? I\u0026#39;ve found -- in much less complex analysis, generally just comparing two options -- that taking a less popular side can make up for a good deal of win probability. Doubt it will make a difference this week, but eventually there will be a team that\u0026#39;s extremely obvious, so your sim says to take them, but half your pool is taking them as well, which presents a problem.\r\r\r","date":1283904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1283904000,"objectID":"aca32c1220114e0a2443799aff304bdb","permalink":"/2010/09/08/nfl-survival-odds/","publishdate":"2010-09-08T00:00:00Z","relpermalink":"/2010/09/08/nfl-survival-odds/","section":"post","summary":"The NFL season is starting tomorrow night and I am in a survival league this year. If you are not familiar, in a survival league, each week you pick one team to win their game. Once you pick a team, you can no longer pick them for the rest of the season. So you have to balance pick who you think has the best odds of winning this week and what the rest of the team's schedule looks like.","tags":null,"title":"NFL Survival Odds","type":"post"},{"authors":null,"categories":null,"content":"\rIf the past is a predictor of future performance, then there is about a 99.3% chance that I will stop updating this in 2 weeks. But you have to start somewhere. That being said, there are two reasons I want to start a blog:\nI think it would be really awesome if I was able to work as a statistician for a major league baseball team. To get a job like that, I will need to give some sort of reference to baseball statistic-ry that I have done. This gives me such an outlet.I am pursuing a PhD in statistics. In order to get a PhD, I will have to write a thesis. On the recommendation of this website, a blog is a good way to improve my writing. Quote:That's why I recommend that new students start a blog. Even if no one else reads it, start one. You don't even have to write about your research. Practicing the act of writing is all that matters.\u0026nbsp; I found the link above because there was another post on the same website that helped put school in perspective for me that is worth sharing http://matt.might.net/articles/phd-school-in-pictures/.\r","date":1283040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1283040000,"objectID":"9d64ac3c0bbf880636d1d320f8067bbf","permalink":"/2010/08/29/why-we-blog/","publishdate":"2010-08-29T00:00:00Z","relpermalink":"/2010/08/29/why-we-blog/","section":"post","summary":"If the past is a predictor of future performance, then there is about a 99.3% chance that I will stop updating this in 2 weeks. But you have to start somewhere. That being said, there are two reasons I want to start a blog:\nI think it would be really awesome if I was able to work as a statistician for a major league baseball team. To get a job like that, I will need to give some sort of reference to baseball statistic-ry that I have done.","tags":null,"title":"Why We Blog","type":"post"},{"authors":null,"categories":null,"content":" Logistic PCA and Generalized PCA My dissertation research with Prof. Yoonkyung Lee deals with dimensionality reduction of binary and count data. We propose a generalization of principal component analysis to non-Gaussian data. Our method minimizes the deviance by solving for a projection matrix which projects the natural parameters of the saturated model onto a lower dimensional space. Two preprint articles are available here. An R package implementing this research for binary data is available on CRAN. A complementary R package for all types of data is available on Github. For this research, I won the department\u0026rsquo;s Whitney Award for Outstanding Thesis Researcher.\nOrigin-Destination Estimation on Bus Routes Given passenger boarding and alighting counts which are automatically generated by buses, we have been developing methods to improve origin-destination (OD) flow estimation. Our method uses variational Bayes to estimate the posterior of the OD flows. When multiple patterns may be occurring during a period, we can determine the number of patterns and cluster bus trips, which leads to increased accuracy. Two papers are in preparation and an R package is in early development. For my work at the transit lab, I was awarded the Whitney Award for best Research Associate.\nPredicting Student Enrollment During my summer at Data Science for Social Good, our team worked with Chicago Public Schools to more accurately predict the number of students enrolling at each of the system\u0026rsquo;s 600 schools. A blog post describing the problem is here and the Github repository of some of our solutions is here.\nPublic Transportation and GHG We broadly assessed the impact that public transportation has on reducing greenhouse gas (GHG) emissions in US cities. Two papers have been published based on this research. The first describes the data collection process and initial results. The second details the model we built which accounts for potential biases.\nCapital One Student Modeling Competition I was part of the team that won the 2013 Capital One Student Modeling Competition. The task was to build a recommender system to offer the most relevant coupons to customers. Our solution extended the matrix factorization techniques that were used in the Netflix Prize.\nOther  Interactive Shiny visualizations Github profile My blog  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"Logistic PCA and Generalized PCA My dissertation research with Prof. Yoonkyung Lee deals with dimensionality reduction of binary and count data. We propose a generalization of principal component analysis to non-Gaussian data. Our method minimizes the deviance by solving for a projection matrix which projects the natural parameters of the saturated model onto a lower dimensional space. Two preprint articles are available here. An R package implementing this research for binary data is available on CRAN.","tags":null,"title":"My Projects","type":"page"}]