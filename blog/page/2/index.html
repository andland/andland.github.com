
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Statistically Significant</title>
  <meta name="author" content="Andrew Landgraf">

  
  <meta name="description" content="Finding the best subset of variables for a regression is a very common task in statistics and machine learning. There are statistical methods based &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://andland.github.io/blog/page/2">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Statistically Significant" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Statistically Significant</a></h1>
  
    <h2>Andrew Landgraf's Blog</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:andland.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Blog Archives</a></li>
  <li><a href="/about">About</a></li>
  <li><a href="/projects">Shiny</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/08/24/finding-best-subset-of-gam-using-tabu/">Finding the Best Subset of a GAM Using Tabu Search and Visualizing It in R</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-08-24T00:00:00-04:00" pubdate data-updated="true">Aug 24<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
Finding the best subset of variables for a regression is a very common task in statistics and machine learning. There are statistical methods based on asymptotic normal theory that can help you decide whether to add or remove a variable at a time. The problem with this is that it is a greedy approach and you can easily get stuck in a local mode. Another approach is to look at all possible subsets of the variables and see which one maximizes an objective function (accuracy on a test set, for example).<br /><br />Heuristics are required if the number of variables, <i>p</i>, gets large (&gt;40) because the number of possible subsets is equal to 2^<i>p</i>, excluding interactions. One method that works well is called <a href="http://en.wikipedia.org/wiki/Tabu_search">tabu search</a> (TS). It is a more general optimization method, but in the case of finding the best subset, it is similar to stepwise methods. At any point, it will try to improve the objective function the most by adding or removing one variable. One difference is that TS will not add or remove variables that have been recently added or removed. This avoids the optimization from getting stuck at a local maximum. There are more details to the algorithm that you can read up on if you would like.<br /><br />There is a <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">tabuSearch<span style="font-family: Arial,Helvetica,sans-serif;"> </span></span>package in <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">R</span> that implements this algorithm. I wanted to find a best subset regression using generalized additive models (GAMs) of the package <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">mgcv</span>. An issue arose, because you need to specify which terms are smooth and which are not in a formula to use <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">mgcv</span>. The general form of the formula looks like:<br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">gam(y ~ s(x1) + s(x2) + x3, data=train)</span><br />where <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">x1</span> and <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">x2</span> are smooth terms and <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">x3</span> is treated like it is in a linear model.<br /><br />My data set has a combination of continuous variables, which I want to treat as smooth, and categorical variables, which I want to treat as factors. For each variable in the subset, I need to identify whether it is a factor or not, and then creating a string of the variable with or without <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">s()</span>. <br /><br />For example, <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">th</span> is a vector of 0&#8217;s and 1&#8217;s which indicate whether each variable (column) is in the regression. I used the housing data set from <a href="http://archive.ics.uci.edu/ml/datasets/Housing">UCI ML repository</a>. With 13 explanatory variables, there are 8192 possible main effects models. I am predicting MEDV, so I start the formula string with &#8220;MEDV ~&#8221;. Then I loop through each element of <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">th</span>. If it is 1 then I want to add it to the formula. I check if it is a factor and if so I just add the name of the variable plus a &#8220;+&#8221;. If it is continuous, I add the column name with &#8221;<span style="font-family: &quot;Courier New&quot;,Courier,monospace;">s()</span>&#8221; around it. Finally I convert the string to a formula using <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">formula()</span>. I can plug in this formula into the <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">gam</span> function. <br /><br /><div style="overflow: auto;"><div class="geshifilter"><pre class="r geshifilter-R" style="font-family: monospace;">num.cols=<a href="http://inside-r.org/r-doc/base/sum"><span style="color: #003399; font-weight: bold;">sum</span></a><span style="color: #009900;">(</span>th<span style="color: #009900;">)</span><br />fo.str=<span style="color: blue;">"MEDV ~"</span><br />cum.cols=<span style="color: #cc66cc;">0</span><br /><span style="color: black; font-weight: bold;">for</span> <span style="color: #009900;">(</span>i <span style="color: black; font-weight: bold;">in</span> <span style="color: #cc66cc;">1</span>:<a href="http://inside-r.org/r-doc/base/length"><span style="color: #003399; font-weight: bold;">length</span></a><span style="color: #009900;">(</span>th<span style="color: #009900;">)</span><span style="color: #009900;">)</span> <span style="color: #009900;">{</span><br />  <span style="color: black; font-weight: bold;">if</span> <span style="color: #009900;">(</span>th<span style="color: #009900;">[</span>i<span style="color: #009900;">]</span>&gt;<span style="color: #cc66cc;">0</span><span style="color: #009900;">)</span> <span style="color: #009900;">{</span><br />    <span style="color: black; font-weight: bold;">if</span> <span style="color: #009900;">(</span><a href="http://inside-r.org/r-doc/base/is.factor"><span style="color: #003399; font-weight: bold;">is.factor</span></a><span style="color: #009900;">(</span>train<span style="color: #009900;">[</span><span style="color: #339933;">,</span>i<span style="color: #009900;">]</span><span style="color: #009900;">)</span><span style="color: #009900;">)</span> <span style="color: #009900;">{</span><br />      fo.str=<a href="http://inside-r.org/r-doc/base/paste"><span style="color: #003399; font-weight: bold;">paste</span></a><span style="color: #009900;">(</span>fo.str<span style="color: #339933;">,</span><a href="http://inside-r.org/r-doc/base/colnames"><span style="color: #003399; font-weight: bold;">colnames</span></a><span style="color: #009900;">(</span>train<span style="color: #009900;">)</span><span style="color: #009900;">[</span>i<span style="color: #009900;">]</span><span style="color: #339933;">,</span>sep=<span style="color: blue;">" "</span><span style="color: #009900;">)</span><br />    <span style="color: #009900;">}</span> <span style="color: black; font-weight: bold;">else</span> <span style="color: #009900;">{</span><br />      fo.str=<a href="http://inside-r.org/r-doc/base/paste"><span style="color: #003399; font-weight: bold;">paste</span></a><span style="color: #009900;">(</span>fo.str<span style="color: #339933;">,</span><span style="color: blue;">" s("</span><span style="color: #339933;">,</span><a href="http://inside-r.org/r-doc/base/colnames"><span style="color: #003399; font-weight: bold;">colnames</span></a><span style="color: #009900;">(</span>train<span style="color: #009900;">)</span><span style="color: #009900;">[</span>i<span style="color: #009900;">]</span><span style="color: #339933;">,</span><span style="color: blue;">")"</span><span style="color: #339933;">,</span>sep=<span style="color: blue;">""</span><span style="color: #009900;">)</span><br />    <span style="color: #009900;">}</span><br />    cum.cols=cum.cols+<span style="color: #cc66cc;">1</span><br />    <span style="color: black; font-weight: bold;">if</span> <span style="color: #009900;">(</span>cum.cols&lt;num.cols<span style="color: #009900;">)</span> <span style="color: #009900;">{</span><br />      fo.str=<a href="http://inside-r.org/r-doc/base/paste"><span style="color: #003399; font-weight: bold;">paste</span></a><span style="color: #009900;">(</span>fo.str<span style="color: #339933;">,</span><span style="color: blue;">"+"</span><span style="color: #009900;">)</span><br />    <span style="color: #009900;">}</span><br />  <span style="color: #009900;">}</span><br /><span style="color: #009900;">}</span><br />fo=<a href="http://inside-r.org/r-doc/stats/as.formula"><span style="color: #003399; font-weight: bold;">as.formula</span></a><span style="color: #009900;">(</span>fo.str<span style="color: #009900;">)</span></pre></div></div><a href="http://www.inside-r.org/pretty-r" title="Created by Pretty R at inside-R.org">Created by Pretty R at inside-R.org</a><br /><br />For evaluating the subsets, I split the data into training, validation, and testing. I trained the subset on the training data set and measured the R-squared on the prediction of the validation set. The full code can be found below.<br /><br /><div style="overflow: auto;"><div class="geshifilter"><pre class="r geshifilter-R" style="font-family: monospace;"><a href="http://inside-r.org/r-doc/base/library"><span style="color: #003399; font-weight: bold;">library</span></a><span style="color: #009900;">(</span><a href="http://inside-r.org/r-doc/mgcv/mgcv"><span style="color: #003399; font-weight: bold;">mgcv</span></a><span style="color: #009900;">)</span><br /><a href="http://inside-r.org/r-doc/base/library"><span style="color: #003399; font-weight: bold;">library</span></a><span style="color: #009900;">(</span>tabuSearch<span style="color: #009900;">)</span><br /><span style="color: #666666; font-style: italic;"># http://archive.ics.uci.edu/ml/datasets/Housing</span><br /><a href="http://inside-r.org/r-doc/MASS/housing"><span style="color: #003399; font-weight: bold;">housing</span></a>=<a href="http://inside-r.org/r-doc/utils/read.table"><span style="color: #003399; font-weight: bold;">read.table</span></a><span style="color: #009900;">(</span><span style="color: blue;">"http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"</span><span style="color: #009900;">)</span><br /><a href="http://inside-r.org/r-doc/base/colnames"><span style="color: #003399; font-weight: bold;">colnames</span></a><span style="color: #009900;">(</span><a href="http://inside-r.org/r-doc/MASS/housing"><span style="color: #003399; font-weight: bold;">housing</span></a><span style="color: #009900;">)</span>=<a href="http://inside-r.org/r-doc/base/c"><span style="color: #003399; font-weight: bold;">c</span></a><span style="color: #009900;">(</span><span style="color: blue;">"CRIM"</span><span style="color: #339933;">,</span> <span style="color: blue;">"ZN"</span><span style="color: #339933;">,</span> <span style="color: blue;">"INDUS"</span><span style="color: #339933;">,</span> <span style="color: blue;">"CHAS"</span><span style="color: #339933;">,</span> <span style="color: blue;">"NOX"</span><span style="color: #339933;">,</span> <span style="color: blue;">"RM"</span><span style="color: #339933;">,</span> <span style="color: blue;">"AGE"</span><span style="color: #339933;">,</span> <span style="color: blue;">"DIS"</span><span style="color: #339933;">,</span> <span style="color: blue;">"RAD"</span><span style="color: #339933;">,</span> <span style="color: blue;">"TAX"</span><span style="color: #339933;">,</span> <span style="color: blue;">"PTRATIO"</span><span style="color: #339933;">,</span> <span style="color: blue;">"B"</span><span style="color: #339933;">,</span> <span style="color: blue;">"LSTAT"</span><span style="color: #339933;">,</span> <span style="color: blue;">"MEDV"</span><span style="color: #009900;">)</span><br />&nbsp;<br /><a href="http://inside-r.org/r-doc/MASS/housing"><span style="color: #003399; font-weight: bold;">housing</span></a>$CHAS=<a href="http://inside-r.org/r-doc/base/as.factor"><span style="color: #003399; font-weight: bold;">as.factor</span></a><span style="color: #009900;">(</span><a href="http://inside-r.org/r-doc/MASS/housing"><span style="color: #003399; font-weight: bold;">housing</span></a>$CHAS<span style="color: #009900;">)</span><br /><a href="http://inside-r.org/r-doc/MASS/housing"><span style="color: #003399; font-weight: bold;">housing</span></a>$RAD=<a href="http://inside-r.org/r-doc/base/as.factor"><span style="color: #003399; font-weight: bold;">as.factor</span></a><span style="color: #009900;">(</span><a href="http://inside-r.org/r-doc/MASS/housing"><span style="color: #003399; font-weight: bold;">housing</span></a>$RAD<span style="color: #009900;">)</span> <span style="color: #666666; font-style: italic;"># Changed to factor bc only 9 unique values</span><br /><a href="http://inside-r.org/r-doc/base/summary"><span style="color: #003399; font-weight: bold;">summary</span></a><span style="color: #009900;">(</span><a href="http://inside-r.org/r-doc/MASS/housing"><span style="color: #003399; font-weight: bold;">housing</span></a><span style="color: #009900;">)</span><br />&nbsp;<br /><a href="http://inside-r.org/r-doc/base/set.seed"><span style="color: #003399; font-weight: bold;">set.seed</span></a><span style="color: #009900;">(</span><span style="color: #cc66cc;">20120823</span><span style="color: #009900;">)</span><br />cv=<a href="http://inside-r.org/r-doc/base/sample"><span style="color: #003399; font-weight: bold;">sample</span></a><span style="color: #009900;">(</span><a href="http://inside-r.org/r-doc/base/nrow"><span style="color: #003399; font-weight: bold;">nrow</span></a><span style="color: #009900;">(</span><a href="http://inside-r.org/r-doc/MASS/housing"><span style="color: #003399; font-weight: bold;">housing</span></a><span style="color: #009900;">)</span><span style="color: #009900;">)</span><br />train=<a href="http://inside-r.org/r-doc/MASS/housing"><span style="color: #003399; font-weight: bold;">housing</span></a><span style="color: #009900;">[</span>cv<span style="color: #009900;">[</span><span style="color: #cc66cc;">1</span>:<span style="color: #cc66cc;">300</span><span style="color: #009900;">]</span><span style="color: #339933;">,</span><span style="color: #009900;">]</span><br />valid=<a href="http://inside-r.org/r-doc/MASS/housing"><span style="color: #003399; font-weight: bold;">housing</span></a><span style="color: #009900;">[</span>cv<span style="color: #009900;">[</span><span style="color: #cc66cc;">301</span>:<span style="color: #cc66cc;">400</span><span style="color: #009900;">]</span><span style="color: #339933;">,</span><span style="color: #009900;">]</span><br />test=<a href="http://inside-r.org/r-doc/MASS/housing"><span style="color: #003399; font-weight: bold;">housing</span></a><span style="color: #009900;">[</span>cv<span style="color: #009900;">[</span><span style="color: #cc66cc;">401</span>:<a href="http://inside-r.org/r-doc/base/nrow"><span style="color: #003399; font-weight: bold;">nrow</span></a><span style="color: #009900;">(</span><a href="http://inside-r.org/r-doc/MASS/housing"><span style="color: #003399; font-weight: bold;">housing</span></a><span style="color: #009900;">)</span><span style="color: #009900;">]</span><span style="color: #339933;">,</span><span style="color: #009900;">]</span><br />&nbsp;<br />ssto=<a href="http://inside-r.org/r-doc/base/sum"><span style="color: #003399; font-weight: bold;">sum</span></a><span style="color: #009900;">(</span><span style="color: #009900;">(</span>valid$MEDV-<a href="http://inside-r.org/r-doc/base/mean"><span style="color: #003399; font-weight: bold;">mean</span></a><span style="color: #009900;">(</span>valid$MEDV<span style="color: #009900;">)</span><span style="color: #009900;">)</span>^<span style="color: #cc66cc;">2</span><span style="color: #009900;">)</span><br />evaluate &lt;- <a href="http://inside-r.org/r-doc/base/function"><span style="color: #003399; font-weight: bold;">function</span></a><span style="color: #009900;">(</span>th<span style="color: #009900;">)</span><span style="color: #009900;">{</span> <br />  num.cols=<a href="http://inside-r.org/r-doc/base/sum"><span style="color: #003399; font-weight: bold;">sum</span></a><span style="color: #009900;">(</span>th<span style="color: #009900;">)</span><br />  <span style="color: black; font-weight: bold;">if</span> <span style="color: #009900;">(</span>num.cols == <span style="color: #cc66cc;">0</span><span style="color: #009900;">)</span> <a href="http://inside-r.org/r-doc/base/return"><span style="color: #003399; font-weight: bold;">return</span></a><span style="color: #009900;">(</span><span style="color: #cc66cc;">0</span><span style="color: #009900;">)</span><br />  fo.str=<span style="color: blue;">"MEDV ~"</span><br />  cum.cols=<span style="color: #cc66cc;">0</span><br />  <span style="color: black; font-weight: bold;">for</span> <span style="color: #009900;">(</span>i <span style="color: black; font-weight: bold;">in</span> <span style="color: #cc66cc;">1</span>:<a href="http://inside-r.org/r-doc/base/length"><span style="color: #003399; font-weight: bold;">length</span></a><span style="color: #009900;">(</span>th<span style="color: #009900;">)</span><span style="color: #009900;">)</span> <span style="color: #009900;">{</span><br />    <span style="color: black; font-weight: bold;">if</span> <span style="color: #009900;">(</span>th<span style="color: #009900;">[</span>i<span style="color: #009900;">]</span>&gt;<span style="color: #cc66cc;">0</span><span style="color: #009900;">)</span> <span style="color: #009900;">{</span><br />      <span style="color: black; font-weight: bold;">if</span> <span style="color: #009900;">(</span><a href="http://inside-r.org/r-doc/base/is.factor"><span style="color: #003399; font-weight: bold;">is.factor</span></a><span style="color: #009900;">(</span>train<span style="color: #009900;">[</span><span style="color: #339933;">,</span>i<span style="color: #009900;">]</span><span style="color: #009900;">)</span><span style="color: #009900;">)</span> <span style="color: #009900;">{</span><br />        fo.str=<a href="http://inside-r.org/r-doc/base/paste"><span style="color: #003399; font-weight: bold;">paste</span></a><span style="color: #009900;">(</span>fo.str<span style="color: #339933;">,</span><a href="http://inside-r.org/r-doc/base/colnames"><span style="color: #003399; font-weight: bold;">colnames</span></a><span style="color: #009900;">(</span>train<span style="color: #009900;">)</span><span style="color: #009900;">[</span>i<span style="color: #009900;">]</span><span style="color: #339933;">,</span>sep=<span style="color: blue;">" "</span><span style="color: #009900;">)</span><br />      <span style="color: #009900;">}</span> <span style="color: black; font-weight: bold;">else</span> <span style="color: #009900;">{</span><br />        fo.str=<a href="http://inside-r.org/r-doc/base/paste"><span style="color: #003399; font-weight: bold;">paste</span></a><span style="color: #009900;">(</span>fo.str<span style="color: #339933;">,</span><span style="color: blue;">" s("</span><span style="color: #339933;">,</span><a href="http://inside-r.org/r-doc/base/colnames"><span style="color: #003399; font-weight: bold;">colnames</span></a><span style="color: #009900;">(</span>train<span style="color: #009900;">)</span><span style="color: #009900;">[</span>i<span style="color: #009900;">]</span><span style="color: #339933;">,</span><span style="color: blue;">")"</span><span style="color: #339933;">,</span>sep=<span style="color: blue;">""</span><span style="color: #009900;">)</span><br />      <span style="color: #009900;">}</span><br />      cum.cols=cum.cols+<span style="color: #cc66cc;">1</span><br />      <span style="color: black; font-weight: bold;">if</span> <span style="color: #009900;">(</span>cum.cols&lt;num.cols<span style="color: #009900;">)</span> <span style="color: #009900;">{</span><br />        fo.str=<a href="http://inside-r.org/r-doc/base/paste"><span style="color: #003399; font-weight: bold;">paste</span></a><span style="color: #009900;">(</span>fo.str<span style="color: #339933;">,</span><span style="color: blue;">"+"</span><span style="color: #009900;">)</span><br />      <span style="color: #009900;">}</span><br />    <span style="color: #009900;">}</span><br />  <span style="color: #009900;">}</span><br /><span style="color: #666666; font-style: italic;">#   colnames(train)[c(th,0)==1]</span><br />  fo=<a href="http://inside-r.org/r-doc/stats/as.formula"><span style="color: #003399; font-weight: bold;">as.formula</span></a><span style="color: #009900;">(</span>fo.str<span style="color: #009900;">)</span><br />  gam1 &lt;- <a href="http://inside-r.org/r-doc/mgcv/gam"><span style="color: #003399; font-weight: bold;">gam</span></a><span style="color: #009900;">(</span>fo<span style="color: #339933;">,</span><a href="http://inside-r.org/r-doc/utils/data"><span style="color: #003399; font-weight: bold;">data</span></a>=train<span style="color: #009900;">)</span><br />  pred1 &lt;- <a href="http://inside-r.org/r-doc/stats/predict"><span style="color: #003399; font-weight: bold;">predict</span></a><span style="color: #009900;">(</span>gam1<span style="color: #339933;">,</span>valid<span style="color: #339933;">,</span>se=<span style="color: black; font-weight: bold;">FALSE</span><span style="color: #009900;">)</span><br />  sse &lt;- <a href="http://inside-r.org/r-doc/base/sum"><span style="color: #003399; font-weight: bold;">sum</span></a><span style="color: #009900;">(</span><span style="color: #009900;">(</span>pred1-valid$MEDV<span style="color: #009900;">)</span>^<span style="color: #cc66cc;">2</span><span style="color: #339933;">,</span>na.rm=<span style="color: black; font-weight: bold;">TRUE</span><span style="color: #009900;">)</span><br />  <a href="http://inside-r.org/r-doc/base/return"><span style="color: #003399; font-weight: bold;">return</span></a><span style="color: #009900;">(</span><span style="color: #cc66cc;">1</span>-sse/ssto<span style="color: #009900;">)</span><br /><span style="color: #009900;">}</span><br />&nbsp;<br />res &lt;- tabuSearch<span style="color: #009900;">(</span>size = <a href="http://inside-r.org/r-doc/base/ncol"><span style="color: #003399; font-weight: bold;">ncol</span></a><span style="color: #009900;">(</span>train<span style="color: #009900;">)</span>-<span style="color: #cc66cc;">1</span><span style="color: #339933;">,</span> iters = <span style="color: #cc66cc;">20</span><span style="color: #339933;">,</span> objFunc = evaluate<span style="color: #339933;">,</span> listSize = <span style="color: #cc66cc;">5</span><span style="color: #339933;">,</span><br />                  config = <a href="http://inside-r.org/r-doc/stats/rbinom"><span style="color: #003399; font-weight: bold;">rbinom</span></a><span style="color: #009900;">(</span><a href="http://inside-r.org/r-doc/base/ncol"><span style="color: #003399; font-weight: bold;">ncol</span></a><span style="color: #009900;">(</span>train<span style="color: #009900;">)</span>-<span style="color: #cc66cc;">1</span><span style="color: #339933;">,</span><span style="color: #cc66cc;">1</span><span style="color: #339933;">,</span><span style="color: #cc66cc;">.5</span><span style="color: #009900;">)</span><span style="color: #339933;">,</span> nRestarts = <span style="color: #cc66cc;">4</span><span style="color: #339933;">,</span>verbose=<span style="color: black; font-weight: bold;">TRUE</span><span style="color: #009900;">)</span></pre></div></div><br />It was able to find a subset with a 0.8678 R-squared on the validation set (and 0.8349 on the test set). The formula found was:<br /><span class="Apple-style-span" style="-webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; background-color: #e1e2e5; border-collapse: separate; color: black; font-family: 'Lucida Console'; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 15px; orphans: 2; text-align: -webkit-left; text-indent: 0px; text-transform: none; white-space: pre-wrap; widows: 2; word-spacing: 0px;"></span><br /><pre class="GDXA2EVBEAB" style="border-bottom-style: none; border-color: initial; border-left-style: none; border-right-style: none; border-top-style: none; border-width: initial; font-family: 'Lucida Console'; font-size: 10pt !important; line-height: 1.2; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px; outline-color: initial; outline-style: none; outline-width: initial; white-space: pre-wrap !important;" tabindex="0">MEDV ~ s(CRIM) + s(INDUS) + s(NOX) + s(RM) + s(DIS) + RAD + s(TAX) + s(PTRATIO) + s(LSTAT).</pre><br /><br /><h3>Visualizing the results</h3>The tabu search gave me a subset which it thinks is best. But I would like to get a better handle on how it derived it, or if there were lots of models with similar quality, but different variables. Or if there were variables that were always in the top performing models. I created a heat plot that showed whether or not a variable was in the regression or not at each iteration.<br /><br />Below you can see which variables were included in each iteration. There are a few variables that seem to be more important because they are included in almost every iteration, like LSAT, PTRATIO,and RM. But this doesn&#8217;t tell me which iterations were the best.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-h7QkQGRQv7c/UDZm5Lfp40I/AAAAAAAAHKY/EDABwfjZ5R8/s1600/tabu1.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-h7QkQGRQv7c/UDZm5Lfp40I/AAAAAAAAHKY/EDABwfjZ5R8/s1600/tabu1.png" /></a></div><br />In the chart below, I shaded each region by the ranking of model in that iteration. The higher ranking mean it did better. It is not easy, but we can glean a little more information from this chart. The models with RAD and DIS do significantly better than the models without them, even though they are not in every iteration. Further, the models with AGE seem a bit worse than those without it.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-TJKQO3ItVkQ/UDZm5o2REyI/AAAAAAAAHKg/UqqyqZ6EELs/s1600/tabu2.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-TJKQO3ItVkQ/UDZm5o2REyI/AAAAAAAAHKg/UqqyqZ6EELs/s1600/tabu2.png" /></a></div><br />The R code to make these plots is below.<br /><br /><div style="overflow: auto;"><div class="geshifilter"><pre class="r geshifilter-R" style="font-family: monospace;"><a href="http://inside-r.org/r-doc/base/library"><span style="color: #003399; font-weight: bold;">library</span></a><span style="color: #009900;">(</span><a href="http://inside-r.org/r-doc/stats/reshape"><span style="color: #003399; font-weight: bold;">reshape</span></a><span style="color: #009900;">)</span><br /><a href="http://inside-r.org/r-doc/base/library"><span style="color: #003399; font-weight: bold;">library</span></a><span style="color: #009900;">(</span><a href="http://inside-r.org/packages/cran/ggplot2">ggplot2</a><span style="color: #009900;">)</span><span style="color: #339933;">;</span> theme_set<span style="color: #009900;">(</span>theme_bw<span style="color: #009900;">(</span><span style="color: #009900;">)</span><span style="color: #009900;">)</span><br />&nbsp;<br />tabu.df=<a href="http://inside-r.org/r-doc/base/data.frame"><span style="color: #003399; font-weight: bold;">data.frame</span></a><span style="color: #009900;">(</span>res$configKeep<span style="color: #009900;">)</span><br /><a href="http://inside-r.org/r-doc/base/colnames"><span style="color: #003399; font-weight: bold;">colnames</span></a><span style="color: #009900;">(</span>tabu.df<span style="color: #009900;">)</span>=<a href="http://inside-r.org/r-doc/base/colnames"><span style="color: #003399; font-weight: bold;">colnames</span></a><span style="color: #009900;">(</span>train<span style="color: #009900;">)</span><span style="color: #009900;">[</span><span style="color: #cc66cc;">1</span>:<span style="color: #009900;">(</span><a href="http://inside-r.org/r-doc/base/ncol"><span style="color: #003399; font-weight: bold;">ncol</span></a><span style="color: #009900;">(</span>train<span style="color: #009900;">)</span>-<span style="color: #cc66cc;">1</span><span style="color: #009900;">)</span><span style="color: #009900;">]</span><br />tabu.df$Iteration=<span style="color: #cc66cc;">1</span>:<a href="http://inside-r.org/r-doc/base/nrow"><span style="color: #003399; font-weight: bold;">nrow</span></a><span style="color: #009900;">(</span>tabu.df<span style="color: #009900;">)</span><br />tabu.df$RSquared=res$eUtilityKeep<br />tabu.df$Rank=<a href="http://inside-r.org/r-doc/base/rank"><span style="color: #003399; font-weight: bold;">rank</span></a><span style="color: #009900;">(</span>tabu.df$RSquared<span style="color: #009900;">)</span><br />tabu.melt=melt<span style="color: #009900;">(</span>tabu.df<span style="color: #339933;">,</span>id=<a href="http://inside-r.org/r-doc/base/c"><span style="color: #003399; font-weight: bold;">c</span></a><span style="color: #009900;">(</span><span style="color: blue;">"Iteration"</span><span style="color: #339933;">,</span><span style="color: blue;">"RSquared"</span><span style="color: #339933;">,</span><span style="color: blue;">"Rank"</span><span style="color: #009900;">)</span><span style="color: #009900;">)</span><br />tabu.melt$RSquared=<a href="http://inside-r.org/r-doc/base/ifelse"><span style="color: #003399; font-weight: bold;">ifelse</span></a><span style="color: #009900;">(</span>tabu.melt$value==<span style="color: #cc66cc;">1</span><span style="color: #339933;">,</span>tabu.melt$RSquared<span style="color: #339933;">,</span><span style="color: #cc66cc;">0</span><span style="color: #009900;">)</span><br />tabu.melt$Rank=<a href="http://inside-r.org/r-doc/base/ifelse"><span style="color: #003399; font-weight: bold;">ifelse</span></a><span style="color: #009900;">(</span>tabu.melt$value==<span style="color: #cc66cc;">1</span><span style="color: #339933;">,</span>tabu.melt$Rank<span style="color: #339933;">,</span><span style="color: #cc66cc;">0</span><span style="color: #009900;">)</span><br /><span style="color: #009900;">(</span>pHeat01 &lt;- <a href="http://inside-r.org/packages/cran/ggplot">ggplot</a><span style="color: #009900;">(</span>tabu.melt<span style="color: #339933;">,</span> aes<span style="color: #009900;">(</span>Iteration<span style="color: #339933;">,</span>variable<span style="color: #009900;">)</span><span style="color: #009900;">)</span> + geom_tile<span style="color: #009900;">(</span>aes<span style="color: #009900;">(</span>fill = value<span style="color: #009900;">)</span><span style="color: #009900;">)</span> +<br />  scale_fill_gradient<span style="color: #009900;">(</span>low = <span style="color: blue;">"white"</span><span style="color: #339933;">,</span> high = <span style="color: blue;">"steelblue"</span><span style="color: #339933;">,</span>guide=<span style="color: black; font-weight: bold;">FALSE</span><span style="color: #009900;">)</span><span style="color: #009900;">)</span><br /><span style="color: #009900;">(</span>pHeatRank &lt;- <a href="http://inside-r.org/packages/cran/ggplot">ggplot</a><span style="color: #009900;">(</span>tabu.melt<span style="color: #339933;">,</span> aes<span style="color: #009900;">(</span>Iteration<span style="color: #339933;">,</span>variable<span style="color: #009900;">)</span><span style="color: #009900;">)</span> + geom_tile<span style="color: #009900;">(</span>aes<span style="color: #009900;">(</span>fill = Rank<span style="color: #009900;">)</span><span style="color: #009900;">)</span> +<br />  scale_fill_gradient<span style="color: #009900;">(</span>low = <span style="color: blue;">"white"</span><span style="color: #339933;">,</span> high = <span style="color: blue;">"steelblue"</span><span style="color: #009900;">)</span><span style="color: #009900;">)</span></pre></div></div><a href="http://www.inside-r.org/pretty-r" title="Created by Pretty R at inside-R.org">Created by Pretty R at inside-R.org</a></div>
<h2>Comments</h2>
<div class='comments'>
<div class='comment'>
<div class='author'>anonymous</div>
<div class='content'>
Good post!<br /><br />Nice approach with the heat map to go beyond just accepting the optimal solution. I agree that variables seem to be important because they are present in every iteration, but this could perhaps also mean that the tabu search was trapped in a subregion - perhaps the search move operator which only adds or deletes a single variable is not powerful enough to escape this region. Some possible remedies for exploring a larger part of the search landscape (if indeed this is a problem) might be:<br /><br />- Multiple tabu searches, each with a randomized set of initial variables in the model<br />- Other move operators. How about finding correlation clusters among the variables, then let a move add or delete the entire cluster? Both move operators can be used in the search, using some metaheuristics to guide them.<br />- Use metaheuristic techniques like &quot;ruin and recreate&quot; during the tabu search; occasionally, &quot;ruin&quot; the model by removing a random subset of the variables.<br /><br />One more thought. With the tabu search already in place for regression, it wouldnÂ´t be too hard to explore some nonlinear models. You could create temp variables that are products of the original variables, then let the tabu search add or delete these as well. Do you think such an approach would have any merit in this case?<br /></div>
</div>
<div class='comment'>
<div class='author'>Andrew</div>
<div class='content'>
@Anonymous I would also like to see how this does compared to the more standard methods. It will take a little effort to implement in this example, though, because these variable selection techniques are not implemented for GAMs (as far as I know). http://www.statmethods.net/stats/regression.html has a nice summary of different methods available for linear models. You are right that the whole point of using a TS is that it will be better than the other methods, so it would be good to have validation. Maybe I could do a comparison for linear models&#8230;<br /><br />I described the meat of the TS algorithm in the post. If you want more info, I found this reference informative http://www.math.ntua.gr/~fouskakis/Publications/4.pdf </div>
</div>
<div class='comment'>
<div class='author'>Andrew</div>
<div class='content'>
@anspiess That is probably a good idea. But one reason to use Tabu Search is that it will speed up the time to find the best model. Running it many times will defeat this purpose. Running time for the parameters used was long already because GAMs take longer to train than LMs (~ 9 mins). In terms of better understanding TS, your idea would be useful. For what its worth, I ran it again with a different CV split and the chosen model was the same except it added s(ZN) and removed s(PTRATIO).</div>
</div>
<div class='comment'>
<div class='author'>anspiess</div>
<div class='content'>
Might it be feasible to sample training and test set a few hundred times to see how stable variable selection is?</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
I wonder how this method is better than using F test, t test or information criterion? did you compare tabusearch to other methods (for linear models)? maybe it its faster than standard methods? I would be grateful for answers:)  if it its possible maybe you can describe this algorithm more closely.</div>
</div>
</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/08/10/a-matrix-factorization-model-for/">A Matrix Factorization Model for Hitter/Pitcher Matchups</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-08-10T00:00:00-04:00" pubdate data-updated="true">Aug 10<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
<h3>Introduction </h3>Matrix factorization has been proven to be one of the best ways to do collaborative filtering. The most common example of collaborative filtering is to predict how much a viewer will like a movie. The power of matrix factorization was a key development of the Netflix Prize (see <a href="http://www2.research.att.com/%7Evolinsky/papers/ieeecomputer.pdf">http://www2.research.att.com/~volinsky/papers/ieeecomputer.pdf</a>).<br /><br />Using the movie rating example, the idea is that there are some underlying features of the movie and underlying attributes of the user that interact to determine if the user will like the movie. So if the user typically likes comedies, and the movie is a comedy, these will interact and the user will like the movie. The best part is that in order to determine these features, we do not need to use any background information on the movie or the user. For example, we don&#8217;t need someone to manually code whether each movie is a comedy or not. Matrix factorization just needs a set of historical ratings, which is usually easily available.<br /><br />Using only a few features, we could probably interpret what each of them mean (comedy, drama, foreign, blockbuster, &#8230;). They found in the Netflix competition that increasing the number of underlying features that make up a user and a movie almost always improved accuracy (they used up to 1500 features). With that many features, they can no longer be interpreted as in the example.<br /><br /><h3>Baseball Data</h3>I find this method interesting, and I wanted to apply it to some real data. One idea I had is predicting whether the result of a pitcher-batter match-up in baseball. Usually in baseball, the analysts cry &#8220;small sample size&#8221; if someone reports that a pitcher is particularly good/bad versus a batter. In order to get an accurate assessment that does not suffer from random variation, we need 100 or more at bats, which usually takes a few years at least.<br /><br />It seems that matrix factorization might be a natural fit, because we are not predicting the specific interaction of the batter/pitcher. Instead, we are estimating some underlying features of the batter and the pitcher. We can estimate these features from all of the match-ups that the batter and pitcher have been in &#8211; not just the ones against each other.<br /><br />One example of a feature that I hoped to extract is batter and pitcher handedness. It is well known that batters fair poorly against pitchers of the same handedness and do better against pitchers of the opposite handedness. Other examples might relate to whether the pitcher can throw fast and how well the batter does against fast pitches; similarly with breaking balls, etc.<br /><br />In the Netflix competition, each user rates movies on a 1 to 5 star scale, and rates the movie only once. The outcome of a match-up is different as there is no obvious &#8220;rating&#8221; of the outcome. I used improvement in expected number of runs scored of the outcome of a plate appearance as the &#8220;rating&#8221; to be predicted (from <a href="http://www.tangotiger.net/RE9902event.html">here</a>). So a single is worth 0.474 runs and an out is worth -0.299 runs. Also, in baseball, there will be many match-ups between the batter and pitcher. I could have averaged the outcomes, but I left the multiple records in the data as is.<br /><br />I used 2010 data and removed all pitchers that were hitting and batters that were pitching. According to my data, there were 65,128 unique batter-pitcher match-ups (with about 173,000 plate appearances). I used 5,000 match-ups as validation, 5,000 as testing, and the remaining as training.<br /><br /><h3>Model Fitting</h3>The first model I used as a baseline just included &#8220;biases&#8221; for the batter and pitcher, as well as an overall league average bias. In the baseline, there are no factors or interactions. So the result of the match-up between a batter i and pitcher j would be estimated as the overall average bias plus the batter i bias plus the pitcher j bias. In math terms:<br /><br /><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cdpi%7B120%7D%20%5Cmu@plus;b_i@plus;p_j" target="_blank"><img src="http://latex.codecogs.com/png.latex?%5Cdpi%7B120%7D%20%5Cmu+b_i+p_j" title="\dpi{120} \mu+b_i+p_j" /></a><br /><br />Higher values for the batter bias mean the batter is better better than average and lower values for the pitcher bias means the pitcher is better than average.<br /><br />So as to not overfit the data, I used L2 regularization on the batter and pitcher biases. For this and future estimation, I used the validation data to determine the best penalty and I used a gradient descent method to solve for the parameters.<br /><br />Adding in the factor interaction, the result of the match-up between a batter i and pitcher j is<br /><br /><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cdpi%7B120%7D%20%5Cmu@plus;b_i@plus;p_j%20@plus;%20%5Cboldsymbol%7Bo%7D_i%5ET%20%5Cboldsymbol%7Bd%7D_j" target="_blank"><img src="http://latex.codecogs.com/png.latex?%5Cdpi%7B120%7D%20%5Cmu+b_i+p_j%20+%20%5Cboldsymbol%7Bo%7D_i%5ET%20%5Cboldsymbol%7Bd%7D_j" title="\dpi{120} \mu+b_i+p_j + \boldsymbol{o}_i^T \boldsymbol{d}_j" /></a><br /><br />where <a href="http://www.codecogs.com/eqnedit.php?latex=%5Cdpi%7B120%7D%20%5Cboldsymbol%7Bo%7D_i" target="_blank"><img src="http://latex.codecogs.com/png.latex?%5Cdpi%7B120%7D%20%5Cboldsymbol%7Bo%7D_i" title="\dpi{120} \boldsymbol{o}_i" /></a> is a vector of offensive features for batter i and <a href="http://www.codecogs.com/eqnedit.php?latex=%5Cdpi%7B120%7D%20%5Cboldsymbol%7Bd%7D_j" target="_blank"><img src="http://latex.codecogs.com/png.latex?%5Cdpi%7B120%7D%20%5Cboldsymbol%7Bd%7D_j" title="\dpi{120} \boldsymbol{d}_j" /></a> is a set of defensive features for pitcher j. The two vectors must be of the same length which we can determine. Again, I used L2 regularization on the feature vectors.<br /><br /><h3>Results</h3>I fit the matrix factorization model using 0, 1, 2, 5, and 10 features. The model with 0 features corresponds to the baseline model. As stated above, I was hoping to see that handedness would show up as one of the first features.<br /><br />I fit the models with the training set, tuned the regularization parameters with the validation set, and finally I am comparing the models with the test set. Unfortunately, the baseline model performed the best of all and the performance degraded as more features were added. A plot of the test mean squared errors is below. The difference between the models is small, but I believe that is because the regularization parameter is shrinking the factor vectors to 0.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-8doS-OZmBzs/UCUmUQWdo_I/AAAAAAAAHKA/CmE1rs9YYOc/s1600/MatFactTestMSE.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-8doS-OZmBzs/UCUmUQWdo_I/AAAAAAAAHKA/CmE1rs9YYOc/s1600/MatFactTestMSE.png" /></a></div>A further baseline model could have been used &#8211; an overall mean with no biases. The MSE for all but the 10-factor  model were better than this average. So the biases help, but unfortunately the factorization did not.<br /><br />I compared the factor vectors of the pitchers by their handedness and I found no difference between righties and lefties.<br /><br /><h3>Summary</h3>The outcome of this analysis was not what I hoped for, but I still learned something about matrix factorization and its implementation. I think the moral of the story is that it is hard to predict batter-pitcher match-ups, probably because there is a lot of variation in every at bat. Matrix factorization has been successfully applied in situations where the data is much more sparse than this application, so this difference may be a reason for its failure. I plan to try this again with the <a href="http://www.libfm.org/">libFM</a> software to see if I get the same results.<br /><br /><i><b>Update:</b></i><br />I implemented this in libFM and basically got the same results, so I guess I was doing it correctly. If I work on this further, I wonder if adding attributes about the pitcher explicitly would help. I would include an indicator for righty/lefty or maybe home/away.</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/07/20/the-magical-sparse-matrix/">The Magical Sparse Matrix</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-07-20T00:00:00-04:00" pubdate data-updated="true">Jul 20<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
I have been toying around with Kaggle&#8217;s <a href="http://www.kaggle.com/c/msdchallenge/">Million Song Dataset Challenge</a> recently because I have some interest in <a href="http://en.wikipedia.org/wiki/Collaborative_filtering">collaborative filtering</a> (using <a href="http://www2.research.att.com/%7Evolinsky/papers/ieeecomputer.pdf" target="_blank">matrix factorization</a>). I haven&#8217;t made much progress with the competition (all 3 of my submissions are below the baseline), but I have learned a few things about dealing with large amounts of data.<br /><br />The goal of the competition is to predict the 500 most likely songs each of 110,000 users will listen to next. As the name implies, there are 1,000,000 songs in the full dataset. To simplify things, I decided to concentrate on the most popular songs. I created a 110,000 x 2,000 matrix of 0&#8217;s and 1&#8217;s. Row i, column j is 1 if user i had listened to song j (the jth most popular song) and 0 if user i had not. As you can imagine, there are a lot more 0&#8217;s than 1&#8217;s in this matrix. The first few rows and columns look like this:<br /><br /><div style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: small;">0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 &#8230;</span></div><div style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: small;">0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 &#8230;</span></div><div style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: small;">1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 &#8230;</span></div><div style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: small;">&#8230;</span></div><br />This matrix was about 430 Mb and took a while to load into MATLAB. So I wisened up and created a <a href="http://en.wikipedia.org/wiki/Sparse_matrix">sparse matrix</a>. A sparse matrix realizes that most of the values are 0&#8217;s and does not record them. Instead, it lists the locations of the non-zero elements and what the value is. For example, this is what the first few rows of the sparse matrix looks like:<br /><br /><div style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: small;">1 3 1<br />1 7 1<br />1 10 1<br />1 13 1<br />1 82 1<br />1 717 1<br />2 1111 1<br />2 2972 1<br />2 3516 1</span></div><div style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: small;">&#8230;</span></div><br />The first column is the row number, the second is the column number, and the third is the value at that location. In this application, all the values are 1. For this matrix, I used the 50,000 most popular songs (instead of just 2,000), and the size was much smaller &#8211; just 17 Mb.<br /><br />It is easy to load the sparse matrix into MATLAB with the <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">spconvert</span> command, and many of MATLAB&#8217;s functions (like singular value decomposition) are optimized for sparse matrices.</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/07/19/random-forest-variable-importance/">Random Forest Variable Importance</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-07-19T00:00:00-04:00" pubdate data-updated="true">Jul 19<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
<a href="http://en.wikipedia.org/wiki/Random_forest">Random</a> <a href="http://www.stat.berkeley.edu/%7Ebreiman/RandomForests/">forests</a> â¢ are great. They are one of the best &#8220;black-box&#8221; supervised learning methods. If you have lots of data and lots of predictor variables, you can do worse than random forests. They can deal with messy, real data. If there are lots of extraneous predictors, it has no problem. It automatically does a good job of finding interactions as well. There are no assumptions that the response has a linear (or even smooth) relationship with the predictors.<br /><br />As cited in the Wikipedia article, they do lack some interpretability. But what they lack in interpretation, they more than make up for in prediction power, which I believe is much more important than interpretation in most cases. Even though you cannot easily tell how one variable affects the prediction, you can easily create a <i>partial dependence plot</i> which shows how the response will change as you change the predictor. You can also do this for two variables at once to see the interaction of the two.<br /><br />Also helping in the interpretation is that they can output a list of predictor variables that they believe to be important in predicting the outcome. If nothing else, you can subset the data to only include the most &#8220;important&#8221; variables, and use that with another model. The randomForest package in R has two measures of importance. <a href="http://rss.acs.unt.edu/Rdoc/library/randomForest/html/importance.html">One </a>is &#8220;total decrease in node impurities from splitting on the variable, averaged over all trees.&#8221; I do not know much about this one, and will not talk about it further. The other is based on a <a href="http://en.wikipedia.org/wiki/Resampling_%28statistics%29#Permutation_tests">permutation test</a>. The idea is that if the variable is not important (the null hypothesis), then rearranging the values of that variable will not degrade prediction accuracy. Random forests use out-of-bag (OOB) samples to measure prediction accuracy.<br /><br />In my experience, it does a pretty good job of finding the most important predictors, but it has issues with correlated predictors. For example, I was working on a problem where I was predicting the price that electricity trades. One feature that I knew would be very important was the amount of electricity being used at that same time. But I thought there might also be a relationship between price and the electricity being used a few hours before and after. When I ran the random forest with these variables, the electricity used 1 hour after was found to be more important than the electricity used at the same time. When including the 1 hour after electricity use instead of the current hour electricity use, the cross validation (CV) error increased. Using both did not significantly change the CV error compared to using just current hour. Because the electricity used at the current hour and the hour after are very correlated, it had trouble telling which one was more important. In truth, given the electricity use at the current hour, the electricity use at the hour after did not improve the predictive accuracy.<br /><br />Why does the importance measure give more weight correlated predictors? <a href="http://www.biomedcentral.com/1471-2105/9/307/">Strobl et al.</a> give some intuition behind what is happening and propose a solution. Basically, the permutation test is ill-posed. The permutation test is testing that the variable is independent of the response as well as all other predictors. Since the correlated predictors are obviously not independent, we get high importance scores. They propose a permutation test where you condition on the correlated predictors. This is a little tricky when the correlated predictors are continuous, but you can read the paper for more details.<br /><br />Another way to think of it is that, since each split only considers a subset of the possible variables, a variable that is correlated with an &#8220;important&#8221; variable may be considered without the &#8220;important&#8221; variable. This would cause the correlated variable to be selected for the split. The correlated value does hold some predictive value, but only because of the truly important variable, so it is understandable why this procedure would consider it important.<br /><br />I ran a simulation experiment (similar to the one in the paper) to demonstrate the issue. I simulated 5 predictor variables. Only the first one is related to the response, but the second one has a correlation of about 0.7 with the first one. Luckily, Strobl et al. included their version of importance in the package <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">party</span> in <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">R</span>. I compare variable importance from the <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">randomForest </span>package and the importance with and without taking correlated predictors into account from the <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">party<span style="font-family: inherit;"> </span></span><span style="font-family: inherit;">pac</span>kage.<br /><br /><pre class="brush: r"># simulate the data<br />x1=rnorm(1000)<br />x2=rnorm(1000,x1,1)<br />y=2*x1+rnorm(1000,0,.5)<br />df=data.frame(y,x1,x2,x3=rnorm(1000),x4=rnorm(1000),x5=rnorm(1000))<br /><br /># run the randomForest implementation<br />library(randomForest)<br />rf1 &lt;- randomForest(y~., data=df, mtry=2, ntree=50, importance=TRUE)<br />importance(rf1,type=1)<br /><br /># run the party implementation<br />library(party)<br />cf1 &lt;- cforest(y~.,data=df,control=cforest_unbiased(mtry=2,ntree=50))<br />varimp(cf1)<br />varimp(cf1,conditional=TRUE)<br /></pre><br />For the <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">randomForest</span>, the ratio of importance of the the first and second variable is 4.53. For <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">party</span> without accounting for correlation it is 7.35. And accounting for correlation, it is 369.5. The higher ratios are better because it means that the importance of the first variable is more prominent. party&#8217;s implementation is clearly doing the job.<br /><br />There is a downside. It takes much longer to calculate importance with correlated predictors than without. For the party package in this example, it took 0.39 seconds to run without and 204.34 seconds with. I could not even run the correlated importance on the electricity price example. There might be a research opportunity to get a quicker approximation.<br /><br />Possibly up next: confidence limits for random forest predictions.</div>
<h2>Comments</h2>
<div class='comments'>
<div class='comment'>
<div class='author'>Abhijeet Patil</div>
<div class='content'>
hi,m new to random forest.. i ve some doubts..<br />1)&quot;we need to choose m number of variables randomly for each node..&quot;-can u pleas explain it..<br />OR<br />2)&quot;take 1 bootstrap sample,choose some variables and create a decision tree&quot;-is it correct??<br /></div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
Could you tell me what the numbers mean when ctree and randonForest return variable importance numbers please? I can get the output but I don&#39;t know how to interpret them.<br /><br />Than you very much.</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
HI Andrew, When i tried to calculate the variable importance, I get the error <br />&quot;Error in model.matrix.default(as.formula(f), data = blocks) : <br />  allocMatrix: too many elements specified&quot;<br /><br />I&#39;m not sure how to deal with it because I have 2 factor levels which I&#39;m predicting with 23 continuous variable predictors (about 2200 data points. I know it&#39;s a lot to work with, but not sure how to get the correct VarImp values with this large data set.<br /><br />I tried increasing the threshold, but it didn&#39;t help. fit.varimp=varimp(fit.cf,threshold=0.8,conditional=TRUE)<br /><br />Do you have any suggestions? Thanks.</div>
</div>
<div class='comment'>
<div class='author'>Andrew Landgraf</div>
<div class='content'>
Unfortunately, I didn&#39;t set the seed. #nexttime</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
you should provide the seed to reproduce your example</div>
</div>
<div class='comment'>
<div class='author'>Andrew</div>
<div class='content'>
It should be able to give you a solution. I have no idea what ramifications this would have on accuracy or variable importance.</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
I have microarray data on 30 entries and thought of using RF to identify important response genes. I understand that this is a N&lt;&lt;p type of problems, but will &#39;party&#39; be able to come up with a solution?</div>
</div>
<div class='comment'>
<div class='author'>Andrew</div>
<div class='content'>
The simple answer is using the partialPlot function in the randomForest package for R (http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=randomForest:partialPlot). <br /><br />What it is doing isn&#39;t all that complicated. Say you want a partial dependence plot for the variable X_1. For each value of X_1=x that you want to plot, you take the average of the prediction with X_1=x and the other explanatory variables equal to the n values that they are in the data set. You are trying to average out the other variables.</div>
</div>
<div class='comment'>
<div class='author'>nanounanue</div>
<div class='content'>
Great explanation!<br /><br />I have a question for you: In your text you said:<br /><br />&quot;&quot;&quot;<br /> &#8230; Even though you cannot easily tell how one variable affects the prediction, you can easily create a partial dependence plot which shows how the response will change as you change the predictor. You can also do this for two variables at once to see the interaction of the two.<br />&quot;&quot;&quot;<br /><br />Could you provide, please, an example of how build the partial dependence plot?<br /><br /><br />Thank you for the post and thanks in advance</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
Great post!  Thank you.</div>
</div>
</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/06/15/rounding-in-r/">Rounding in R</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-06-15T00:00:00-04:00" pubdate data-updated="true">Jun 15<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
Forgive me if you are already aware of this, but I found it quite alarming. I know that most code is interpreted by the computer in binary and we input in decimal, so problems can arise in conversion and with <a href="http://en.wikipedia.org/wiki/Floating_point">floating point</a>. But the example I have below is so simple that it really surprised me.<br /><br />I was converting a function from R into MATLAB so that a colleague could use it. I tested it out on the same data and got slightly different results. Digging into the problem, the difference was due to the fact that R was rounding 4.5 to 4 and MATLAB was rounding it to 5. I thought the &#8220;4.5&#8221; must have really been &#8220;4.49999&#8230;&#8221;. But that was not so.<br /><br />For example, this is the result of the <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/Round.html">round</a> function for a few numbers.<br /><pre class="brush: r">&gt; round(0.5,0)<br />[1] 0<br />&gt; round(1.5,0)<br />[1] 2<br />&gt; round(2.5,0)<br />[1] 2<br />&gt; round(3.5,0)<br />[1] 4<br />&gt; round(4.5,0)<br />[1] 4<br />&gt; round(5.5,0)<br />[1] 6<br />&gt; round(6.5,0)<br />[1] 6<br /></pre><br />Do you see a pattern?<br /><br />I tried this on versions 2.13.1 and 2.14.0. I ran the same with MATLAB and it gave the expected results. I am not any kind of expert on computer sciences, so I was not sure why this is happening. <a href="http://www.mathsisfun.com/binary-decimal-hexadecimal-converter.html">Converting </a>any decimal number that ends in .5 into binary results in a finite length binary number. For example, 4.5 is 100.1 in binary. Because of this, I wouldn&#8217;t think the error would be due to floating points, but I really don&#8217;t know.<br /><br />Looking at the <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/Round.html">documentation</a> for round, I found the reason. It states in the notes, &#8220;Note that for rounding off a 5, the <a href="http://en.wikipedia.org/wiki/IEEE_754-2008#Rounding_rules">IEC 60559 standard</a> is expected to be used, â<em>go to the even digit</em>â.&#8221; It is a little comforting knowing that there is a logic behind it and that R is abiding to some standard. But why isn&#8217;t MATLAB abiding by the same standard? Also, I think most people expect numbers ending in .5 to round up, not the nearest even digit.</div>
<h2>Comments</h2>
<div class='comments'>
<div class='comment'>
<div class='author'>Analytic Bastard</div>
<div class='content'>
kudos Blaise</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
Andrew wrote &quot;Also, I think most people expect numbers ending in .5 to round up (not the nearest even digit)&quot;. This kind of rounding is in German called &quot;kaufmÃ¤nnische Rundung&quot; (rounding in commerce). For this purpose I use the following function:<br /><br />#Definition of a function for &quot;rounding in commerce&quot;<br />cround = function(x,n){<br />vorz = sign(x)<br />z = abs(x)*10^n<br />z = z + 0.5<br />z = trunc(z)<br />z = z/10^n<br />z*vorz<br />}<br /><br /># Example<br />&gt; round(seq(0.5,6.5,1),0)<br />[1] 0 2 2 4 4 6 6<br />&gt; cround(seq(0.5,6.5,1),0)<br />[1] 1 2 3 4 5 6 7</div>
</div>
<div class='comment'>
<div class='author'>cellocgw</div>
<div class='content'>
This &quot;round to even&quot; approach has been accepted by just about everyone (except matlab, and no surprise, except Msoft Excel).  <br />Sadly, the flame wars over &quot;round to even&quot; vs. &quot;round up&quot; continue, rather the way people argue about &quot;0.999&#8230; != 1&quot;<br /><br />PS: @a Tom:  I&#39;m highly skeptical of your <br />claim about 2.46&#8211;&gt;3.  Do you have a citation?</div>
</div>
<div class='comment'>
<div class='author'>a Tom</div>
<div class='content'>
I&#39;m ever amazed that something so seemingly basic can have so many different approaches.<br /><br />I understand that in many middle east countries they start with the far right digit and round up or down, so 2.46 is rounded to 3!</div>
</div>
<div class='comment'>
<div class='author'>Blaise</div>
<div class='content'>
This is discussed in Don Knuth&#39;s 1973 classic Seminumerical Algorithms. He gives the following example of what can happen when 5s are always rounded upwards. Suppose u = 10000000 and v = 0.5555556. Then u + v = 1.5555556. If we subtract v from this  result we get u&#39; = 1.0000001. Adding and then subtracting v from u&#39; and we get 1.0000002 and if we do it again we get 1.0000003 and so on. He says &quot;This phenomenon, called drift, will not occur when we use a stable rounding rule based on the parity of the least significant digit.&quot;</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
I was the #2 anonymous poster. Echoing Ben, I think that for ease of teaching, the &quot;round 5 up&quot; method is taught to children (and adults?) below the university level, and only if you go on for advance work is the more complicated method taught.<br /><br />Can you imagine trying to teach a 10 or 12 year old the IEC 60559 standard? Unfortunately, this is the method most adults are used to&#8230;<br /><br />I agree, it is a little troubling that Matlab doesn&#39;t abide by the standard. Yet another reason to stick with R!</div>
</div>
<div class='comment'>
<div class='author'>Ben Bolker</div>
<div class='content'>
Wikipedia ( http://en.wikipedia.org/wiki/Rounding#Round_half_to_even ) says of round-to-even:<br /><br />This method also treats positive and negative values symmetrically, and therefore is free of overall bias if the original numbers are positive or negative with equal probability. In addition, for most reasonable distributions of y values, the expected (average) value of the rounded numbers is essentially the same as that of the original numbers, even if the latter are all positive (or all negative). However, this rule will still introduce a positive bias for even numbers (including zero), and a negative bias for the odd ones.<br /><br />So round-to-even seems to have *slightly* better numerical properties than &quot;round ties away from zero&quot;, which is what is (I think) most often taught, because it&#39;s easier to understand. http://www.mathworks.com/matlabcentral/fileexchange/6752 gives a MATLAB function for &quot;round to even&quot;.<br /><br />If I had to guess I would predict that in borderline cases (which this certainly is) MATLAB would favor &quot;do what will lead to happier users&quot; and R would favor &quot;do what is thought to be the best numerical practice&quot;.</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
Hi,<br />I&#39;m not sure I understand what you mean by &quot;expected results&quot;?<br /><br />Regarding rounding, I was taught to round numbers ending in &quot;1, 2, 3, and 4&quot; *down*, and numbers that ended in &quot;6, 7, 8, 9&quot; *up*. Then, specifically regarding &quot;5&quot;, if the preceding digit is odd, round up and if the preceding digit is even, to round down. <br /><br />As you can see, this will then result in 50% of the numbers being rounded up, and 50% rounded down. If you round *down* on &quot;1, 2, 3, 4&quot; and round up on &quot;5, 6, 7, 8, 9&quot; you are rounding up 5/9th&#39;s of the time, and so introducing a bias.<br /><br />It sounds like R is handling it the way I would. Is that what you were wondering about?</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
To learn something about how computers handle numbers, especially as it relates to statistics and econometrics:<br /><br />B. D. McCullough and H. D. Vinod<br />&quot;The Numerical Reliability of Econometric Software,&quot;<br />Journal of Economic Literature 37(2), 633-665, 1999 <br /><br />A temporary link is available here:<br />http://www.pages.drexel.edu/~bdm25/jel.pdf</div>
</div>
</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/05/30/space-time-swing-probability-plot-for/">Space Time Swing Probability Plot for Ichiro</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-05-30T00:00:00-04:00" pubdate data-updated="true">May 30<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
I was having some fun with PITCHf/x data and generalize additive models. <a href="http://en.wikipedia.org/wiki/PITCHf/x">PITCHf/x</a> keeps track of the trajectory, path, location of every pitch in the MLB. It is pretty accurate and opens up baseball to more analyses than ever before. <a href="http://en.wikipedia.org/wiki/Generalized_additive_model">Generalized additive models</a> (GAMs) are statistical models that put minimal assumptions on the type of model you are fitting. Traditional statistical models are linear, in that they assume that the response variable you are modelling is a linear function of the explanatory variables. GAMs just assumes that the relationship is &#8220;smooth.&#8221; <a href="http://www.stat.ubc.ca/%7Ematias/rgam/binomial.png">Here </a>is a good example of a relationship that may have traditionally been modeled as linear, but it is a much better assumption that the relationship is smooth.<br /><br />I fit a GAM to PITCHf/x data. The response is whether or not Ichiro swung. The explanatory variables are pitch location on the x, pitch location on the z, and the day of the year. Obviously, we expect the probability of swinging to change as the pitch is closer or further away from the center of the strike zone. Additionally, I was interested in seeing his swinging propensity changed as the year went on.<br /><br /><div class="separator" style="clear: both; text-align: center;"><object class="BLOGGER-youtube-video" classid="clsid:D27CDB6E-AE6D-11cf-96B8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0" data-thumbnail-src="http://i.ytimg.com/vi/44X9VXXjYPI/0.jpg" height="266" width="320"><param name="movie" value="http://www.youtube.com/v/44X9VXXjYPI?version=3&f=user_uploads&c=google-webdrive-0&app=youtube_gdata" />  <param name="bgcolor" value="#FFFFFF" />  <embed width="320" height="266"  src="http://www.youtube.com/v/44X9VXXjYPI?version=3&f=user_uploads&c=google-webdrive-0&app=youtube_gdata" type="application/x-shockwave-flash"></embed></object></div><br />You can see that the probability of swinging is smooth in both location and time. Also, you can see (ever so slightly) that the probability of swinging increased as the year went on. Looking at <a href="http://www.baseball-reference.com/players/split.cgi?id=suzukic01&amp;year=2010&amp;t=b#half">the splits</a>, you can see that his walk percentage was 28/395 (7.1%) in the first half and 17/337 (5.0%) in the second half. This is in agreement with the swing probability increasing,<br /><br />I used the <a href="http://people.bath.ac.uk/sw283/mgcv/tampere/mgcv.pdf">mgcv </a>package in R to run the GAM. I created an image for every day and stitched them together into a movie with <a href="http://ffmpeg.org/">ffmpeg</a>. The R code is <a href="https://docs.google.com/open?id=0B21DLJKq18K9ejNUX0RXQnlJam8">here</a>.</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/05/25/sending-text-in-r/">Sending a Text in R</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-05-25T00:00:00-04:00" pubdate data-updated="true">May 25<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
Don&#8217;t you hate it when you are running a long piece of code and you keep checking the results every 15 minutes, hoping it will finish? There is a better way.<br /><br />I got the idea from <a href="http://jbdeaton.com/2012/send-yourself-an-sms-via-python/">here</a>. He uses a Python script and the text interface is not free. I thought someone must have already thought of this for R. There is an easy solution. You can send an email fairly easily in R. You can also send <a href="http://allthingsmarked.com/2006/09/04/howto-send-free-text-messages-through-email/">text messages as emails</a>. <a href="http://www.decisionstats.com/send-email-by-r/">This page </a>gives a few examples of how to send an email in R. Sending a text via email will depend on what carrier you have and I am sure standard text message fees apply.You can only send 20 emails a day with this package, but that probably won&#8217;t be a big deal.<br /><br />It is basically a one-liner. Here is the code I used:<br /><pre class="brush: r">install.packages("mail",repos="http://cran.case.edu/")<br />library(mail)<br />sendmail("5555555555@txt.att.net", <br />         subject="Notification from R",message="MCMC Finished", password="rmail")<br /></pre><br />That&#8217;s it!<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-DaUTmyXUfks/T7_mu1CRtII/AAAAAAAAG8Y/Ike9L7_C_yI/s1600/photo.PNG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="320" src="http://1.bp.blogspot.com/-DaUTmyXUfks/T7_mu1CRtII/AAAAAAAAG8Y/Ike9L7_C_yI/s320/photo.PNG" width="212" /></a></div></div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/05/20/cleveland-indians/">Cleveland Indians&#8217; Attendance</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-05-20T00:00:00-04:00" pubdate data-updated="true">May 20<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
Recently, Chris Perez, the closer for the Indians, displayed <a href="http://throwinheat.mlblogs.com/2012/05/20/chris-perez-expands-on-his-comments-mark-shapiro-responds/">some frustration </a>with the fans for not supporting the team. Currently, they have the <a href="http://www.baseball-reference.com/leagues/current_attendance.shtml">lowest attendance </a>in the majors &#8211; by a decent margin. The Indians are averaging about 15,000 fans per home game, while the next closest team, the Oakland A&#8217;s, is averaging 19,000. It seemed like an odd time for Perez to bring this up because they <a href="http://www.baseball-reference.com/teams/CLE/2012-schedule-scores.shtml">have had </a>attendance in the 29,000s each of the last two home games. So that intrigued me to look into the numbers of what causes attendance to vary.<br /><br />I looked at <a href="http://www.baseball-reference.com/teams/CLE/2011-schedule-scores.shtml">2011 attendance data </a>for the Cleveland Indians only. I had a strong suspicion that a popular opponent would definitely and weekend games cause attendance to increase. Also, there is usually some press at the beginning of the season that claims no one wants to go to the games because it is too cold for baseball. (There is also more competing entertainment at the beginning of the season.)<br /><br />What I found to be significant (based on an exploratory approach) are summarized in the graph below. This plot explores the relationship of attendance with 5 other variables. I plotted attendance on the y-axis and the date on the x-axis. I don&#8217;t expect date to have any effect, but it organizes other aspects well (and you can see opening day had the highest attendance of the year). Instead of plotting points, I plotted the name of the opponent. You can see there are some larger attendances when they are playing the New York Yankees and the Cincinnati Reds, for example. The color of the team name indicates whether they are playing on the weekend or not and the size indicates the <a href="http://academic.udayton.edu/kissock/http/Weather/citylistUS.htm">temperature</a>. Probably the biggest effect, weekend games outdraw weekday games consistently. The colder temperatures are only in the beginning of the season, and seem to have a noticeable effect (at least for the coldest days).<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-c7JBG0k4OBQ/T7kvpyZYOmI/AAAAAAAAG20/tC71mpum_lA/s1600/IndainsAttendance2011.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="633" src="https://lh3.ggpht.com/-c7JBG0k4OBQ/T7kvpyZYOmI/AAAAAAAAG20/tC71mpum_lA/s640/IndainsAttendance2011.jpeg" width="640" /></a></div><br /><br />I also looked into how many games above .500 the team was and how close they were in the division race. Neither of these showed any correlation, at least at the marginal level. This is interesting because the main reason Chris Perez is frustrated is that the team is winning, so the fans should be supporting them. This shows that wining did not make much of a difference within a single year. This should be more prevalent over multiple years.<br /><br />Some other information that might be useful is the quality of the opponent or whether the ace of the opposing pitching staff is starting. I only included temperature and not precipitation or any other weather information.<br /><br />Here is the basic R code I used: <br /><pre class="brush: r">library(ggplot2)<br />ggplot(data=home.attend,aes(x=Date,y=Attendance,colour=Weekend,label=Opp,size=Temp))+<br />  geom_text()+scale_size(to = c(2, 5))+theme_bw()&nbsp;</pre><pre class="brush: r">&nbsp;</pre><i><b>Update (9/21/2013)</b></i>:<br />Since attendance is still a hot topic, I created the same plot for the 2013 season so far.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-MiH0WF4KyGU/Uj5E45dKMSI/AAAAAAAAH9A/3MoE4VGYjUA/s1600/IndainsAttendance2013.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="426" src="https://lh3.ggpht.com/-MiH0WF4KyGU/Uj5E45dKMSI/AAAAAAAAH9A/3MoE4VGYjUA/s1600/IndainsAttendance2013.jpeg" width="640" /></a></div><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-mT8HBGEKwak/Uj5EauQomGI/AAAAAAAAH84/qB4e0VE6n94/s1600/IndainsAttendance2013.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><br /></a></div><br /></div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/05/05/whats-up-with-alber-pujols/">What&#8217;s Up With Albert Pujols?</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-05-05T00:00:00-04:00" pubdate data-updated="true">May 5<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
After signing a huge deal with the Angels, Pujols has been having a really bad year. He hasn&#8217;t hit a home run this year, breaking a career long streak. So I thought it would be a good idea to use some statistics to tell how good or bad we think Pujols will actually be this year.<br /><br />Coming into the year, he had a career .328/.420/.617 career AVG/OBP/SLG. Through one month, he has a .194/.237/.269. So what do we expect out of Pujols for the rest of the year?<br /><br />In Bayesian statistical terms, we can quantify our prior beliefs about Pujols from his history before this year. Below are histograms and fitted distributions of Pujols&#8217; yearly batting lines from 2001 to 2011. His numbers are well above normal and he has been the best player in baseball for a while.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-eBkY_jm8kn0/T6WDvtfUlZI/AAAAAAAAGss/dNE4TkjFC6I/s1600/pujols.bmp" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-eBkY_jm8kn0/T6WDvtfUlZI/AAAAAAAAGss/dNE4TkjFC6I/s1600/pujols.bmp" /></a></div>So coming into this year, we would expect him to have a batting average between .290 and .370, with .330 being the most likely, for example.<br /><br />Combining our prior expectations with the data we have observed from this year, we can get our posterior beliefs. When we do that, we get a posterior expectation that Pujols is a true .312/.379/.451 hitter. The league averages from 2001 to 2011 are .263/.331/.418, so he is still expected to be well above the average player, even with the poor start. If we use data from just this year, we do not have enough data to give us an accurate reflection of how good he is. If we combine the data with our prior beliefs, we get a better indication of what to expect. Below is a table that compares these numbers.<br /><br /><style type="text/css">.nobrtable br { display: none } </style><br /><div class="nobrtable"><table border="2" bordercolor="#3366cc" cellpadding="3" cellspacing="3" style="background-color: white;"><tbody><tr style="background-color: #3366cc; color: white; padding-bottom: 4px; padding-top: 5px;"> <th><br /></th><th>AVG</th><th>OBP</th><th>SLG</th></tr><tr><td>League</td><td>.263</td><td>.331</td><td>.418</td></tr><tr><td>Prior</td><td>.328</td><td>.420</td><td>.617</td></tr><tr><td>This Year</td><td>.194</td><td>.237</td><td>.269</td></tr><tr><td>Posterior</td><td>.312</td><td>.379</td><td>.451</td></tr></tbody></table></div><br />Finally, we can also get the whole posterior distribution (not just the expectation). I have plotted the prior and posterior distributions on the same graph. You can see that Pujols&#8217; bad month has caused our beliefs about him to decrease quite a bit. Most notable is the slugging percentage, which is likely because of his career-long homerless streak.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-ZmO18a8vj0w/T6WLW6LnjsI/AAAAAAAAGs4/ll2ab7sGJAg/s1600/pujols-prior+and+posterior.bmp" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-ZmO18a8vj0w/T6WLW6LnjsI/AAAAAAAAGs4/ll2ab7sGJAg/s1600/pujols-prior+and+posterior.bmp" /></a></div><br />What Bayesian analysis does is shrinks (or regresses) the data from this year to the prior average. This is&nbsp; the same idea of regressing to the overall mean that is talked about frequently in sabermetric blogs. The difference is that regressing to the mean usually regresses to the average player in the league. With Bayesian analysis, we can regress to our prior expectation about the specific player (Albert Pujols). I believe this approach will give us better results in most cases.<br /><br />The R code to do the analysis can be found <a href="https://docs.google.com/open?id=0B21DLJKq18K9NkNJb0lQRHNqbHM">here</a>. I used data from <a href="http://www.baseball-reference.com/players/p/pujolal01.shtml">Baseball Reference</a>.</div>
<h2>Comments</h2>
<div class='comments'>
<div class='comment'>
<div class='author'>Andrew Landgraf</div>
<div class='content'>
Just a follow up: From May 6 to the end of the season he had a .305/.365/.569 split. The posterior expectation of .312/.379/.451 matches the average and on base percentage well, but his actual slugging is quite a bit higher than the posterior distribution would have predicted.<br /><br />Not sure what to make of this except to say he had a really bad start with slugging, which may have been caused more by random chance than the other two. And obviously the distributional assumptions are an approximation to reality.</div>
</div>
</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/02/17/visualizing-correlations-of-matrix/">Visualizing the Correlations of a Matrix</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-02-17T00:00:00-05:00" pubdate data-updated="true">Feb 17<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
Correlation matrices are a common way to look at the dependence of a set of variables. When the variables have spatial relationships, the correlation matrix loses some information.<br /><br />Lets say you have repeated observations, each one being a matrix. For example, you could have yearly observations of health statistics for a spatial grid. Lets say the grid is n by p (n*p variables) and there are m observations of the grid. If we want to get the correlations of each element of the grid, the typical way to do that would be to convert the matrix of variables into a vector of length n*p, and then calculate the correlation matrix of the vector. When you do that, however, it will no longer be obvious which of the variables are on the same row/column or are close to each other. So the typical correlation matrix is not satisfactory.<br /><br />What I propose is a set of <a href="http://en.wikipedia.org/wiki/Small_multiple">small multiples </a>of correlation matrices. Instead of having an n*p by n*p correlation matrix, we will have an n by p grid of correlation matrices, each correlation matrix representing the correlation with the variable in that position of the matrix. Below is an example.<br /><br /><div class="separator" style="clear: both; text-align: center;"></div><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-uKlTWBzaYhQ/Tz7SB8nALQI/AAAAAAAAF7c/ioCipoIuVgg/s1600/matrix+cor+plot.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-uKlTWBzaYhQ/Tz7SB8nALQI/AAAAAAAAF7c/ioCipoIuVgg/s1600/matrix+cor+plot.png" /></a></div><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-lv9XCCr-k0E/Tz7PVojdkfI/AAAAAAAAF7U/tFH2H8im_3Q/s1600/matrix+cor+plot.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><br /></a></div>The above example is just random data, so all correlations are spurious. Blue is positive, red is negative, and white is no correlation. You can see in the row 1, column 1 matrix that the 1st row and 1st column is dark blue. This is because this is the correlation with itself. Similarly in all other rows and columns.<br /><br />Using a real example might display the usefulness more clearly. I am on a project estimating elements of a matrix with only the row totals and column totals. I simulated data many times and kept track of the errors. I was interested in how the errors in the different cells are correlated with each other. Below, you can see that the errors in the same row or column are positively correlated with each other, while the errors in other rows and columns are negatively correlated. This pops out at you with the below plot, but would have been difficult to figure out with a typical correlation matrix.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-wSVkWLxaxH8/Tz7NN6lNobI/AAAAAAAAF7M/wQ7T-B-3Kx0/s1600/Correlation+of+Errors+Matrix1.jpg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="400" src="http://1.bp.blogspot.com/-wSVkWLxaxH8/Tz7NN6lNobI/AAAAAAAAF7M/wQ7T-B-3Kx0/s400/Correlation+of+Errors+Matrix1.jpg" width="400" /></a></div><br />Code:<br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace; font-size: small;"># Generate random data for the example</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace; font-size: small;">reps=10<br />mat.data=array(0,c(4,5,reps))<br />for (i in 1:reps) {<br />&nbsp; mat.data[,,i]=matrix(rmultinom(1,20,rep(1,4*5)/(4*5)),4,5)<br />}</span><br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace; font-size: small;">matrix.cor.plot(mat.data)</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace; font-size: small;"><br /># the function<br />matrix.cor.plot &lt;- function(mat.data) {<br />&nbsp; #mat.data should be a nrow by ncol by nrep array<br />&nbsp; <br />&nbsp; nrow=dim(mat.data)[1]<br />&nbsp; ncol=dim(mat.data)[2]<br />#&nbsp;&nbsp; nrep=dim(mat.data)[3]<br />&nbsp; <br />&nbsp; par(mfrow=c(nrow,ncol),cex=.75,bty=&#8221;o&#8221;,mar=c(1, 1, 1, 1) + 0.1)<br />&nbsp; <br />&nbsp; # red is -1, white is 0, blue is +1<br />&nbsp; rgb.palette &lt;- colorRampPalette(c(&#8220;red&#8221;,&#8221;white&#8221;,&#8221;blue&#8221;), space = &#8220;rgb&#8221;)<br />&nbsp; <br />&nbsp; for (r in 1:nrow) {<br />&nbsp;&nbsp;&nbsp; for (c in 1:ncol) {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cor.mat=matrix(0,nrow,ncol)<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for (r2 in 1:nrow) { for (c2 in 1:ncol) {cor.mat[r2,c2]=cor(mat.data[r,c,],mat.data[r2,c2,]) } }<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cor.mat=t(cor.mat)<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cor.mat=cor.mat[,ncol(cor.mat):1]<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image(cor.mat,zlim=c(-1,1),col=rgb.palette(120),axes = FALSE,main=paste(&#8220;Row:&#8221;,r,&#8221;Col:&#8221;,c))<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; box()<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; }<br />}</span></div>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/3/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2013/12/27/top-songs-by-artist-on-cd1025-in-2013/">Top Songs by Artist on CD102.5 in 2013</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/08/27/when-did-cd1025-book-summerfest-artists/">When Did CD102.5 Book the Summerfest Artists?</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/08/20/downloading-and-analyzing-cd1025s/">Downloading and Analyzing CD1025&#8217;s Playlist</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/04/21/what-is-probability-of-16-seed-beating/">What Is the Probability of a 16 Seed Beating a 1 Seed?</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/04/18/easily-access-academic-journals-off/">Easily Access Academic Journals Off Campus With a Firefox Bookmark</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Andrew Landgraf -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
