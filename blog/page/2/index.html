
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Statistically Significant</title>
  <meta name="author" content="Andrew Landgraf">

  
  <meta name="description" content="Random forests ™ are great. They are one of the best &#8220;black-box&#8221; supervised learning methods. If you have lots of data and lots of &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://andland.github.io/blog/page/2">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Statistically Significant" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Statistically Significant</a></h1>
  
    <h2>Andrew Landgraf's Blog</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:andland.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Blog Archives</a></li>
  <li><a href="/about">About</a></li>
  <li><a href="/projects">Shiny</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/07/19/random-forest-variable-importance/">Random Forest Variable Importance</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-07-19T00:00:00-04:00" pubdate data-updated="true">Jul 19<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
<a href="http://en.wikipedia.org/wiki/Random_forest">Random</a> <a href="http://www.stat.berkeley.edu/%7Ebreiman/RandomForests/">forests</a> ™ are great. They are one of the best &#8220;black-box&#8221; supervised learning methods. If you have lots of data and lots of predictor variables, you can do worse than random forests. They can deal with messy, real data. If there are lots of extraneous predictors, it has no problem. It automatically does a good job of finding interactions as well. There are no assumptions that the response has a linear (or even smooth) relationship with the predictors.<br /><br />As cited in the Wikipedia article, they do lack some interpretability. But what they lack in interpretation, they more than make up for in prediction power, which I believe is much more important than interpretation in most cases. Even though you cannot easily tell how one variable affects the prediction, you can easily create a <i>partial dependence plot</i> which shows how the response will change as you change the predictor. You can also do this for two variables at once to see the interaction of the two.<br /><br />Also helping in the interpretation is that they can output a list of predictor variables that they believe to be important in predicting the outcome. If nothing else, you can subset the data to only include the most &#8220;important&#8221; variables, and use that with another model. The randomForest package in R has two measures of importance. <a href="http://rss.acs.unt.edu/Rdoc/library/randomForest/html/importance.html">One </a>is &#8220;total decrease in node impurities from splitting on the variable, averaged over all trees.&#8221; I do not know much about this one, and will not talk about it further. The other is based on a <a href="http://en.wikipedia.org/wiki/Resampling_%28statistics%29#Permutation_tests">permutation test</a>. The idea is that if the variable is not important (the null hypothesis), then rearranging the values of that variable will not degrade prediction accuracy. Random forests use out-of-bag (OOB) samples to measure prediction accuracy.<br /><br />In my experience, it does a pretty good job of finding the most important predictors, but it has issues with correlated predictors. For example, I was working on a problem where I was predicting the price that electricity trades. One feature that I knew would be very important was the amount of electricity being used at that same time. But I thought there might also be a relationship between price and the electricity being used a few hours before and after. When I ran the random forest with these variables, the electricity used 1 hour after was found to be more important than the electricity used at the same time. When including the 1 hour after electricity use instead of the current hour electricity use, the cross validation (CV) error increased. Using both did not significantly change the CV error compared to using just current hour. Because the electricity used at the current hour and the hour after are very correlated, it had trouble telling which one was more important. In truth, given the electricity use at the current hour, the electricity use at the hour after did not improve the predictive accuracy.<br /><br />Why does the importance measure give more weight correlated predictors? <a href="http://www.biomedcentral.com/1471-2105/9/307/">Strobl et al.</a> give some intuition behind what is happening and propose a solution. Basically, the permutation test is ill-posed. The permutation test is testing that the variable is independent of the response as well as all other predictors. Since the correlated predictors are obviously not independent, we get high importance scores. They propose a permutation test where you condition on the correlated predictors. This is a little tricky when the correlated predictors are continuous, but you can read the paper for more details.<br /><br />Another way to think of it is that, since each split only considers a subset of the possible variables, a variable that is correlated with an &#8220;important&#8221; variable may be considered without the &#8220;important&#8221; variable. This would cause the correlated variable to be selected for the split. The correlated value does hold some predictive value, but only because of the truly important variable, so it is understandable why this procedure would consider it important.<br /><br />I ran a simulation experiment (similar to the one in the paper) to demonstrate the issue. I simulated 5 predictor variables. Only the first one is related to the response, but the second one has a correlation of about 0.7 with the first one. Luckily, Strobl et al. included their version of importance in the package <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">party</span> in <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">R</span>. I compare variable importance from the <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">randomForest </span>package and the importance with and without taking correlated predictors into account from the <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">party<span style="font-family: inherit;"> </span></span><span style="font-family: inherit;">pac</span>kage.<br /><br /><pre class="brush: r"># simulate the data<br />x1=rnorm(1000)<br />x2=rnorm(1000,x1,1)<br />y=2*x1+rnorm(1000,0,.5)<br />df=data.frame(y,x1,x2,x3=rnorm(1000),x4=rnorm(1000),x5=rnorm(1000))<br /><br /># run the randomForest implementation<br />library(randomForest)<br />rf1 &lt;- randomForest(y~., data=df, mtry=2, ntree=50, importance=TRUE)<br />importance(rf1,type=1)<br /><br /># run the party implementation<br />library(party)<br />cf1 &lt;- cforest(y~.,data=df,control=cforest_unbiased(mtry=2,ntree=50))<br />varimp(cf1)<br />varimp(cf1,conditional=TRUE)<br /></pre><br />For the <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">randomForest</span>, the ratio of importance of the the first and second variable is 4.53. For <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">party</span> without accounting for correlation it is 7.35. And accounting for correlation, it is 369.5. The higher ratios are better because it means that the importance of the first variable is more prominent. party&#8217;s implementation is clearly doing the job.<br /><br />There is a downside. It takes much longer to calculate importance with correlated predictors than without. For the party package in this example, it took 0.39 seconds to run without and 204.34 seconds with. I could not even run the correlated importance on the electricity price example. There might be a research opportunity to get a quicker approximation.<br /><br />Possibly up next: confidence limits for random forest predictions.</div>
<h2>Comments</h2>
<div class='comments'>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
HI Andrew, When i tried to calculate the variable importance, I get the error <br />&quot;Error in model.matrix.default(as.formula(f), data = blocks) : <br />  allocMatrix: too many elements specified&quot;<br /><br />I&#39;m not sure how to deal with it because I have 2 factor levels which I&#39;m predicting with 23 continuous variable predictors (about 2200 data points. I know it&#39;s a lot to work with, but not sure how to get the correct VarImp values with this large data set.<br /><br />I tried increasing the threshold, but it didn&#39;t help. fit.varimp=varimp(fit.cf,threshold=0.8,conditional=TRUE)<br /><br />Do you have any suggestions? Thanks.</div>
</div>
<div class='comment'>
<div class='author'>Andrew Landgraf</div>
<div class='content'>
Unfortunately, I didn&#39;t set the seed. #nexttime</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
you should provide the seed to reproduce your example</div>
</div>
<div class='comment'>
<div class='author'>Andrew</div>
<div class='content'>
It should be able to give you a solution. I have no idea what ramifications this would have on accuracy or variable importance.</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
I have microarray data on 30 entries and thought of using RF to identify important response genes. I understand that this is a N&lt;&lt;p type of problems, but will &#39;party&#39; be able to come up with a solution?</div>
</div>
<div class='comment'>
<div class='author'>Andrew</div>
<div class='content'>
The simple answer is using the partialPlot function in the randomForest package for R (http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=randomForest:partialPlot). <br /><br />What it is doing isn&#39;t all that complicated. Say you want a partial dependence plot for the variable X_1. For each value of X_1=x that you want to plot, you take the average of the prediction with X_1=x and the other explanatory variables equal to the n values that they are in the data set. You are trying to average out the other variables.</div>
</div>
<div class='comment'>
<div class='author'>nanounanue</div>
<div class='content'>
Great explanation!<br /><br />I have a question for you: In your text you said:<br /><br />&quot;&quot;&quot;<br /> &#8230; Even though you cannot easily tell how one variable affects the prediction, you can easily create a partial dependence plot which shows how the response will change as you change the predictor. You can also do this for two variables at once to see the interaction of the two.<br />&quot;&quot;&quot;<br /><br />Could you provide, please, an example of how build the partial dependence plot?<br /><br /><br />Thank you for the post and thanks in advance</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
Great post!  Thank you.</div>
</div>
</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/06/15/rounding-in-r/">Rounding in R</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-06-15T00:00:00-04:00" pubdate data-updated="true">Jun 15<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
Forgive me if you are already aware of this, but I found it quite alarming. I know that most code is interpreted by the computer in binary and we input in decimal, so problems can arise in conversion and with <a href="http://en.wikipedia.org/wiki/Floating_point">floating point</a>. But the example I have below is so simple that it really surprised me.<br /><br />I was converting a function from R into MATLAB so that a colleague could use it. I tested it out on the same data and got slightly different results. Digging into the problem, the difference was due to the fact that R was rounding 4.5 to 4 and MATLAB was rounding it to 5. I thought the &#8220;4.5&#8221; must have really been &#8220;4.49999&#8230;&#8221;. But that was not so.<br /><br />For example, this is the result of the <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/Round.html">round</a> function for a few numbers.<br /><pre class="brush: r">&gt; round(0.5,0)<br />[1] 0<br />&gt; round(1.5,0)<br />[1] 2<br />&gt; round(2.5,0)<br />[1] 2<br />&gt; round(3.5,0)<br />[1] 4<br />&gt; round(4.5,0)<br />[1] 4<br />&gt; round(5.5,0)<br />[1] 6<br />&gt; round(6.5,0)<br />[1] 6<br /></pre><br />Do you see a pattern?<br /><br />I tried this on versions 2.13.1 and 2.14.0. I ran the same with MATLAB and it gave the expected results. I am not any kind of expert on computer sciences, so I was not sure why this is happening. <a href="http://www.mathsisfun.com/binary-decimal-hexadecimal-converter.html">Converting </a>any decimal number that ends in .5 into binary results in a finite length binary number. For example, 4.5 is 100.1 in binary. Because of this, I wouldn&#8217;t think the error would be due to floating points, but I really don&#8217;t know.<br /><br />Looking at the <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/Round.html">documentation</a> for round, I found the reason. It states in the notes, &#8220;Note that for rounding off a 5, the <a href="http://en.wikipedia.org/wiki/IEEE_754-2008#Rounding_rules">IEC 60559 standard</a> is expected to be used, ‘<em>go to the even digit</em>’.&#8221; It is a little comforting knowing that there is a logic behind it and that R is abiding to some standard. But why isn&#8217;t MATLAB abiding by the same standard? Also, I think most people expect numbers ending in .5 to round up, not the nearest even digit.</div>
<h2>Comments</h2>
<div class='comments'>
<div class='comment'>
<div class='author'>Analytic Bastard</div>
<div class='content'>
kudos Blaise</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
Andrew wrote &quot;Also, I think most people expect numbers ending in .5 to round up (not the nearest even digit)&quot;. This kind of rounding is in German called &quot;kaufmännische Rundung&quot; (rounding in commerce). For this purpose I use the following function:<br /><br />#Definition of a function for &quot;rounding in commerce&quot;<br />cround = function(x,n){<br />vorz = sign(x)<br />z = abs(x)*10^n<br />z = z + 0.5<br />z = trunc(z)<br />z = z/10^n<br />z*vorz<br />}<br /><br /># Example<br />&gt; round(seq(0.5,6.5,1),0)<br />[1] 0 2 2 4 4 6 6<br />&gt; cround(seq(0.5,6.5,1),0)<br />[1] 1 2 3 4 5 6 7</div>
</div>
<div class='comment'>
<div class='author'>cellocgw</div>
<div class='content'>
This &quot;round to even&quot; approach has been accepted by just about everyone (except matlab, and no surprise, except Msoft Excel).  <br />Sadly, the flame wars over &quot;round to even&quot; vs. &quot;round up&quot; continue, rather the way people argue about &quot;0.999&#8230; != 1&quot;<br /><br />PS: @a Tom:  I&#39;m highly skeptical of your <br />claim about 2.46&#8211;&gt;3.  Do you have a citation?</div>
</div>
<div class='comment'>
<div class='author'>a Tom</div>
<div class='content'>
I&#39;m ever amazed that something so seemingly basic can have so many different approaches.<br /><br />I understand that in many middle east countries they start with the far right digit and round up or down, so 2.46 is rounded to 3!</div>
</div>
<div class='comment'>
<div class='author'>Blaise</div>
<div class='content'>
This is discussed in Don Knuth&#39;s 1973 classic Seminumerical Algorithms. He gives the following example of what can happen when 5s are always rounded upwards. Suppose u = 10000000 and v = 0.5555556. Then u + v = 1.5555556. If we subtract v from this  result we get u&#39; = 1.0000001. Adding and then subtracting v from u&#39; and we get 1.0000002 and if we do it again we get 1.0000003 and so on. He says &quot;This phenomenon, called drift, will not occur when we use a stable rounding rule based on the parity of the least significant digit.&quot;</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
I was the #2 anonymous poster. Echoing Ben, I think that for ease of teaching, the &quot;round 5 up&quot; method is taught to children (and adults?) below the university level, and only if you go on for advance work is the more complicated method taught.<br /><br />Can you imagine trying to teach a 10 or 12 year old the IEC 60559 standard? Unfortunately, this is the method most adults are used to&#8230;<br /><br />I agree, it is a little troubling that Matlab doesn&#39;t abide by the standard. Yet another reason to stick with R!</div>
</div>
<div class='comment'>
<div class='author'>Ben Bolker</div>
<div class='content'>
Wikipedia ( http://en.wikipedia.org/wiki/Rounding#Round_half_to_even ) says of round-to-even:<br /><br />This method also treats positive and negative values symmetrically, and therefore is free of overall bias if the original numbers are positive or negative with equal probability. In addition, for most reasonable distributions of y values, the expected (average) value of the rounded numbers is essentially the same as that of the original numbers, even if the latter are all positive (or all negative). However, this rule will still introduce a positive bias for even numbers (including zero), and a negative bias for the odd ones.<br /><br />So round-to-even seems to have *slightly* better numerical properties than &quot;round ties away from zero&quot;, which is what is (I think) most often taught, because it&#39;s easier to understand. http://www.mathworks.com/matlabcentral/fileexchange/6752 gives a MATLAB function for &quot;round to even&quot;.<br /><br />If I had to guess I would predict that in borderline cases (which this certainly is) MATLAB would favor &quot;do what will lead to happier users&quot; and R would favor &quot;do what is thought to be the best numerical practice&quot;.</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
Hi,<br />I&#39;m not sure I understand what you mean by &quot;expected results&quot;?<br /><br />Regarding rounding, I was taught to round numbers ending in &quot;1, 2, 3, and 4&quot; *down*, and numbers that ended in &quot;6, 7, 8, 9&quot; *up*. Then, specifically regarding &quot;5&quot;, if the preceding digit is odd, round up and if the preceding digit is even, to round down. <br /><br />As you can see, this will then result in 50% of the numbers being rounded up, and 50% rounded down. If you round *down* on &quot;1, 2, 3, 4&quot; and round up on &quot;5, 6, 7, 8, 9&quot; you are rounding up 5/9th&#39;s of the time, and so introducing a bias.<br /><br />It sounds like R is handling it the way I would. Is that what you were wondering about?</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
To learn something about how computers handle numbers, especially as it relates to statistics and econometrics:<br /><br />B. D. McCullough and H. D. Vinod<br />&quot;The Numerical Reliability of Econometric Software,&quot;<br />Journal of Economic Literature 37(2), 633-665, 1999 <br /><br />A temporary link is available here:<br />http://www.pages.drexel.edu/~bdm25/jel.pdf</div>
</div>
</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/05/30/space-time-swing-probability-plot-for/">Space Time Swing Probability Plot for Ichiro</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-05-30T00:00:00-04:00" pubdate data-updated="true">May 30<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
I was having some fun with PITCHf/x data and generalize additive models. <a href="http://en.wikipedia.org/wiki/PITCHf/x">PITCHf/x</a> keeps track of the trajectory, path, location of every pitch in the MLB. It is pretty accurate and opens up baseball to more analyses than ever before. <a href="http://en.wikipedia.org/wiki/Generalized_additive_model">Generalized additive models</a> (GAMs) are statistical models that put minimal assumptions on the type of model you are fitting. Traditional statistical models are linear, in that they assume that the response variable you are modelling is a linear function of the explanatory variables. GAMs just assumes that the relationship is &#8220;smooth.&#8221; <a href="http://www.stat.ubc.ca/%7Ematias/rgam/binomial.png">Here </a>is a good example of a relationship that may have traditionally been modeled as linear, but it is a much better assumption that the relationship is smooth.<br /><br />I fit a GAM to PITCHf/x data. The response is whether or not Ichiro swung. The explanatory variables are pitch location on the x, pitch location on the z, and the day of the year. Obviously, we expect the probability of swinging to change as the pitch is closer or further away from the center of the strike zone. Additionally, I was interested in seeing his swinging propensity changed as the year went on.<br /><br /><div class="separator" style="clear: both; text-align: center;"><object class="BLOGGER-youtube-video" classid="clsid:D27CDB6E-AE6D-11cf-96B8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0" data-thumbnail-src="http://i.ytimg.com/vi/44X9VXXjYPI/0.jpg" height="266" width="320"><param name="movie" value="http://www.youtube.com/v/44X9VXXjYPI?version=3&f=user_uploads&c=google-webdrive-0&app=youtube_gdata" />  <param name="bgcolor" value="#FFFFFF" />  <embed width="320" height="266"  src="http://www.youtube.com/v/44X9VXXjYPI?version=3&f=user_uploads&c=google-webdrive-0&app=youtube_gdata" type="application/x-shockwave-flash"></embed></object></div><br />You can see that the probability of swinging is smooth in both location and time. Also, you can see (ever so slightly) that the probability of swinging increased as the year went on. Looking at <a href="http://www.baseball-reference.com/players/split.cgi?id=suzukic01&amp;year=2010&amp;t=b#half">the splits</a>, you can see that his walk percentage was 28/395 (7.1%) in the first half and 17/337 (5.0%) in the second half. This is in agreement with the swing probability increasing,<br /><br />I used the <a href="http://people.bath.ac.uk/sw283/mgcv/tampere/mgcv.pdf">mgcv </a>package in R to run the GAM. I created an image for every day and stitched them together into a movie with <a href="http://ffmpeg.org/">ffmpeg</a>. The R code is <a href="https://docs.google.com/open?id=0B21DLJKq18K9ejNUX0RXQnlJam8">here</a>.</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/05/25/sending-text-in-r/">Sending a Text in R</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-05-25T00:00:00-04:00" pubdate data-updated="true">May 25<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
Don&#8217;t you hate it when you are running a long piece of code and you keep checking the results every 15 minutes, hoping it will finish? There is a better way.<br /><br />I got the idea from <a href="http://jbdeaton.com/2012/send-yourself-an-sms-via-python/">here</a>. He uses a Python script and the text interface is not free. I thought someone must have already thought of this for R. There is an easy solution. You can send an email fairly easily in R. You can also send <a href="http://allthingsmarked.com/2006/09/04/howto-send-free-text-messages-through-email/">text messages as emails</a>. <a href="http://www.decisionstats.com/send-email-by-r/">This page </a>gives a few examples of how to send an email in R. Sending a text via email will depend on what carrier you have and I am sure standard text message fees apply.You can only send 20 emails a day with this package, but that probably won&#8217;t be a big deal.<br /><br />It is basically a one-liner. Here is the code I used:<br /><pre class="brush: r">install.packages("mail",repos="http://cran.case.edu/")<br />library(mail)<br />sendmail("5555555555@txt.att.net", <br />         subject="Notification from R",message="MCMC Finished", password="rmail")<br /></pre><br />That&#8217;s it!<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-DaUTmyXUfks/T7_mu1CRtII/AAAAAAAAG8Y/Ike9L7_C_yI/s1600/photo.PNG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="320" src="http://1.bp.blogspot.com/-DaUTmyXUfks/T7_mu1CRtII/AAAAAAAAG8Y/Ike9L7_C_yI/s320/photo.PNG" width="212" /></a></div></div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/05/20/cleveland-indians/">Cleveland Indians&#8217; Attendance</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-05-20T00:00:00-04:00" pubdate data-updated="true">May 20<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
Recently, Chris Perez, the closer for the Indians, displayed <a href="http://throwinheat.mlblogs.com/2012/05/20/chris-perez-expands-on-his-comments-mark-shapiro-responds/">some frustration </a>with the fans for not supporting the team. Currently, they have the <a href="http://www.baseball-reference.com/leagues/current_attendance.shtml">lowest attendance </a>in the majors &#8211; by a decent margin. The Indians are averaging about 15,000 fans per home game, while the next closest team, the Oakland A&#8217;s, is averaging 19,000. It seemed like an odd time for Perez to bring this up because they <a href="http://www.baseball-reference.com/teams/CLE/2012-schedule-scores.shtml">have had </a>attendance in the 29,000s each of the last two home games. So that intrigued me to look into the numbers of what causes attendance to vary.<br /><br />I looked at <a href="http://www.baseball-reference.com/teams/CLE/2011-schedule-scores.shtml">2011 attendance data </a>for the Cleveland Indians only. I had a strong suspicion that a popular opponent would definitely and weekend games cause attendance to increase. Also, there is usually some press at the beginning of the season that claims no one wants to go to the games because it is too cold for baseball. (There is also more competing entertainment at the beginning of the season.)<br /><br />What I found to be significant (based on an exploratory approach) are summarized in the graph below. This plot explores the relationship of attendance with 5 other variables. I plotted attendance on the y-axis and the date on the x-axis. I don&#8217;t expect date to have any effect, but it organizes other aspects well (and you can see opening day had the highest attendance of the year). Instead of plotting points, I plotted the name of the opponent. You can see there are some larger attendances when they are playing the New York Yankees and the Cincinnati Reds, for example. The color of the team name indicates whether they are playing on the weekend or not and the size indicates the <a href="http://academic.udayton.edu/kissock/http/Weather/citylistUS.htm">temperature</a>. Probably the biggest effect, weekend games outdraw weekday games consistently. The colder temperatures are only in the beginning of the season, and seem to have a noticeable effect (at least for the coldest days).<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-c7JBG0k4OBQ/T7kvpyZYOmI/AAAAAAAAG20/tC71mpum_lA/s1600/IndainsAttendance2011.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="633" src="http://3.bp.blogspot.com/-c7JBG0k4OBQ/T7kvpyZYOmI/AAAAAAAAG20/tC71mpum_lA/s640/IndainsAttendance2011.jpeg" width="640" /></a></div><br /><br />I also looked into how many games above .500 the team was and how close they were in the division race. Neither of these showed any correlation, at least at the marginal level. This is interesting because the main reason Chris Perez is frustrated is that the team is winning, so the fans should be supporting them. This shows that wining did not make much of a difference within a single year. This should be more prevalent over multiple years.<br /><br />Some other information that might be useful is the quality of the opponent or whether the ace of the opposing pitching staff is starting. I only included temperature and not precipitation or any other weather information.<br /><br />Here is the basic R code I used: <br /><pre class="brush: r">library(ggplot2)<br />ggplot(data=home.attend,aes(x=Date,y=Attendance,colour=Weekend,label=Opp,size=Temp))+<br />  geom_text()+scale_size(to = c(2, 5))+theme_bw()<br /></pre></div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/05/05/whats-up-with-alber-pujols/">What&#8217;s Up With Albert Pujols?</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-05-05T00:00:00-04:00" pubdate data-updated="true">May 5<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
After signing a huge deal with the Angels, Pujols has been having a really bad year. He hasn&#8217;t hit a home run this year, breaking a career long streak. So I thought it would be a good idea to use some statistics to tell how good or bad we think Pujols will actually be this year.<br /><br />Coming into the year, he had a career .328/.420/.617 career AVG/OBP/SLG. Through one month, he has a .194/.237/.269. So what do we expect out of Pujols for the rest of the year?<br /><br />In Bayesian statistical terms, we can quantify our prior beliefs about Pujols from his history before this year. Below are histograms and fitted distributions of Pujols&#8217; yearly batting lines from 2001 to 2011. His numbers are well above normal and he has been the best player in baseball for a while.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-eBkY_jm8kn0/T6WDvtfUlZI/AAAAAAAAGss/dNE4TkjFC6I/s1600/pujols.bmp" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-eBkY_jm8kn0/T6WDvtfUlZI/AAAAAAAAGss/dNE4TkjFC6I/s1600/pujols.bmp" /></a></div>So coming into this year, we would expect him to have a batting average between .290 and .370, with .330 being the most likely, for example.<br /><br />Combining our prior expectations with the data we have observed from this year, we can get our posterior beliefs. When we do that, we get a posterior expectation that Pujols is a true .312/.379/.451 hitter. The league averages from 2001 to 2011 are .263/.331/.418, so he is still expected to be well above the average player, even with the poor start. If we use data from just this year, we do not have enough data to give us an accurate reflection of how good he is. If we combine the data with our prior beliefs, we get a better indication of what to expect. Below is a table that compares these numbers.<br /><br /><style type="text/css">.nobrtable br { display: none } </style><br /><div class="nobrtable"><table border="2" bordercolor="#3366cc" cellpadding="3" cellspacing="3" style="background-color: white;"><tbody><tr style="background-color: #3366cc; color: white; padding-bottom: 4px; padding-top: 5px;"> <th><br /></th><th>AVG</th><th>OBP</th><th>SLG</th></tr><tr><td>League</td><td>.263</td><td>.331</td><td>.418</td></tr><tr><td>Prior</td><td>.328</td><td>.420</td><td>.617</td></tr><tr><td>This Year</td><td>.194</td><td>.237</td><td>.269</td></tr><tr><td>Posterior</td><td>.312</td><td>.379</td><td>.451</td></tr></tbody></table></div><br />Finally, we can also get the whole posterior distribution (not just the expectation). I have plotted the prior and posterior distributions on the same graph. You can see that Pujols&#8217; bad month has caused our beliefs about him to decrease quite a bit. Most notable is the slugging percentage, which is likely because of his career-long homerless streak.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-ZmO18a8vj0w/T6WLW6LnjsI/AAAAAAAAGs4/ll2ab7sGJAg/s1600/pujols-prior+and+posterior.bmp" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-ZmO18a8vj0w/T6WLW6LnjsI/AAAAAAAAGs4/ll2ab7sGJAg/s1600/pujols-prior+and+posterior.bmp" /></a></div><br />What Bayesian analysis does is shrinks (or regresses) the data from this year to the prior average. This is&nbsp; the same idea of regressing to the overall mean that is talked about frequently in sabermetric blogs. The difference is that regressing to the mean usually regresses to the average player in the league. With Bayesian analysis, we can regress to our prior expectation about the specific player (Albert Pujols). I believe this approach will give us better results in most cases.<br /><br />The R code to do the analysis can be found <a href="https://docs.google.com/open?id=0B21DLJKq18K9NkNJb0lQRHNqbHM">here</a>. I used data from <a href="http://www.baseball-reference.com/players/p/pujolal01.shtml">Baseball Reference</a>.</div>
<h2>Comments</h2>
<div class='comments'>
<div class='comment'>
<div class='author'>Andrew Landgraf</div>
<div class='content'>
Just a follow up: From May 6 to the end of the season he had a .305/.365/.569 split. The posterior expectation of .312/.379/.451 matches the average and on base percentage well, but his actual slugging is quite a bit higher than the posterior distribution would have predicted.<br /><br />Not sure what to make of this except to say he had a really bad start with slugging, which may have been caused more by random chance than the other two. And obviously the distributional assumptions are an approximation to reality.</div>
</div>
</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/02/17/visualizing-correlations-of-matrix/">Visualizing the Correlations of a Matrix</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-02-17T00:00:00-05:00" pubdate data-updated="true">Feb 17<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
Correlation matrices are a common way to look at the dependence of a set of variables. When the variables have spatial relationships, the correlation matrix loses some information.<br /><br />Lets say you have repeated observations, each one being a matrix. For example, you could have yearly observations of health statistics for a spatial grid. Lets say the grid is n by p (n*p variables) and there are m observations of the grid. If we want to get the correlations of each element of the grid, the typical way to do that would be to convert the matrix of variables into a vector of length n*p, and then calculate the correlation matrix of the vector. When you do that, however, it will no longer be obvious which of the variables are on the same row/column or are close to each other. So the typical correlation matrix is not satisfactory.<br /><br />What I propose is a set of <a href="http://en.wikipedia.org/wiki/Small_multiple">small multiples </a>of correlation matrices. Instead of having an n*p by n*p correlation matrix, we will have an n by p grid of correlation matrices, each correlation matrix representing the correlation with the variable in that position of the matrix. Below is an example.<br /><br /><div class="separator" style="clear: both; text-align: center;"></div><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-uKlTWBzaYhQ/Tz7SB8nALQI/AAAAAAAAF7c/ioCipoIuVgg/s1600/matrix+cor+plot.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-uKlTWBzaYhQ/Tz7SB8nALQI/AAAAAAAAF7c/ioCipoIuVgg/s1600/matrix+cor+plot.png" /></a></div><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-lv9XCCr-k0E/Tz7PVojdkfI/AAAAAAAAF7U/tFH2H8im_3Q/s1600/matrix+cor+plot.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><br /></a></div>The above example is just random data, so all correlations are spurious. Blue is positive, red is negative, and white is no correlation. You can see in the row 1, column 1 matrix that the 1st row and 1st column is dark blue. This is because this is the correlation with itself. Similarly in all other rows and columns.<br /><br />Using a real example might display the usefulness more clearly. I am on a project estimating elements of a matrix with only the row totals and column totals. I simulated data many times and kept track of the errors. I was interested in how the errors in the different cells are correlated with each other. Below, you can see that the errors in the same row or column are positively correlated with each other, while the errors in other rows and columns are negatively correlated. This pops out at you with the below plot, but would have been difficult to figure out with a typical correlation matrix.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-wSVkWLxaxH8/Tz7NN6lNobI/AAAAAAAAF7M/wQ7T-B-3Kx0/s1600/Correlation+of+Errors+Matrix1.jpg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="400" src="http://1.bp.blogspot.com/-wSVkWLxaxH8/Tz7NN6lNobI/AAAAAAAAF7M/wQ7T-B-3Kx0/s400/Correlation+of+Errors+Matrix1.jpg" width="400" /></a></div><br />Code:<br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace; font-size: small;"># Generate random data for the example</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace; font-size: small;">reps=10<br />mat.data=array(0,c(4,5,reps))<br />for (i in 1:reps) {<br />&nbsp; mat.data[,,i]=matrix(rmultinom(1,20,rep(1,4*5)/(4*5)),4,5)<br />}</span><br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace; font-size: small;">matrix.cor.plot(mat.data)</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace; font-size: small;"><br /># the function<br />matrix.cor.plot &lt;- function(mat.data) {<br />&nbsp; #mat.data should be a nrow by ncol by nrep array<br />&nbsp; <br />&nbsp; nrow=dim(mat.data)[1]<br />&nbsp; ncol=dim(mat.data)[2]<br />#&nbsp;&nbsp; nrep=dim(mat.data)[3]<br />&nbsp; <br />&nbsp; par(mfrow=c(nrow,ncol),cex=.75,bty=&#8221;o&#8221;,mar=c(1, 1, 1, 1) + 0.1)<br />&nbsp; <br />&nbsp; # red is -1, white is 0, blue is +1<br />&nbsp; rgb.palette &lt;- colorRampPalette(c(&#8220;red&#8221;,&#8221;white&#8221;,&#8221;blue&#8221;), space = &#8220;rgb&#8221;)<br />&nbsp; <br />&nbsp; for (r in 1:nrow) {<br />&nbsp;&nbsp;&nbsp; for (c in 1:ncol) {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cor.mat=matrix(0,nrow,ncol)<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for (r2 in 1:nrow) { for (c2 in 1:ncol) {cor.mat[r2,c2]=cor(mat.data[r,c,],mat.data[r2,c2,]) } }<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cor.mat=t(cor.mat)<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cor.mat=cor.mat[,ncol(cor.mat):1]<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image(cor.mat,zlim=c(-1,1),col=rgb.palette(120),axes = FALSE,main=paste(&#8220;Row:&#8221;,r,&#8221;Col:&#8221;,c))<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; box()<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; }<br />}</span></div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/02/12/unsupervised-image-segmentation-with/">Unsupervised Image Segmentation With Spectral Clustering With R</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-02-12T00:00:00-05:00" pubdate data-updated="true">Feb 12<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
That title is quite a mouthful. This quarter, I have been reading papers on <a href="http://en.wikipedia.org/wiki/Spectral_clustering">Spectral Clustering</a> for a reading group. The basic goal of clustering is to find groups of data points that are similar to each other. Also, data points in one group should be dissimilar to data in other clusters. This way you can summarize your data by saying there are a few groups to consider instead of all the points. Clustering is an unsupervised learning method in that there are no &#8220;true&#8221; groups that you are comparing the clusters to.<br /><br />There are many ways to do this, two of the most popular are k-means and hierarchical clustering. Spectral clustering is nice because it gives you as much flexibility as you want to define how pairs of data points are similar or dissimilar. K-means only works well for data that are grouped in elliptically shaped, whereas spectral clustering can theoretically work well for any group. For example, the data in <a href="http://www.ml.uni-saarland.de/code/pSpectralClustering/images/clusters_11b_notitle2.png" target="_blank">this image</a> is easily clustered by spectral, but would not be by k-means. The flexibility of spectral clustering can also be a burden in that there are an infinite ways to group points.<br /><br />The basic idea (and all the flexibility) behind spectral clustering is that you define the similarity between any two data points however you want, and put them in a matrix. So if you have 100 data points, you will end up with a 100x100 matrix, where the rth row and cth column is the similarity between the rth data point and the cth data point. You can define &#8220;similarity&#8221; any way you want. Popular methods are Euclidean distance, a kernel function of the Euclidean distance, or a k nearest neighbors approach.<br /><br />Once you have the similarity matrix, you need to create a normalized/unnormalized <a href="http://en.wikipedia.org/wiki/Laplacian_matrix">Laplacian</a> matrix, then calculate the eigenvectors and eigenvalues of the the Laplacian. Finally, use the k-means algorithm on the eigenvalues corresponding to the k smallest eigenvectors. This will give you k clusters (something else you need to specify).<br /><br />The other day, someone in my office was working a project of <a href="http://en.wikipedia.org/wiki/Image_segmentation">Image Segmentation</a> (a topic I know nothing about) for a machine learning class. I thought this would be a perfect application for spectral clustering because you can define similarity of pixels in terms of both the contrast of the pixel as well as the proximity to nearby pixels. I downloaded a few pictures from the<span id="goog_358016683"></span><span id="goog_358016684"></span> <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/">Berkeley Segmentation Dataset Benchmark</a> website.<br /><br />One thing I quickly found was that even these moderately sized pictures were too big to create a similarity matrix for in R. A typical image is 481x321=154401 pixels. So a similarity matrix between all the pixels would be 154401x154401=23 billion elements. R only allows 2^31-1=2.1 billion elements in a matrix. Even if I could create the matrix, it would take forever to calculate the eigenvectors and eigenvalues. [Note: Some people from my department actually tackled <a href="http://www.stat.osu.edu/~taoshi/research/papers/2011_Schuetter_Shi_JCGS.pdf" target="_blank">this exact problem</a> using sampling methods.]<br /><br />So I had to reduce the size of the image. For this I just created an image of a factor of the original dimension (about 10 to 20 times smaller), and averaged the contrast of all the points that were collapsed. I also experimented with smoothing the image first and then doing the averaging In some cases it helped in some it hurt, I think.<br /><br />For example here is an original picture.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300/html/images/plain/normal/gray/86016.jpg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300/html/images/plain/normal/gray/86016.jpg" /></a></div><br />Then I smoothed using the image.smooth function of the fields package.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-Rd4S9Z1Y5IE/Tzf9PWxlGII/AAAAAAAAF6M/xBUzV_EMDrE/s1600/smooth.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://lh3.ggpht.com/-Rd4S9Z1Y5IE/Tzf9PWxlGII/AAAAAAAAF6M/xBUzV_EMDrE/s1600/smooth.jpeg" /></a></div>Then I reduced the dimension by a factor of 10 and averaged the original pixels. The resulting image is below. You can see there is a decent loss of information in the averaging.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-tzIV9TQYHVI/Tzf9cKaR9YI/AAAAAAAAF6U/2sne_VgLUJQ/s1600/averaged.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://lh3.ggpht.com/-tzIV9TQYHVI/Tzf9cKaR9YI/AAAAAAAAF6U/2sne_VgLUJQ/s1600/averaged.jpeg" /></a></div><br />Finally, for the similarity, I only considered pixels that were within 3 horizontally or vertically to be similar (otherwise 0). Also, for those within 3, I used a Gaussian kernel of the difference in contrast with variance equal to 0.01. I chose this number because the variance in the data was about 0.01. Varying both of these parameters wildly affected quality of the results. I also tried using a k nearest neighbors similarity and I did not get any good results. Hence, you can see both the positive and negative of the flexibility.<br /><br />Anyway, here are the two clusters (white and black) using the unnormalized Laplacian.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-dtUccbD9WeM/TzgAJH51ZAI/AAAAAAAAF6c/eUL8YFLOETE/s1600/clusters.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://lh3.ggpht.com/-dtUccbD9WeM/TzgAJH51ZAI/AAAAAAAAF6c/eUL8YFLOETE/s1600/clusters.jpeg" /></a></div><br />It looks very good and encouraging for future problems. As stated before, however, I am not sure how to determine the parameters for a generic problem.<br /><br />Overlaying the cluster on the original image, you can see the two segments of the image clearly. You can also see the loss in fidelity due to reducing the size of the image.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-FqY4mvTF2Yo/TzgKwX9ZlBI/AAAAAAAAF68/_WpfIetZVmc/s1600/overlay1b.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://lh3.ggpht.com/-FqY4mvTF2Yo/TzgKwX9ZlBI/AAAAAAAAF68/_WpfIetZVmc/s1600/overlay1b.jpeg" /></a></div><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-LDWLf0qEKCo/TzgAJciHcaI/AAAAAAAAF6k/c4veitNzH4A/s1600/overlay.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><br /></a></div>Here are a couple of other examples that worked well. With the airplane one, in particular, you can see that the clustering was able to identify an unusual shape. I was not able to get it to work well with more than two clusters, although I only tried one image that was not that easy.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-k-7m1FTg_fI/TzgKx9lj0XI/AAAAAAAAF7E/LDjpO3w1JBg/s1600/overlay3b.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://lh3.ggpht.com/-k-7m1FTg_fI/TzgKx9lj0XI/AAAAAAAAF7E/LDjpO3w1JBg/s1600/overlay3b.jpeg" /></a></div><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-EVSnrPb3CaA/TzgD21GoJoI/AAAAAAAAF60/y6NlonQ8Nw0/s1600/overlay3.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"></a></div><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-36TaE1FBMGg/TzgD2VmAOFI/AAAAAAAAF6s/W0SsqyG-J_M/s1600/overlay2.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://lh3.ggpht.com/-36TaE1FBMGg/TzgD2VmAOFI/AAAAAAAAF6s/W0SsqyG-J_M/s1600/overlay2.jpeg" /></a></div>For posterity, here is the code I used.<br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"># Import the image </span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">library(jpeg)<br /># http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300/html/images/plain/normal/gray/86016.jpg<br />rawimg=readJPEG(&#8220;segment.jpeg&#8221;)<br />rawimg=t(rawimg)<br />rawimg=rawimg[,ncol(rawimg):1]<br />image(rawimg,col = grey((0:12)/12))</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><br /></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"># Smooth the image</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">library(fields)<br />smoothimg=image.smooth(rawimg,theta=2)<br />image(smoothimg,col = grey((0:12)/12))<br /></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"># Reduce Size of Image</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">olddim=dim(rawimg)<br />newdim=c(round(olddim/10))<br />prod(newdim)&gt;2^31<br />img=matrix(NA,newdim[1],newdim[2])<br />for (r in 1:newdim[1]) {<br />&nbsp; centerx=(r-1)/newdim[1]*olddim[1]+1<br />&nbsp; lowerx=max(1,round(centerx-olddim[1]/newdim[1]/2,0))<br />&nbsp; upperx=min(olddim[1],round(centerx+olddim[1]/newdim[1]/2,0))<br />&nbsp; for (c in 1:newdim[2]) {<br />&nbsp;&nbsp;&nbsp; centery=(c-1)/newdim[2]*olddim[2]+1<br />&nbsp;&nbsp;&nbsp; lowery=max(1,round(centery-olddim[2]/newdim[2]/2,0))<br />&nbsp;&nbsp;&nbsp; uppery=min(olddim[2],round(centery+olddim[2]/newdim[2]/2,0))<br />&nbsp;&nbsp;&nbsp; img[r,c]=mean(smoothimg$z[lowerx:upperx,lowery:uppery])<br />&nbsp; }<br />}<br />image(img,col = grey((0:12)/12))</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><br /></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"># Convert matrix to vector</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">imgvec=matrix(NA,prod(dim(img)),3)<br />counter=1<br />for (r in 1:nrow(img)) {<br />&nbsp; for (c in 1:ncol(img)) {<br />&nbsp;&nbsp;&nbsp; imgvec[counter,1]=r<br />&nbsp;&nbsp;&nbsp; imgvec[counter,2]=c<br />&nbsp;&nbsp;&nbsp; imgvec[counter,3]=img[r,c]<br />&nbsp;&nbsp;&nbsp; <br />&nbsp;&nbsp;&nbsp; counter=counter+1<br />&nbsp; }<br />}<br /><br /></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"># Similarity Matrix</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">pixdiff=2<br />sigma2=.01 #</span><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">var(imgvec[,3])</span><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><br />simmatrix=matrix(0,nrow(imgvec),nrow(imgvec))<br />for(r in 1:nrow(imgvec)) {<br />&nbsp; cat(r,&#8221;out of&#8221;,nrow(imgvec),&#8221;\n&#8221;)<br />&nbsp; simmatrix[r,]=ifelse(abs(imgvec[r,1]-imgvec[,1])&lt;=pixdiff &amp; abs(imgvec[r,2]-imgvec[,2])&lt;=pixdiff,exp(-(imgvec[r,3]-imgvec[,3])^2/sigma2),0)<br />}<br />&nbsp;</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"># Weighted and Unweighted Laplacian</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">D=diag(rowSums(simmatrix))<br />Dinv=diag(1/rowSums(simmatrix))<br />L=diag(rep(1,nrow(simmatrix)))-Dinv %*% simmatrix<br />U=D-simmatrix<br /></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"># Eigen and k-means</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">evL=eigen(L,symmetric=TRUE)<br />evU=eigen(U,symmetric=TRUE)<br /><br />kmL=kmeans(evL$vectors[,(ncol(simmatrix)-1):(ncol(simmatrix)-0)],centers=2,nstart=5)<br />segmatL=matrix(kmL$cluster-1,newdim[1],newdim[2],byrow=T)<br />if(max(segmatL) &amp; sum(segmatL==1)&lt;sum(segmatL==0)) {segmatL=abs(segmatL-1)}<br /><br />kmU=kmeans(evU$vectors[,(ncol(simmatrix)-1):(ncol(simmatrix)-0)],centers=2,nstart=5)<br />segmatU=matrix(kmU$cluster-1,newdim[1],newdim[2],byrow=T)<br />if(max(segmatU) &amp;sum(segmatU==1)&lt;sum(segmatU==0)) {segmatU=abs(segmatU-1)}</span><br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"># Plotting the clusters</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">image(segmatL, col=grey((0:15)/15))<br />image(segmatU, col=grey((0:12)/12))<br /></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"># Overlaying the original and the clusters</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">############</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">image(seq(0,1,length.out=olddim[1]),seq(0,1,length.out=olddim[2]),rawimg,col = grey((0:12)/12),xlim=c(-.1,1.1),ylim=c(-.1,1.1),xlab=&#8221;&#8220;,ylab=&#8221;&#8220;)<br /><br />segmat=segmatU<br />linecol=2<br />linew=3<br />for(r in 2:newdim[1]) {<br />&nbsp; for (c in 2:newdim[2]) {<br />&nbsp;&nbsp;&nbsp; if(abs(segmat[r-1,c]-segmat[r,c])&gt;0) {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xloc=(r-1)/(newdim[1])<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ymin=(c-1)/(newdim[2])<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ymax=(c-0)/(newdim[2])<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; segments(xloc,ymin,xloc,ymax,col=linecol,lwd=linew)<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp; if(abs(segmat[r,c-1]-segmat[r,c])&gt;0) {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yloc=(c-1)/(newdim[2])<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xmin=(r-1)/(newdim[1])<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xmax=(r-0)/(newdim[1])<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; segments(xmin,yloc,xmax,yloc,col=linecol,lwd=linew)<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; }<br />}</span></div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/03/10/using-jmp-to-create-map/">Using JMP to Create a Map</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-03-10T00:00:00-05:00" pubdate data-updated="true">Mar 10<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
I am a big fan of SAS&#8217;s JMP software. It is the first statistical program I learned and I really like how the emphasize visualization. In their most recent update, JMP 9 now has the ability to create maps. I have been itching to test this out for a little while and I came across a <a href="http://blog.cgpgrey.com/how-many-americans-have-a-passport-the-percentages-state-by-state/">map on the internet </a>that I thought would be a good test. It is the percentage of the population of each state that has a passport.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://andrewsullivan.theatlantic.com/.a/6a00d83451c45669e2014e868f74e0970d-550wi" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="353" src="http://andrewsullivan.theatlantic.com/.a/6a00d83451c45669e2014e868f74e0970d-550wi" width="400" />&nbsp;</a></div><div class="separator" style="clear: both; text-align: center;"><br /></div><br />Luckily, the website has the source data so I &#8220;jumped&#8221; right in. It was really easy.<br /><ol><li>Copy and paste the data into JMP.&nbsp;</li><li>Open the Graph Builder under the Graph menu.&nbsp;</li><li>Drag the State field into the shape area on the lower left corner.</li></ol><div class="separator" style="clear: both; text-align: center;"><a href="https://lh4.googleusercontent.com/-lfAtOpUGw0w/TXltfam1YpI/AAAAAAAAFrc/9Prld6kfodc/s1600/JMPMAP1.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="404" src="https://lh4.googleusercontent.com/-lfAtOpUGw0w/TXltfam1YpI/AAAAAAAAFrc/9Prld6kfodc/s640/JMPMAP1.png" width="640" /></a></div>You can see the outline of the USA in the map. JMP recognizes that the state field is filled with US state names, so it knows to open the shapefile of the US states.<br />&nbsp;&nbsp;&nbsp; 4. Drag the Population with Passport field onto the main map. You can also drag it into the Color area.<br /><div class="separator" style="clear: both; text-align: center;"><a href="https://lh6.googleusercontent.com/-DPzzTB6dRYc/TXlvrZUn45I/AAAAAAAAFrg/8JoGc11lp5Y/s1600/JMPMAP2.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="377" src="https://lh6.googleusercontent.com/-DPzzTB6dRYc/TXlvrZUn45I/AAAAAAAAFrg/8JoGc11lp5Y/s640/JMPMAP2.png" width="640" /></a></div><br />&nbsp;&nbsp;&nbsp; 5. Right click on the color and select &#8220;Gradient&#8230;&#8221; to customize the colors as you like. I changed it to &#8220;White to Blue&#8221; and checked the &#8220;Reverse Colors&#8221; check box to match the original map.<br /><br />Below is the final result. Very quick and easy with a pretty result.<br /><div class="separator" style="clear: both; text-align: center;"><a href="https://lh3.googleusercontent.com/-uahR9PsA2D4/TXlvrybSM0I/AAAAAAAAFrk/FB-iMviw_oA/s1600/JMPMAP3.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://lh3.googleusercontent.com/-uahR9PsA2D4/TXlvrybSM0I/AAAAAAAAFrk/FB-iMviw_oA/s1600/JMPMAP3.png" /></a></div></div>
<h2>Comments</h2>
<div class='comments'>
<div class='comment'>
<div class='author'>Xan Gregg</div>
<div class='content'>
Nice work. You might try the reverse the labels instead of the colors &#8211; I usually like the bigger numbers to be at the top of the legend list. (I also like bigger numbers to be associated with darker colors, but I realize you&#39;re trying to duplicate that feature of the original.)</div>
</div>
</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2010/12/31/empirical-bayes-estimation-of-on-base/">Empirical Bayes Estimation of on Base Percentage</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2010-12-31T00:00:00-05:00" pubdate data-updated="true">Dec 31<span>st</span>, 2010</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
I guess you could call this On Bayes Percentage. *cough*<br /><br />Fresh off learning Bayesian techniques in one of my classes last quarter, I thought it would be fun to try to apply the method. I was able to find some examples of <a href="http://en.wikipedia.org/wiki/Hierarchical_Bayes_model">Hierarchical Bayes </a>being used to analyze baseball data at <a href="http://www-stat.wharton.upenn.edu/%7Estjensen/">Wharton</a>. <br /><br /><b>Setting up the problem</b><br />On base percentage (OBP) is probably the most important basic offensive statistic in baseball. Getting a reliable estimate of a players true ability to get on base is therefore important. The basic problem is that the sample size we get from one season rarely has enough observations so that we are certain of a player&#8217;s ability. Even though there are 162 games in a season, there is a possibility that the actual OBP is the result of luck rather than skill. Bayesian analysis will &#8220;regress&#8221; the actual observed OBP to the mean, in that if a player has a small number of plate appearances (PA) it doesn&#8217;t give them very much weight and the result will be something closer to the overall (MLB) average. On the other hand, if a player has quite a few PAs then it believes that the results are not the result of luck and it gives the observations a lot of weight.<br />We are trying to estimate the &#8220;true&#8221; OBP of each batter. Bayesian analysis assumes that the true OBP is random. <a href="http://en.wikipedia.org/wiki/Empirical_Bayes_method">Empirical Bayes</a> is a method of figuring out the distribution of &#8220;true&#8221; OBP using the data. OBP is times on base divided by PA. Times on base (X) for each batter is distributed <a href="http://en.wikipedia.org/wiki/Binomial_distribution">binomial</a> with n=PA and p=true OBP. We further assume that p is distributed <a href="http://en.wikipedia.org/wiki/Beta_distribution">Beta</a> with parameters a and b. It follows from this that the marginal distribution of X is distributed according to the distribution:<br />gamma(a+b)*gamma(a+x)*gamma(n-x+b)*(n choose x)/(gamma(a)*gamma(b)*gamma(a+b+n))<br />where gamma is the <a href="http://en.wikipedia.org/wiki/Gamma_function">gamma function</a>.<br />We will estimate the parameters a and b based on the data (X), using its marginal distribution (the &#8220;empirical&#8221; part of Bayes). To do this I found that likelihood of the marginal distribution of all the batters. Then I maximized this likelihood by adjusting the parameters a and b. This is called the ML-II.<br /><br /><b>The Analysis</b><br />I used data for all non-pitchers in 2010. I assume that each player is independent. In doing that, I just have to multiply all the marginals for each player together to get the likelihood. When I do this and maximize it with respect to a and b, I get estimates that a = 83.48291 and b = 174.9038. I think this can be interpreted that prior mean (what we would assume that average OBP of a batter is before seeing him bat) is a/(a+b) = 0.323. This is pretty close to what the overall OBP of the league was (0.330). I think it makes sense that the prior is lower than the league average because batters who do well will get more opportunities and players that do poorly will get fewer. So the league average is biased high. <br />Below is a graph of the prior distribution and the updated posteriors of every batter. You can (sort of) see that the posteriors have tighter distributions than the prior does. (The posterior distribution of each batter in this case is the distribution of OBP after we have observed PA and the actual OBP.)<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/_qD9db1gpElc/TR1lbFd8yfI/AAAAAAAAFhQ/IkoRr-9FrZw/s1600/dist.jpg" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/_qD9db1gpElc/TR1lbFd8yfI/AAAAAAAAFhQ/IkoRr-9FrZw/s1600/dist.jpg" /></a></div><br /><br />One way to see why this Bayesian analysis is useful is to compare the posterior means with the observed OBP. If someone has only a few PAs, their OBP could be very high or very low and this may mislead you into thinking that this batter is very good or bad. However, the posterior mean takes into account the number of PAs. Below is a graph comparing the two. You can see that the range of values for posterior mean is pretty small, especially compare to actual OBP.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/_qD9db1gpElc/TR1mzX6IK2I/AAAAAAAAFhU/hlrgx0oDK38/s1600/OBPvsPostMean.jpg" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/_qD9db1gpElc/TR1mzX6IK2I/AAAAAAAAFhU/hlrgx0oDK38/s1600/OBPvsPostMean.jpg" /></a></div><br />Here is a list of the highest posterior mean OBP:<br /><br /><table border="0" cellpadding="0" cellspacing="0" style="width: 252px;"><colgroup><col style="width: 87pt;" width="116"></col>  <col style="width: 54pt;" width="72"></col>  <col style="width: 48pt;" width="64"></col>  </colgroup><tbody><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="height: 15pt; width: 87pt;" width="116"><b>Batter</b></td>   <td class="xl65" style="border-left: medium none; width: 54pt;" width="72"><b>Posterior Mean</b></td>   <td class="xl65" style="border-left: medium none; width: 48pt;" width="64"><b>Actual OBP</b></td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Joey Votto</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.396</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.424</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Miguel Cabrera</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.392</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.420</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Albert Pujols</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.390</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.414</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Justin Morneau</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.388</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.437</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Josh Hamilton</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.383</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.411</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Prince Fielder</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.380</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.401</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Shin-Soo Choo</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.379</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.401</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Kevin Youkilis</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.379</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.412</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Joe Mauer</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.378</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.402</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Adrian   Gonzalez</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.374</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.393</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Daric Barton</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.374</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.393</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Jim Thome</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.373</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.412</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Paul Konerko</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.373</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.393</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Jason Heyward</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.373</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.393</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Matt Holliday</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.371</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.390</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Carlos Ruiz</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.371</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.400</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Manny Ramirez</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.371</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.409</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Billy Butler</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.370</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.388</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Jayson Werth</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.370</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.388</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl65" height="20" style="border-top: medium none; height: 15pt;">Ryan Zimmerman</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.369</td>   <td align="right" class="xl66" style="border-left: medium none; border-top: medium none;">0.388</td>  </tr></tbody></table><br />And here is a list of the lowest posterior mean OBP:<br /><br /><table border="0" cellpadding="0" cellspacing="0" style="width: 245px;"><colgroup><col style="width: 98pt;" width="131"></col>  <col style="width: 29pt;" width="39"></col>  <col style="width: 56pt;" width="75"></col>  </colgroup><tbody><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="height: 15pt; width: 98pt;" width="131"><b>Batter</b></td>   <td class="xl66" style="border-left: medium none; width: 29pt;" width="39"><b>Posterior Mean</b></td>   <td class="xl66" style="border-left: medium none; width: 56pt;" width="75"><b>Actual OBP</b></td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Brandon Wood</td>   <td align="right" class="xl65">0.252</td>   <td align="right" class="xl67" style="border-top: medium none;">0.175</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Pedro Feliz</td>   <td align="right" class="xl65">0.271</td>   <td align="right" class="xl67" style="border-top: medium none;">0.240</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Jeff Mathis</td>   <td align="right" class="xl65">0.276</td>   <td align="right" class="xl67" style="border-top: medium none;">0.219</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Garret   Anderson</td>   <td align="right" class="xl65">0.277</td>   <td align="right" class="xl67" style="border-top: medium none;">0.204</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Adam Moore</td>   <td align="right" class="xl65">0.281</td>   <td align="right" class="xl67" style="border-top: medium none;">0.230</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Josh Bell</td>   <td align="right" class="xl65">0.285</td>   <td align="right" class="xl67" style="border-top: medium none;">0.224</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Jose Lopez</td>   <td align="right" class="xl65">0.286</td>   <td align="right" class="xl67" style="border-top: medium none;">0.270</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Peter Bourjos</td>   <td align="right" class="xl65">0.287</td>   <td align="right" class="xl67" style="border-top: medium none;">0.237</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Aaron Hill</td>   <td align="right" class="xl65">0.287</td>   <td align="right" class="xl67" style="border-top: medium none;">0.271</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Tony Abreu</td>   <td align="right" class="xl65">0.288</td>   <td align="right" class="xl67" style="border-top: medium none;">0.244</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Koyie Hill</td>   <td align="right" class="xl65">0.291</td>   <td align="right" class="xl67" style="border-top: medium none;">0.254</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Gerald Laird</td>   <td align="right" class="xl65">0.291</td>   <td align="right" class="xl67" style="border-top: medium none;">0.263</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Drew Butera</td>   <td align="right" class="xl65">0.291</td>   <td align="right" class="xl67" style="border-top: medium none;">0.237</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Jeff Clement</td>   <td align="right" class="xl65">0.291</td>   <td align="right" class="xl67" style="border-top: medium none;">0.237</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Matt Carson</td>   <td align="right" class="xl65">0.291</td>   <td align="right" class="xl67" style="border-top: medium none;">0.193</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Humberto   Quintero</td>   <td align="right" class="xl65">0.292</td>   <td align="right" class="xl67" style="border-top: medium none;">0.262</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Wil Nieves</td>   <td align="right" class="xl65">0.292</td>   <td align="right" class="xl67" style="border-top: medium none;">0.244</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Matt   Tuiasosopo</td>   <td align="right" class="xl65">0.292</td>   <td align="right" class="xl67" style="border-top: medium none;">0.234</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Luis Montanez</td>   <td align="right" class="xl65">0.292</td>   <td align="right" class="xl67" style="border-top: medium none;">0.155</td>  </tr><tr height="20" style="height: 15pt;">   <td class="xl66" height="20" style="border-top: medium none; height: 15pt;">Cesar Izturis</td>   <td align="right" class="xl65">0.292</td>   <td align="right" class="xl67" style="border-top: medium none;">0.277</td>  </tr></tbody></table><br />You can see that all of the posterior means are pulled closer to the overall mean (the good players look worse and the bad players look better). The order changes a little bit but not too much. <br /><br />You can see the effect of sample size (PAs) by comparing Justin Morneau with Joey Votto. Morneau had a higher OBP, but Votto ended up with a higher posterior mean because he had more PAs (Votto had 648 while Morneau had 348). Here are their posterior distributions:<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/_qD9db1gpElc/TR1tYF814sI/AAAAAAAAFhY/4-gKxwH1JgY/s1600/distcompare.jpg" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/_qD9db1gpElc/TR1tYF814sI/AAAAAAAAFhY/4-gKxwH1JgY/s1600/distcompare.jpg" /></a></div><br /><br />Because of the additional PAs, you can see that the distribution of Votto is a little tighter than Morneau. We are more sure that Votto is excellent than we are sure that Morneau is excellent.</div>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/3/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2013/04/21/what-is-probability-of-16-seed-beating/">What Is the Probability of a 16 Seed Beating a 1 Seed?</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/04/18/easily-access-academic-journals-off/">Easily Access Academic Journals Off Campus With a Firefox Bookmark</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/02/24/copying-data-from-excel-to-r-and-back_24/">Copying Data From Excel to R and Back</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/01/23/text-decryption-using-mcmc/">Text Decryption Using MCMC</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/01/14/restricted-boltzmann-machines-in-r/">Restricted Boltzmann Machines in R</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Andrew Landgraf -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
