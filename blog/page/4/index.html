
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Statistically Significant</title>
  <meta name="author" content="Andrew Landgraf">

  
  <meta name="description" content="I have been toying around with Kaggle&#8217;s Million Song Dataset Challenge recently because I have some interest in collaborative filtering (using &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://andland.github.io/blog/page/4">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Statistically Significant" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-1827475-3']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>


<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>


<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Statistically Significant</a></h1>
  
    <h2>Andrew Landgraf's Blog</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:andland.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/about">About</a></li>
  <li><a href="/blog/archives">Blog Archives</a></li>
  <li><a href="/projects">Shiny</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/07/20/the-magical-sparse-matrix/">The Magical Sparse Matrix</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-07-20T00:00:00-04:00" pubdate data-updated="true">Jul 20<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/07/20/the-magical-sparse-matrix/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
I have been toying around with Kaggle&#8217;s <a href="http://www.kaggle.com/c/msdchallenge/">Million Song Dataset Challenge</a> recently because I have some interest in <a href="http://en.wikipedia.org/wiki/Collaborative_filtering">collaborative filtering</a> (using <a href="http://www2.research.att.com/%7Evolinsky/papers/ieeecomputer.pdf" target="_blank">matrix factorization</a>). I haven&#8217;t made much progress with the competition (all 3 of my submissions are below the baseline), but I have learned a few things about dealing with large amounts of data.<br /><br />The goal of the competition is to predict the 500 most likely songs each of 110,000 users will listen to next. As the name implies, there are 1,000,000 songs in the full dataset. To simplify things, I decided to concentrate on the most popular songs. I created a 110,000 x 2,000 matrix of 0&#8217;s and 1&#8217;s. Row i, column j is 1 if user i had listened to song j (the jth most popular song) and 0 if user i had not. As you can imagine, there are a lot more 0&#8217;s than 1&#8217;s in this matrix. The first few rows and columns look like this:<br /><br /><div style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: small;">0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 &#8230;</span></div><div style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: small;">0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 &#8230;</span></div><div style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: small;">1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 &#8230;</span></div><div style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: small;">&#8230;</span></div><br />This matrix was about 430 Mb and took a while to load into MATLAB. So I wisened up and created a <a href="http://en.wikipedia.org/wiki/Sparse_matrix">sparse matrix</a>. A sparse matrix realizes that most of the values are 0&#8217;s and does not record them. Instead, it lists the locations of the non-zero elements and what the value is. For example, this is what the first few rows of the sparse matrix looks like:<br /><br /><div style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: small;">1 3 1<br />1 7 1<br />1 10 1<br />1 13 1<br />1 82 1<br />1 717 1<br />2 1111 1<br />2 2972 1<br />2 3516 1</span></div><div style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: small;">&#8230;</span></div><br />The first column is the row number, the second is the column number, and the third is the value at that location. In this application, all the values are 1. For this matrix, I used the 50,000 most popular songs (instead of just 2,000), and the size was much smaller &#8211; just 17 Mb.<br /><br />It is easy to load the sparse matrix into MATLAB with the <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">spconvert</span> command, and many of MATLAB&#8217;s functions (like singular value decomposition) are optimized for sparse matrices.</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/07/19/random-forest-variable-importance/">Random Forest Variable Importance</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-07-19T00:00:00-04:00" pubdate data-updated="true">Jul 19<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/07/19/random-forest-variable-importance/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
<a href="http://en.wikipedia.org/wiki/Random_forest">Random</a> <a href="http://www.stat.berkeley.edu/%7Ebreiman/RandomForests/">forests</a> â„¢ are great. They are one of the best &#8220;black-box&#8221; supervised learning methods. If you have lots of data and lots of predictor variables, you can do worse than random forests. They can deal with messy, real data. If there are lots of extraneous predictors, it has no problem. It automatically does a good job of finding interactions as well. There are no assumptions that the response has a linear (or even smooth) relationship with the predictors.<br /><br />As cited in the Wikipedia article, they do lack some interpretability. But what they lack in interpretation, they more than make up for in prediction power, which I believe is much more important than interpretation in most cases. Even though you cannot easily tell how one variable affects the prediction, you can easily create a <i>partial dependence plot</i> which shows how the response will change as you change the predictor. You can also do this for two variables at once to see the interaction of the two.<br /><br />Also helping in the interpretation is that they can output a list of predictor variables that they believe to be important in predicting the outcome. If nothing else, you can subset the data to only include the most &#8220;important&#8221; variables, and use that with another model. The randomForest package in R has two measures of importance. <a href="http://rss.acs.unt.edu/Rdoc/library/randomForest/html/importance.html">One </a>is &#8220;total decrease in node impurities from splitting on the variable, averaged over all trees.&#8221; I do not know much about this one, and will not talk about it further. The other is based on a <a href="http://en.wikipedia.org/wiki/Resampling_%28statistics%29#Permutation_tests">permutation test</a>. The idea is that if the variable is not important (the null hypothesis), then rearranging the values of that variable will not degrade prediction accuracy. Random forests use out-of-bag (OOB) samples to measure prediction accuracy.<br /><br />In my experience, it does a pretty good job of finding the most important predictors, but it has issues with correlated predictors. For example, I was working on a problem where I was predicting the price that electricity trades. One feature that I knew would be very important was the amount of electricity being used at that same time. But I thought there might also be a relationship between price and the electricity being used a few hours before and after. When I ran the random forest with these variables, the electricity used 1 hour after was found to be more important than the electricity used at the same time. When including the 1 hour after electricity use instead of the current hour electricity use, the cross validation (CV) error increased. Using both did not significantly change the CV error compared to using just current hour. Because the electricity used at the current hour and the hour after are very correlated, it had trouble telling which one was more important. In truth, given the electricity use at the current hour, the electricity use at the hour after did not improve the predictive accuracy.<br /><br />Why does the importance measure give more weight correlated predictors? <a href="http://www.biomedcentral.com/1471-2105/9/307/">Strobl et al.</a> give some intuition behind what is happening and propose a solution. Basically, the permutation test is ill-posed. The permutation test is testing that the variable is independent of the response as well as all other predictors. Since the correlated predictors are obviously not independent, we get high importance scores. They propose a permutation test where you condition on the correlated predictors. This is a little tricky when the correlated predictors are continuous, but you can read the paper for more details.<br /><br />Another way to think of it is that, since each split only considers a subset of the possible variables, a variable that is correlated with an &#8220;important&#8221; variable may be considered without the &#8220;important&#8221; variable. This would cause the correlated variable to be selected for the split. The correlated value does hold some predictive value, but only because of the truly important variable, so it is understandable why this procedure would consider it important.<br /><br />I ran a simulation experiment (similar to the one in the paper) to demonstrate the issue. I simulated 5 predictor variables. Only the first one is related to the response, but the second one has a correlation of about 0.7 with the first one. Luckily, Strobl et al. included their version of importance in the package <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">party</span> in <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">R</span>. I compare variable importance from the <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">randomForest </span>package and the importance with and without taking correlated predictors into account from the <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">party<span style="font-family: inherit;"> </span></span><span style="font-family: inherit;">pac</span>kage.<br /><br /><pre class="brush: r"># simulate the data<br />x1=rnorm(1000)<br />x2=rnorm(1000,x1,1)<br />y=2*x1+rnorm(1000,0,.5)<br />df=data.frame(y,x1,x2,x3=rnorm(1000),x4=rnorm(1000),x5=rnorm(1000))<br /><br /># run the randomForest implementation<br />library(randomForest)<br />rf1 &lt;- randomForest(y~., data=df, mtry=2, ntree=50, importance=TRUE)<br />importance(rf1,type=1)<br /><br /># run the party implementation<br />library(party)<br />cf1 &lt;- cforest(y~.,data=df,control=cforest_unbiased(mtry=2,ntree=50))<br />varimp(cf1)<br />varimp(cf1,conditional=TRUE)<br /></pre><br />For the <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">randomForest</span>, the ratio of importance of the the first and second variable is 4.53. For <span style="font-family: &quot;Courier New&quot;,Courier,monospace;">party</span> without accounting for correlation it is 7.35. And accounting for correlation, it is 369.5. The higher ratios are better because it means that the importance of the first variable is more prominent. party&#8217;s implementation is clearly doing the job.<br /><br />There is a downside. It takes much longer to calculate importance with correlated predictors than without. For the party package in this example, it took 0.39 seconds to run without and 204.34 seconds with. I could not even run the correlated importance on the electricity price example. There might be a research opportunity to get a quicker approximation.<br /><br />Possibly up next: confidence limits for random forest predictions.</div>
<h2>Comments</h2>
<div class='comments'>
<div class='comment'>
<div class='author'>Abhijeet Patil</div>
<div class='content'>
hi,m new to random forest.. i ve some doubts..<br />1)&quot;we need to choose m number of variables randomly for each node..&quot;-can u pleas explain it..<br />OR<br />2)&quot;take 1 bootstrap sample,choose some variables and create a decision tree&quot;-is it correct??<br /></div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
Could you tell me what the numbers mean when ctree and randonForest return variable importance numbers please? I can get the output but I don&#39;t know how to interpret them.<br /><br />Than you very much.</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
HI Andrew, When i tried to calculate the variable importance, I get the error <br />&quot;Error in model.matrix.default(as.formula(f), data = blocks) : <br />  allocMatrix: too many elements specified&quot;<br /><br />I&#39;m not sure how to deal with it because I have 2 factor levels which I&#39;m predicting with 23 continuous variable predictors (about 2200 data points. I know it&#39;s a lot to work with, but not sure how to get the correct VarImp values with this large data set.<br /><br />I tried increasing the threshold, but it didn&#39;t help. fit.varimp=varimp(fit.cf,threshold=0.8,conditional=TRUE)<br /><br />Do you have any suggestions? Thanks.</div>
</div>
<div class='comment'>
<div class='author'>Andrew Landgraf</div>
<div class='content'>
Unfortunately, I didn&#39;t set the seed. #nexttime</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
you should provide the seed to reproduce your example</div>
</div>
<div class='comment'>
<div class='author'>Andrew</div>
<div class='content'>
It should be able to give you a solution. I have no idea what ramifications this would have on accuracy or variable importance.</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
I have microarray data on 30 entries and thought of using RF to identify important response genes. I understand that this is a N&lt;&lt;p type of problems, but will &#39;party&#39; be able to come up with a solution?</div>
</div>
<div class='comment'>
<div class='author'>Andrew</div>
<div class='content'>
The simple answer is using the partialPlot function in the randomForest package for R (http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=randomForest:partialPlot). <br /><br />What it is doing isn&#39;t all that complicated. Say you want a partial dependence plot for the variable X_1. For each value of X_1=x that you want to plot, you take the average of the prediction with X_1=x and the other explanatory variables equal to the n values that they are in the data set. You are trying to average out the other variables.</div>
</div>
<div class='comment'>
<div class='author'>nanounanue</div>
<div class='content'>
Great explanation!<br /><br />I have a question for you: In your text you said:<br /><br />&quot;&quot;&quot;<br /> &#8230; Even though you cannot easily tell how one variable affects the prediction, you can easily create a partial dependence plot which shows how the response will change as you change the predictor. You can also do this for two variables at once to see the interaction of the two.<br />&quot;&quot;&quot;<br /><br />Could you provide, please, an example of how build the partial dependence plot?<br /><br /><br />Thank you for the post and thanks in advance</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
Great post!  Thank you.</div>
</div>
</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/06/15/rounding-in-r/">Rounding in R</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-06-15T00:00:00-04:00" pubdate data-updated="true">Jun 15<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/06/15/rounding-in-r/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
Forgive me if you are already aware of this, but I found it quite alarming. I know that most code is interpreted by the computer in binary and we input in decimal, so problems can arise in conversion and with <a href="http://en.wikipedia.org/wiki/Floating_point">floating point</a>. But the example I have below is so simple that it really surprised me.<br /><br />I was converting a function from R into MATLAB so that a colleague could use it. I tested it out on the same data and got slightly different results. Digging into the problem, the difference was due to the fact that R was rounding 4.5 to 4 and MATLAB was rounding it to 5. I thought the &#8220;4.5&#8221; must have really been &#8220;4.49999&#8230;&#8221;. But that was not so.<br /><br />For example, this is the result of the <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/Round.html">round</a> function for a few numbers.<br /><pre class="brush: r">&gt; round(0.5,0)<br />[1] 0<br />&gt; round(1.5,0)<br />[1] 2<br />&gt; round(2.5,0)<br />[1] 2<br />&gt; round(3.5,0)<br />[1] 4<br />&gt; round(4.5,0)<br />[1] 4<br />&gt; round(5.5,0)<br />[1] 6<br />&gt; round(6.5,0)<br />[1] 6<br /></pre><br />Do you see a pattern?<br /><br />I tried this on versions 2.13.1 and 2.14.0. I ran the same with MATLAB and it gave the expected results. I am not any kind of expert on computer sciences, so I was not sure why this is happening. <a href="http://www.mathsisfun.com/binary-decimal-hexadecimal-converter.html">Converting </a>any decimal number that ends in .5 into binary results in a finite length binary number. For example, 4.5 is 100.1 in binary. Because of this, I wouldn&#8217;t think the error would be due to floating points, but I really don&#8217;t know.<br /><br />Looking at the <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/Round.html">documentation</a> for round, I found the reason. It states in the notes, &#8220;Note that for rounding off a 5, the <a href="http://en.wikipedia.org/wiki/IEEE_754-2008#Rounding_rules">IEC 60559 standard</a> is expected to be used, â€˜<em>go to the even digit</em>â€™.&#8221; It is a little comforting knowing that there is a logic behind it and that R is abiding to some standard. But why isn&#8217;t MATLAB abiding by the same standard? Also, I think most people expect numbers ending in .5 to round up, not the nearest even digit.</div>
<h2>Comments</h2>
<div class='comments'>
<div class='comment'>
<div class='author'>Analytic Bastard</div>
<div class='content'>
kudos Blaise</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
Andrew wrote &quot;Also, I think most people expect numbers ending in .5 to round up (not the nearest even digit)&quot;. This kind of rounding is in German called &quot;kaufmÃ¤nnische Rundung&quot; (rounding in commerce). For this purpose I use the following function:<br /><br />#Definition of a function for &quot;rounding in commerce&quot;<br />cround = function(x,n){<br />vorz = sign(x)<br />z = abs(x)*10^n<br />z = z + 0.5<br />z = trunc(z)<br />z = z/10^n<br />z*vorz<br />}<br /><br /># Example<br />&gt; round(seq(0.5,6.5,1),0)<br />[1] 0 2 2 4 4 6 6<br />&gt; cround(seq(0.5,6.5,1),0)<br />[1] 1 2 3 4 5 6 7</div>
</div>
<div class='comment'>
<div class='author'>cellocgw</div>
<div class='content'>
This &quot;round to even&quot; approach has been accepted by just about everyone (except matlab, and no surprise, except Msoft Excel).  <br />Sadly, the flame wars over &quot;round to even&quot; vs. &quot;round up&quot; continue, rather the way people argue about &quot;0.999&#8230; != 1&quot;<br /><br />PS: @a Tom:  I&#39;m highly skeptical of your <br />claim about 2.46&#8211;&gt;3.  Do you have a citation?</div>
</div>
<div class='comment'>
<div class='author'>a Tom</div>
<div class='content'>
I&#39;m ever amazed that something so seemingly basic can have so many different approaches.<br /><br />I understand that in many middle east countries they start with the far right digit and round up or down, so 2.46 is rounded to 3!</div>
</div>
<div class='comment'>
<div class='author'>Blaise</div>
<div class='content'>
This is discussed in Don Knuth&#39;s 1973 classic Seminumerical Algorithms. He gives the following example of what can happen when 5s are always rounded upwards. Suppose u = 10000000 and v = 0.5555556. Then u + v = 1.5555556. If we subtract v from this  result we get u&#39; = 1.0000001. Adding and then subtracting v from u&#39; and we get 1.0000002 and if we do it again we get 1.0000003 and so on. He says &quot;This phenomenon, called drift, will not occur when we use a stable rounding rule based on the parity of the least significant digit.&quot;</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
I was the #2 anonymous poster. Echoing Ben, I think that for ease of teaching, the &quot;round 5 up&quot; method is taught to children (and adults?) below the university level, and only if you go on for advance work is the more complicated method taught.<br /><br />Can you imagine trying to teach a 10 or 12 year old the IEC 60559 standard? Unfortunately, this is the method most adults are used to&#8230;<br /><br />I agree, it is a little troubling that Matlab doesn&#39;t abide by the standard. Yet another reason to stick with R!</div>
</div>
<div class='comment'>
<div class='author'>Ben Bolker</div>
<div class='content'>
Wikipedia ( http://en.wikipedia.org/wiki/Rounding#Round_half_to_even ) says of round-to-even:<br /><br />This method also treats positive and negative values symmetrically, and therefore is free of overall bias if the original numbers are positive or negative with equal probability. In addition, for most reasonable distributions of y values, the expected (average) value of the rounded numbers is essentially the same as that of the original numbers, even if the latter are all positive (or all negative). However, this rule will still introduce a positive bias for even numbers (including zero), and a negative bias for the odd ones.<br /><br />So round-to-even seems to have *slightly* better numerical properties than &quot;round ties away from zero&quot;, which is what is (I think) most often taught, because it&#39;s easier to understand. http://www.mathworks.com/matlabcentral/fileexchange/6752 gives a MATLAB function for &quot;round to even&quot;.<br /><br />If I had to guess I would predict that in borderline cases (which this certainly is) MATLAB would favor &quot;do what will lead to happier users&quot; and R would favor &quot;do what is thought to be the best numerical practice&quot;.</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
Hi,<br />I&#39;m not sure I understand what you mean by &quot;expected results&quot;?<br /><br />Regarding rounding, I was taught to round numbers ending in &quot;1, 2, 3, and 4&quot; *down*, and numbers that ended in &quot;6, 7, 8, 9&quot; *up*. Then, specifically regarding &quot;5&quot;, if the preceding digit is odd, round up and if the preceding digit is even, to round down. <br /><br />As you can see, this will then result in 50% of the numbers being rounded up, and 50% rounded down. If you round *down* on &quot;1, 2, 3, 4&quot; and round up on &quot;5, 6, 7, 8, 9&quot; you are rounding up 5/9th&#39;s of the time, and so introducing a bias.<br /><br />It sounds like R is handling it the way I would. Is that what you were wondering about?</div>
</div>
<div class='comment'>
<div class='author'>Anonymous</div>
<div class='content'>
To learn something about how computers handle numbers, especially as it relates to statistics and econometrics:<br /><br />B. D. McCullough and H. D. Vinod<br />&quot;The Numerical Reliability of Econometric Software,&quot;<br />Journal of Economic Literature 37(2), 633-665, 1999 <br /><br />A temporary link is available here:<br />http://www.pages.drexel.edu/~bdm25/jel.pdf</div>
</div>
</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/05/30/space-time-swing-probability-plot-for/">Space Time Swing Probability Plot for Ichiro</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-05-30T00:00:00-04:00" pubdate data-updated="true">May 30<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/05/30/space-time-swing-probability-plot-for/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
I was having some fun with PITCHf/x data and generalize additive models. <a href="http://en.wikipedia.org/wiki/PITCHf/x">PITCHf/x</a> keeps track of the trajectory, path, location of every pitch in the MLB. It is pretty accurate and opens up baseball to more analyses than ever before. <a href="http://en.wikipedia.org/wiki/Generalized_additive_model">Generalized additive models</a> (GAMs) are statistical models that put minimal assumptions on the type of model you are fitting. Traditional statistical models are linear, in that they assume that the response variable you are modelling is a linear function of the explanatory variables. GAMs just assumes that the relationship is &#8220;smooth.&#8221; <a href="http://www.stat.ubc.ca/%7Ematias/rgam/binomial.png">Here </a>is a good example of a relationship that may have traditionally been modeled as linear, but it is a much better assumption that the relationship is smooth.<br /><br />I fit a GAM to PITCHf/x data. The response is whether or not Ichiro swung. The explanatory variables are pitch location on the x, pitch location on the z, and the day of the year. Obviously, we expect the probability of swinging to change as the pitch is closer or further away from the center of the strike zone. Additionally, I was interested in seeing his swinging propensity changed as the year went on.<br /><br /><div class="separator" style="clear: both; text-align: center;"><object class="BLOGGER-youtube-video" classid="clsid:D27CDB6E-AE6D-11cf-96B8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0" data-thumbnail-src="http://i.ytimg.com/vi/44X9VXXjYPI/0.jpg" height="266" width="320"><param name="movie" value="http://www.youtube.com/v/44X9VXXjYPI?version=3&f=user_uploads&c=google-webdrive-0&app=youtube_gdata" />  <param name="bgcolor" value="#FFFFFF" />  <embed width="320" height="266"  src="http://www.youtube.com/v/44X9VXXjYPI?version=3&f=user_uploads&c=google-webdrive-0&app=youtube_gdata" type="application/x-shockwave-flash"></embed></object></div><br />You can see that the probability of swinging is smooth in both location and time. Also, you can see (ever so slightly) that the probability of swinging increased as the year went on. Looking at <a href="http://www.baseball-reference.com/players/split.cgi?id=suzukic01&amp;year=2010&amp;t=b#half">the splits</a>, you can see that his walk percentage was 28/395 (7.1%) in the first half and 17/337 (5.0%) in the second half. This is in agreement with the swing probability increasing,<br /><br />I used the <a href="http://people.bath.ac.uk/sw283/mgcv/tampere/mgcv.pdf">mgcv </a>package in R to run the GAM. I created an image for every day and stitched them together into a movie with <a href="http://ffmpeg.org/">ffmpeg</a>. The R code is <a href="https://docs.google.com/open?id=0B21DLJKq18K9ejNUX0RXQnlJam8">here</a>.</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/05/25/sending-text-in-r/">Sending a Text in R</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-05-25T00:00:00-04:00" pubdate data-updated="true">May 25<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/05/25/sending-text-in-r/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
Don&#8217;t you hate it when you are running a long piece of code and you keep checking the results every 15 minutes, hoping it will finish? There is a better way.<br /><br />I got the idea from <a href="http://jbdeaton.com/2012/send-yourself-an-sms-via-python/">here</a>. He uses a Python script and the text interface is not free. I thought someone must have already thought of this for R. There is an easy solution. You can send an email fairly easily in R. You can also send <a href="http://allthingsmarked.com/2006/09/04/howto-send-free-text-messages-through-email/">text messages as emails</a>. <a href="http://www.decisionstats.com/send-email-by-r/">This page </a>gives a few examples of how to send an email in R. Sending a text via email will depend on what carrier you have and I am sure standard text message fees apply.You can only send 20 emails a day with this package, but that probably won&#8217;t be a big deal.<br /><br />It is basically a one-liner. Here is the code I used:<br /><pre class="brush: r">install.packages("mail",repos="http://cran.case.edu/")<br />library(mail)<br />sendmail("5555555555@txt.att.net", <br />         subject="Notification from R",message="MCMC Finished", password="rmail")<br /></pre><br />That&#8217;s it!<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-DaUTmyXUfks/T7_mu1CRtII/AAAAAAAAG8Y/Ike9L7_C_yI/s1600/photo.PNG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="320" src="http://1.bp.blogspot.com/-DaUTmyXUfks/T7_mu1CRtII/AAAAAAAAG8Y/Ike9L7_C_yI/s320/photo.PNG" width="212" /></a></div></div>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/5/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/3/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/12/24/time-stack-time-slice-r/">Time Stacking and Time Slicing in R</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/04/22/yet-another-baseball-defensive-statistic/">Yet Another Baseball Defense Statistic</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/03/02/hastie-and-tibshirani-interview-jerome-friedman/">Hastie and Tibshirani Interview Jerome Friedman</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/12/27/top-songs-by-artist-on-cd1025-in-2013/">Top Songs by Artist on CD102.5 in 2013</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/08/27/when-did-cd1025-book-summerfest-artists/">When Did CD102.5 Book the Summerfest Artists?</a>
      </li>
    
  </ul>
</section>




<section>
  <h1>Blogroll</h1>
  <p>
  <ul>
	<li><a href="http://www.r-bloggers.com">R-Bloggers</a> </li>
	<li><a href="http://fastml.com/">FastML</a> </li>
	<li><a href="http://blog.kaggle.com/">No Free Hunch (Kaggle)</a> </li>
	<li><a href="http://andrewgelman.com/">Statistical Modeling, Causal Inference, and Social Science</a> </li>
	<li><a href="http://simplystatistics.org/">Simply Statistics</a> </li>
	<li><a href="https://kpq.github.io/chartsnthings/">chartsnthings (NYT Graphics Dept.)</a> </li>
  </ul> 
  </p>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Andrew Landgraf -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'andland';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
