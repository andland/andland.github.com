<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data on Statistically Significant</title>
    <link>/categories/data/</link>
    <description>Recent content in data on Statistically Significant</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Apr 2015 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/categories/data/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Monitoring Price Fluctuations of Book Trade-In Values on Amazon</title>
      <link>/2015/04/08/monitoring-price-fluctuations-of-book-trade-in-values-on-amazon/</link>
      <pubDate>Wed, 08 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/04/08/monitoring-price-fluctuations-of-book-trade-in-values-on-amazon/</guid>
      <description>&lt;p&gt;I am planning to finish school soon and I would like to shed some weight before moving on. I have collected a fair number of books that I will likely never use again and it would be nice to get some money for them. Sites like Amazon and eBay let you sell your book to other customers, but Amazon will also buy some of your books directly (Trade-Ins), saving you the hassle of waiting for a buyer.&lt;/p&gt;

&lt;p&gt;Before selling, I remembered listening to a &lt;a href=&#34;http://www.npr.org/blogs/money/2014/11/10/363103753/textbook-arbitrage-making-money-off-used-books&#34; target=&#34;_blank&#34;&gt;Planet Money episode&lt;/a&gt; about a couple of guys that tried to make money off of buying and selling used textbooks on Amazon. Their strategy was to buy books at the end of a semester when students are itching to get rid of them, and sell them to other students at the beginning of the next semester. To back up their business, they have been scraping Amazon&amp;rsquo;s website for years, keeping track of prices in order to find the optimal times to buy and sell.&lt;/p&gt;

&lt;p&gt;I collected a few books I was willing to part with and set up a scraper in R. I am primarily interested in selling my books to Amazon, so I tracked Amazon&amp;rsquo;s Trade-In prices for these books. This was done fairly easily with Hadley Wickham&amp;rsquo;s package &lt;a href=&#34;https://github.com/hadley/rvest&#34; target=&#34;_blank&#34;&gt;rvest&lt;/a&gt; and the Chrome extension &lt;a href=&#34;http://selectorgadget.com/&#34; target=&#34;_blank&#34;&gt;Selector Gadget&lt;/a&gt;. Selector Gadget tells me that the node I&amp;rsquo;m interested in is &lt;code&gt;#tradeInButton_tradeInValue&lt;/code&gt;. The code to do the scraping is below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rvest)
 
urls_df = read.csv(&amp;quot;AmazonBookURLs.csv&amp;quot;, stringsAsFactors = FALSE, comment.char = &amp;quot;&amp;quot;)
load(file = &amp;quot;AmazonBookPrices.RData&amp;quot;)
price_df_temp = data.frame(Title = urls_df$Title,
                      Date = Sys.time(),
                      Price = NA_real_, stringsAsFactors = FALSE)
for (i in 1:nrow(urls_df)) {
  tradein_html = urls_df$URL[i] %&amp;gt;% html() %&amp;gt;%
    html_node(&amp;quot;#tradeInButton_tradeInValue&amp;quot;)
  if (is.null(tradein_html)) {
    next
  }
  price = tradein_html %&amp;gt;%
    html_text() %&amp;gt;%
    gsub(&amp;quot;(^[[:space:]]+\\$|[[:space:]]+$)&amp;quot;, &amp;quot;&amp;quot;, .) %&amp;gt;%
    as.numeric()
  price_df_temp$Price[i] = price
}
price_df = rbind(price_df, price_df_temp)
 
save(price_df, file = &amp;quot;AmazonBookPrices.RData&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After manually collecting this data for less than a week, I am able to plot the trends for the eight books I am interested in selling. The plot and code are below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggplot2); theme_set(theme_bw())
library(scales)
 
price_df$TitleTrunc = paste0(substring(price_df$TitleTrunc, 1, 30), ifelse(nchar(price_df$Title) &amp;gt; 30, &amp;quot;...&amp;quot;, &amp;quot;&amp;quot;))
ggplot(price_df, aes(Date, Price)) +
  geom_step() + geom_point() + facet_wrap(~ TitleTrunc, scales = &amp;quot;free_y&amp;quot;) +
  scale_y_continuous(labels = dollar) + theme(axis.text.x = element_text(angle = 90, vjust = 0))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/img/AmazonScrape/prices.png&#34; alt=&#34;Amazon Trade-In Prices&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I am surprised how much the prices fluctuate. I expected them to be constant for the most part, with large changes once a week or less often. Apparently Amazon is doing quite a bit of tweaking to determine optimal price points. I would also guess that $2 is their minimum trade-in price. It looks like I missed out on my best chance to sell &lt;em&gt;A First Course in Stochastic Processes&lt;/em&gt;, but that the price of &lt;em&gt;A Primer on Linear Models&lt;/em&gt; will keep rising forever.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Yet Another Baseball Defense Statistic</title>
      <link>/2014/04/22/yet-another-baseball-defense-statistic/</link>
      <pubDate>Tue, 22 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/04/22/yet-another-baseball-defense-statistic/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://www.fangraphs.com&#34; target=&#34;_blank&#34;&gt;Fangraphs&lt;/a&gt; recently published an interesting dataset that measures defensive efficiency of fielders. For each player, the &lt;a href=&#34;http://www.fangraphs.com/blogs/inside-edge-fielding-data/&#34; target=&#34;_blank&#34;&gt;Inside Edge&lt;/a&gt; dataset breaks their opportunities to make plays into five categories, ranging from almost impossible to routine. It also records the proportion of times that the player successfully made the play. With this data, we can see how successful each player is for each type of play. I wanted to think of a way to combine these five proportions into one fielding metric. From here on, I will assume that there is no error in categorizing a play as easy or hard and that there is no bias in the categorizations.&lt;/p&gt;

&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;

&lt;p&gt;The model I will build is motivated by &lt;a href=&#34;http://en.wikipedia.org/wiki/Ordered_logit&#34; target=&#34;_blank&#34;&gt;ordinal regression&lt;/a&gt;. If we only were concerned with the success rate in one of the categories, we could use standard logistic regression, and the probability that player $i$ successfully made a play would be assumed to be $\sigma(\theta_i)$, where $\sigma()$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_function&#34; target=&#34;_blank&#34;&gt;logistic function&lt;/a&gt;. Using our prior knowledge that plays categorized as easy should have a higher success rate than plays categorized as difficult, I would like to generalize this.&lt;/p&gt;

&lt;p&gt;Say there are only two categories: easy and hard. We could model the probability that player $i$ successfully made an hard play as $\sigma(\theta_i)$ and the probability that he made an easy play as $\sigma(\theta_i+\gamma)$. Here, we would assume that $\gamma$ is the same for all players. This assumption implies that if player $i$ is better than player $j$ at easy plays, he will also be better at hard plays. This is a reasonable assumption, but maybe not true in all cases.&lt;/p&gt;

&lt;p&gt;Since we have five different categories of difficulty, we can generalize this by having $\gamma_k, k=1,\ldots,4$. Again, these $\gamma_k$&amp;rsquo;s would be the same for everyone. A picture of what this looks like for shortstops is below. In this model, every player will effectively be shifting the curve either left or right. A positive $\theta_i$ means the player is better than average and cause the curve to shift left and vice versa for negative $\theta_i$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/InsideEdge/LogitOrdered.png&#34; alt=&#34;Logit Ordered&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I modeled this as a multi-level mixed effects model, with the players being random effects and the $\gamma_k$&amp;rsquo;s being fixed. Technically, I should optimize subject to the condition that the $\gamma_k$&amp;rsquo;s are increasing, but the unconstrained optimization always yields increasing $\gamma_k$&amp;rsquo;s because there is a big difference between success rate in the categories. I used combined data from 2012 and 2013 seasons and included all players with at least one success and one failure. I modeled each position separately. Modeling player effects as random, there is a fair amount of regression to the mean built in. In this sense, I am more trying to estimate the true ability of the player, rather than measuring what he did during the two years. This is an important distinction, which may differ from other defensive statistics.&lt;/p&gt;

&lt;h3 id=&#34;model-fit&#34;&gt;Model fit&lt;/h3&gt;

&lt;p&gt;Below is a summary of the results of the model for shortstops. I am only plotting the players with the at least 800 innings, for readability. A bonus of modeling the data like this is that we get standard error estimates as a result. I plotted the estimated effect of each player along with +/- 2 standard errors. We can be fairly certain that the effects for the top few shortstops is greater than 0 since their confidence intervals do not include 0. The same is true for the bottom few. Images for the other positions can be found &lt;a href=&#34;https://github.com/andland/andland.github.com/tree/master/images/InsideEdge&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/InsideEdge/SSlist.png&#34; alt=&#34;Results for Shortstops&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The results seem to make sense for the most part. Simmons and Tulowitzki have reputations as being strong defenders and Derek Jeter has a reputation as a poor defender.&lt;/p&gt;

&lt;p&gt;Further, I can validate this data by comparing it to other defensive metrics. One that is readily available on Fangraphs is UZR per 150 games. For each position, I took the correlation of my estimated effect with UZR per 150 games, weighted by the number of innings played. Pitchers and catchers do not have UZR&amp;rsquo;s so I cannot compare them. The correlations, which are in the plot below, range from about 0.2 to 0.65.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/InsideEdge/CorrelationWithUZR.png&#34; alt=&#34;Correlation with UZR&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;interpreting-parameters&#34;&gt;Interpreting parameters&lt;/h3&gt;

&lt;p&gt;In order to make this fielding metric more useful, I would like to convert the parameters to something more interpretable. One option which makes a lot of sense is &amp;ldquo;plays made above/below average&amp;rdquo;. Given an estimated $\theta_i$ for a player, we can calculate the probability that he would make a play in each of the five categories. We can then compare those probabilities to the probability an average player would make a play in each of the categories, which would be fit with $\theta=0$. Finally, we can weight these differences in probabilities by the relative rate that plays of various difficulties occur.&lt;/p&gt;

&lt;p&gt;For example, assuming there are only two categories again, suppose a player has a 0.10 and 0.05 higher probability than average of making hard and easy plays, respectively. Further assume that 75% of all plays are hard and 25% are easy. On a random play, the improvement in probability over an average player of making a play is $.10(.75)+.05(.25)=0.0875$. If a player has an opportunity for 300 plays in an average season, this player would be $300 \times 0.0875=26.25$ plays better than average over a typical season.&lt;/p&gt;

&lt;p&gt;I will assume that the number of opportunities to make plays is directly related to the number of innings played. To convert innings to opportunities, I took the median number of opportunities per inning for each position. For example, shortstops had the highest opportunities per inning at 0.40 and catchers had the lowest at 0.08. The plot below shows the distribution of opportunities per inning for each position.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/InsideEdge/PlaysPerInning.png&#34; alt=&#34;Plays Per Inning by Position&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can extend this to the impact on saving runs from being scored as well by assuming each successful play saves $x$ runs. I will not do this for this analysis.&lt;/p&gt;

&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/InsideEdge/shiny.png&#34; alt=&#34;Shiny app&#34; /&gt;&lt;/p&gt;

&lt;p&gt;[&lt;strong&gt;Update&lt;/strong&gt;: The Shiny app is no longer live, but you can view the resulting estimates in the &lt;a href=&#34;https://github.com/andland/InsideEdgeFielding/blob/master/Shiny/InsideEdgeModelResults.csv&#34; target=&#34;_blank&#34;&gt;Github repository&lt;/a&gt;.]&lt;/p&gt;

&lt;p&gt;Finally, I put together a &lt;a href=&#34;http://www.rstudio.com/shiny/&#34; target=&#34;_blank&#34;&gt;Shiny&lt;/a&gt; app to display &lt;a href=&#34;https://github.com/andland/InsideEdgeFielding/blob/master/Shiny/InsideEdgeModelResults.csv&#34; target=&#34;_blank&#34;&gt;the results&lt;/a&gt;. You can search by team, position, and innings played. A team of &amp;lsquo;- - -&amp;rsquo; means the player played for multiple teams over this period. You can also choose to display the results as a rate statistic (extra plays made per season) or a count statistic (extra plays made over the two seasons). To get a seasonal number, I assume position players played 150 games with 8.5 innings in each game. For pitchers, I assumed that they pitched 30 games, 6 innings each.&lt;/p&gt;

&lt;h3 id=&#34;conclusions-and-future-work&#34;&gt;Conclusions and future work&lt;/h3&gt;

&lt;p&gt;I don&amp;rsquo;t know if I will do anything more with this data, but if I could do it again, I may have modeled each year separately instead of combining the two years together. With that, it would have been interesting to model the effect of age by observing how a player&amp;rsquo;s ability to make a play changes from one year to the next. I also think it would be interesting to see how changing positions affects a players efficiency. For example, we could have a $9 \times 9$ matrix of fixed effects that represent the improvement or degradation in ability as a player switches from their main position to another one. Further assumptions would be needed to make sure the $\theta$&amp;rsquo;s are on the same scale for every position.&lt;/p&gt;

&lt;p&gt;At the very least, this model and its results can be considered another data point in the analysis of a player&amp;rsquo;s fielding ability. One thing we need to be concerned about is the classification of each play into the difficultness categories. The human eye can be fooled into thinking a routine play is hard just because a fielder dove to make the play, when a superior fielder could have made it look easier.&lt;/p&gt;

&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;

&lt;p&gt;I have put the R code together to do this analysis in &lt;a href=&#34;https://gist.github.com/andland/c6adb69ea365c2580859&#34; target=&#34;_blank&#34;&gt;a gist&lt;/a&gt;. If there is interest, I will put together a repo with all the data as well.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: I have a &lt;a href=&#34;https://github.com/andland/InsideEdgeFielding&#34; target=&#34;_blank&#34;&gt;Github repository&lt;/a&gt; with the data, R code for the analysis, the results, and code for the Shiny app. Let me know what you think.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hastie and Tibshirani Interview Jerome Friedman</title>
      <link>/2014/03/02/hastie-and-tibshirani-interview-jerome-friedman/</link>
      <pubDate>Sun, 02 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/03/02/hastie-and-tibshirani-interview-jerome-friedman/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.stanford.edu/~hastie/&#34; target=&#34;_blank&#34;&gt;Trevor Hastie&lt;/a&gt; and &lt;a href=&#34;http://statweb.stanford.edu/~tibs/&#34; target=&#34;_blank&#34;&gt;Rob Tibshirani&lt;/a&gt; are currently teaching a &lt;a href=&#34;http://en.wikipedia.org/wiki/Massive_open_online_course&#34; target=&#34;_blank&#34;&gt;MOOC&lt;/a&gt; covering an &lt;a href=&#34;https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/about&#34; target=&#34;_blank&#34;&gt;introduction to statistical learning&lt;/a&gt;. I am very familiar with most of the material in the course, having read &lt;em&gt;Elements of Statistical Learning&lt;/em&gt; many times over.&lt;/p&gt;

&lt;p&gt;One great thing about the class, however, is that they are truely experts and have collaborated with many of the influencial researchers in their field. Because of this, when covering certain topics, they have included interviews with statisticians who made important developments to the field. When introducing the class to R, they interviewed &lt;a href=&#34;https://www.youtube.com/watch?v=jk9S3RTAl38&#34; target=&#34;_blank&#34;&gt;John Chambers&lt;/a&gt;, who was able to give a personal account of the history of S and R because he was one of the developers. Further, when covering resampling methods, they spoke with &lt;a href=&#34;https://www.youtube.com/watch?v=6l9V1sINzhE&#34; target=&#34;_blank&#34;&gt;Brad Efron&lt;/a&gt;, who talked about the history of the &lt;a href=&#34;http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29&#34; target=&#34;_blank&#34;&gt;bootstrap&lt;/a&gt; and how he struggled to get it published.&lt;/p&gt;

&lt;p&gt;Today, they released &lt;a href=&#34;https://www.youtube.com/watch?v=79tR7BvYE6w&#34; target=&#34;_blank&#34;&gt;a video interview&lt;/a&gt; with Jerome Friedman. Friedman revealed many interesting facts about the history of &lt;a href=&#34;http://en.wikipedia.org/wiki/Decision_tree_learning&#34; target=&#34;_blank&#34;&gt;tree-based methods&lt;/a&gt;, including the fact that there weren&amp;rsquo;t really any journal articles written about CART when they wrote &lt;a href=&#34;http://www.amazon.com/Classification-Regression-Trees-Leo-Breiman/dp/0412048418&#34; target=&#34;_blank&#34;&gt;their book&lt;/a&gt;. There was one quote that I particularly enjoyed.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;And of course, I&amp;rsquo;m very gratified that something that I was
intellectually interested in for all this time has now
become very popular and very important.
I mean, data has risen to the top.
My only regret is two of my mentors who also pushed it,
probably harder and more effectively than I did &amp;ndash;
namely, John Tukey and Leo Breiman &amp;ndash; are not around to
actually see how data has triumphed over,
say, theorem proving.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/79tR7BvYE6w&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Top Songs by Artist on CD102.5 in 2013</title>
      <link>/2013/12/27/top-songs-by-artist-on-cd102.5-in-2013/</link>
      <pubDate>Fri, 27 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/12/27/top-songs-by-artist-on-cd102.5-in-2013/</guid>
      <description>
&lt;div class=&#39;post&#39;&gt;
In a &lt;a href=&#34;http://alandgraf.blogspot.com/2013/08/downloading-and-analyzing-cd1025s.html&#34;&gt;previous post&lt;/a&gt;, I showed you how to scrape playlist data from Columbus, OH alternative rock station &lt;a href=&#34;http://cd1025.com/&#34;&gt;CD102.5&lt;/a&gt;. Since it&#39;s the end of the year and best-of lists are all the fad, I thought I would share the most popular songs and artists of the year, according to this data. In addition to this, I am going to make an interactive graph using &lt;a href=&#34;http://www.rstudio.com/shiny/&#34;&gt;Shiny&lt;/a&gt;, where the user can select an artist and it will graph the most popular songs from that artist.&lt;br /&gt;&lt;br /&gt;First off, I am assuming that you have scraped the appropriate data using the code from the previous post.&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;library(lubridate)&lt;br /&gt;library(sqldf)&lt;br /&gt;&lt;br /&gt;playlist=read.csv(&#34;CD101Playlist.csv&#34;,stringsAsFactors=FALSE)&lt;br /&gt;dates=mdy(substring(playlist[,3],nchar(playlist[,3])-9,nchar(playlist[,3])))&lt;br /&gt;times=hm(substring(playlist[,3],1,nchar(playlist[,3])-10))&lt;br /&gt;playlist$Month=ymd(paste(year(dates),month(dates),&#34;1&#34;,sep=&#34;-&#34;))&lt;br /&gt;playlist$Day=dates&lt;br /&gt;playlist$Time=times&lt;br /&gt;playlist=playlist[order(playlist$Day,playlist$Time),]&lt;/code&gt;&lt;/pre&gt;&lt;br /&gt;Next, I will select just the data from 2013 and find the songs that were played most often. &lt;br /&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;playlist=subset(playlist,Day&amp;gt;=mdy(&#34;1/1/13&#34;))&lt;br /&gt;playlist$ArtistSong=paste(playlist$Artist,playlist$Song,sep=&#34;-&#34;)&lt;br /&gt;top.songs=sqldf(&#34;Select ArtistSong, Count(ArtistSong) as Num&lt;br /&gt;      From playlist&lt;br /&gt;      Group By ArtistSong&lt;br /&gt;      Order by Num DESC&lt;br /&gt;      Limit 10&#34;)&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br /&gt;The top 10 songs are the following:&lt;br /&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;                              Artist-Song Number Plays&lt;br /&gt;1  FITZ AND THE TANTRUMS-OUT OF MY LEAGUE 809&lt;br /&gt;2                      ALT J-BREEZEBLOCKS 764&lt;br /&gt;3              COLD WAR KIDS-MIRACLE MILE 759&lt;br /&gt;4                      ATLAS GENIUS-IF SO 750&lt;br /&gt;5                         FOALS-MY NUMBER 687&lt;br /&gt;6                         MS MR-HURRICANE 679&lt;br /&gt;7       THE NEIGHBOURHOOD-SWEATER WEATHER 657&lt;br /&gt;8           CAPITAL CITIES-SAFE AND SOUND 646&lt;br /&gt;9             VAMPIRE WEEKEND-DIANE YOUNG 639&lt;br /&gt;10             THE FEATURES-THIS DISORDER 632&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br /&gt;I will make a plot similar to the plots made in &lt;a href=&#34;http://alandgraf.blogspot.com/2013/08/when-did-cd1025-book-summerfest-artists.html&#34;&gt;the last post&lt;/a&gt; to show when the top 5 songs were played throughout the year.&lt;br /&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;    &lt;br /&gt;plays.per.day=sqldf(&#34;Select Day, Count(Artist) as Num&lt;br /&gt;      From playlist&lt;br /&gt;      Group By Day&lt;br /&gt;      Order by Day&#34;)&lt;br /&gt;&lt;br /&gt;playlist.top.songs=subset(playlist,ArtistSong %in% top.songs$ArtistSong[1:5])&lt;br /&gt;&lt;br /&gt;song.per.day=sqldf(paste0(&#34;Select Day, ArtistSong, Count(ArtistSong) as Num&lt;br /&gt;                          From [playlist.top.songs]&lt;br /&gt;                          Group By Day, ArtistSong&lt;br /&gt;                          Order by Day, ArtistSong&#34;))&lt;br /&gt;dspd=dcast(song.per.day,Day~ArtistSong,sum,value.var=&#34;Num&#34;)&lt;br /&gt;&lt;br /&gt;song.per.day=merge(plays.per.day[,1,drop=FALSE],dspd,all.x=TRUE)&lt;br /&gt;song.per.day[is.na(song.per.day)]=0&lt;br /&gt;&lt;br /&gt;song.per.day=melt(song.per.day,1,variable.name=&#34;ArtistSong&#34;,value.name=&#34;Num&#34;)&lt;br /&gt;song.per.day$Alpha=ifelse(song.per.day$Num&amp;gt;0,1,0)&lt;br /&gt;&lt;br /&gt;library(ggplot2)&lt;br /&gt;ggplot(song.per.day,aes(Day,Num,colour=ArtistSong))+geom_point(aes(alpha=Alpha))+&lt;br /&gt;  geom_smooth(method=&#34;gam&#34;,family=poisson,formula=y~s(x),se=F,size=1)+&lt;br /&gt;  labs(x=&#34;Date&#34;,y=&#34;Plays Per Day&#34;,title=&#34;Top Songs&#34;,colour=NULL)+&lt;br /&gt;  scale_alpha_continuous(guide=FALSE,range=c(0,.5))+theme_bw()&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-JWP3RQkgQeo/Ur4-RG1px7I/AAAAAAAAH-U/JH6gP0Jk6Tc/s1600/TopSongs.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-JWP3RQkgQeo/Ur4-RG1px7I/AAAAAAAAH-U/JH6gP0Jk6Tc/s1600/TopSongs.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;Alt-J was more popular in the beginning of the year and the Foals have been more popular recently.&lt;br /&gt;&lt;br /&gt;I can similarly summarize by artist as well.&lt;br /&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;top.artists=sqldf(&#34;Select Artist, Count(Artist) as Num&lt;br /&gt;                From playlist&lt;br /&gt;                Group By Artist&lt;br /&gt;                Order by Num DESC&lt;br /&gt;                Limit 10&#34;)&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;                    Artist  Num&lt;br /&gt;1                     MUSE 1683&lt;br /&gt;2          VAMPIRE WEEKEND 1504&lt;br /&gt;3        SILVERSUN PICKUPS 1442&lt;br /&gt;4                    FOALS 1439&lt;br /&gt;5                  PHOENIX 1434&lt;br /&gt;6            COLD WAR KIDS 1425&lt;br /&gt;7                JAKE BUGG 1316&lt;br /&gt;8  QUEENS OF THE STONE AGE 1296&lt;br /&gt;9                    ALT J 1233&lt;br /&gt;10     OF MONSTERS AND MEN 1150&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;playlist.top.artists=subset(playlist,Artist %in% top.artists$Artist[1:5])&lt;br /&gt;&lt;br /&gt;artists.per.day=sqldf(paste0(&#34;Select Day, Artist, Count(Artist) as Num&lt;br /&gt;                          From [playlist.top.artists]&lt;br /&gt;                          Group By Day, Artist&lt;br /&gt;                          Order by Day, Artist&#34;))&lt;br /&gt;dspd=dcast(artists.per.day,Day~Artist,sum,value.var=&#34;Num&#34;)&lt;br /&gt;&lt;br /&gt;artists.per.day=merge(plays.per.day[,1,drop=FALSE],dspd,all.x=TRUE)&lt;br /&gt;artists.per.day[is.na(artists.per.day)]=0&lt;br /&gt;&lt;br /&gt;artists.per.day=melt(artists.per.day,1,variable.name=&#34;Artist&#34;,value.name=&#34;Num&#34;)&lt;br /&gt;artists.per.day$Alpha=ifelse(artists.per.day$Num&amp;gt;0,1,0)&lt;br /&gt;&lt;br /&gt;ggplot(artists.per.day,aes(Day,Num,colour=Artist))+geom_point(aes(alpha=Alpha))+&lt;br /&gt;  geom_smooth(method=&#34;gam&#34;,family=poisson,formula=y~s(x),se=F,size=1)+&lt;br /&gt;  labs(x=&#34;Date&#34;,y=&#34;Plays Per Day&#34;,title=&#34;Top Artists&#34;,colour=NULL)+&lt;br /&gt;  scale_alpha_continuous(guide=FALSE,range=c(0,.5))+theme_bw()&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-57hZwciuIug/Ur5AO9l7NWI/AAAAAAAAH-g/Xi2_4e-FwZw/s1600/TopArtist.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-57hZwciuIug/Ur5AO9l7NWI/AAAAAAAAH-g/Xi2_4e-FwZw/s1600/TopArtist.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;The pattern for the artists are not as clear as it is for the songs.&lt;br /&gt;&lt;br /&gt;Finally, I wrote &lt;a href=&#34;https://andland.shinyapps.io/CD1025Playlist2/&#34;&gt;a Shiny interactive app&lt;/a&gt;. They are surprisingly easy to create and if you are thinking about experimenting with it, I suggest you try it. I will leave the code for the app in &lt;a href=&#34;https://gist.github.com/andland/8155783&#34;&gt;a gist&lt;/a&gt;. In &lt;a href=&#34;https://andland.shinyapps.io/CD1025Playlist2/&#34;&gt;the app&lt;/a&gt;, you can enter any artist you want, and it will show you the most popular songs on CD102.5 for that artist. You can also select the number of songs that it plots with the slider.&lt;br /&gt;&lt;br /&gt;For example, even though Muse did not have one of the most popular songs of the year, they were still the band that was played the most. By typing in &#34;MUSE&#34; in the Artist text input, you will get the following output.&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-EuKnItqFzCo/Ur5CQELkw0I/AAAAAAAAH-s/Pv4dJwueLQM/s1600/Muse.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-EuKnItqFzCo/Ur5CQELkw0I/AAAAAAAAH-s/Pv4dJwueLQM/s1600/Muse.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;They had two songs that were very popular this year and a few others that were decently popular as well.&lt;br /&gt;&lt;br /&gt;&lt;a href=&#34;https://andland.shinyapps.io/CD1025Playlist2/&#34;&gt;Play around&lt;/a&gt; with it and let me know what you think.&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Downloading and Analyzing CD1025&#39;s Playlist</title>
      <link>/2013/08/20/downloading-and-analyzing-cd1025s-playlist/</link>
      <pubDate>Tue, 20 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/08/20/downloading-and-analyzing-cd1025s-playlist/</guid>
      <description>
&lt;div class=&#39;post&#39;&gt;
&lt;a href=&#34;http://www.cd101.com/&#34;&gt;CD1025&lt;/a&gt; is an “alternative” radio station here in Columbus. They are one of the few remaining radio stations that are independently owned and they take great pride in it. For data nerds like me, they also put a real time list of recently played songs on &lt;a href=&#34;http://cd1025.com/about/playlists/now-playing&#34;&gt;their website&lt;/a&gt;. The page has the most recent 50 songs played, but you can also click on “Older Tracks” to go back in time. When you do this, the URL ends “now-playing/?start=50”. If you got back again, it says “now-playing/?start=100”.&lt;br /&gt;&lt;br /&gt;Using this structure, I decided to see if I could download all of their historical data and see how far it goes back. In the code below, I use the XML package to go to the website and download the 50 songs and then increment the number by 50 to find the previous 50 songs. I am telling the code to keep doing this until I get to January 1, 2012.&lt;br /&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;library(ggplot2)&lt;br /&gt;theme_set(theme_bw())&lt;br /&gt;library(XML)&lt;br /&gt;library(lubridate)&lt;br /&gt;library(sqldf)&lt;br /&gt;startNum = 0&lt;br /&gt;while (TRUE) {&lt;br /&gt;    theurl &amp;lt;- paste0(&#34;http://cd1025.com/about/playlists/now-playing/?start=&#34;, &lt;br /&gt;        startNum)&lt;br /&gt;    table &amp;lt;- readHTMLTable(theurl, stringsAsFactors = FALSE)[[1]]&lt;br /&gt;    if (startNum == 0) {&lt;br /&gt;        playlist = table[, -1]&lt;br /&gt;    } else {&lt;br /&gt;        playlist = rbind(playlist, table[, -1])&lt;br /&gt;    }&lt;br /&gt;    dt = mdy(substring(table[1, 4], nchar(table[1, 4]) - 9, nchar(table[1, 4])))&lt;br /&gt;    print(dt)&lt;br /&gt;    if (dt &amp;lt; mdy(&#34;1/1/12&#34;)) {&lt;br /&gt;        break&lt;br /&gt;    }&lt;br /&gt;    startNum = startNum + 50&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;playlist = unique(playlist)  # Remove Dupes&lt;br /&gt;&lt;br /&gt;write.csv(playlist, &#34;CD101Playlist.csv&#34;, row.names = FALSE)&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;This takes a while and is fairly large. My file has over 150,000 songs. If you want just a little data, change the date to last week or so. The first thing I will do is parse the dates and times of the songs, order them, and look at the first few songs. You can see that data only goes back to March of 2012.&lt;br /&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;dates = mdy(substring(playlist[, 3], nchar(playlist[, 3]) - 9, nchar(playlist[, &lt;br /&gt;    3])))&lt;br /&gt;times = hm(substring(playlist[, 3], 1, nchar(playlist[, 3]) - 10))&lt;br /&gt;playlist$Month = ymd(paste(year(dates), month(dates), &#34;1&#34;, sep = &#34;-&#34;))&lt;br /&gt;playlist$Day = dates&lt;br /&gt;playlist$Time = times&lt;br /&gt;playlist = playlist[order(playlist$Day, playlist$Time), ]&lt;br /&gt;head(playlist)&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;##                     Artist                Song       Last.Played&lt;br /&gt;## 151638 DEATH CAB FOR CUTIE   YOU ARE A TOURIST 12:34am03/01/2012&lt;br /&gt;## 151637       SLEEPER AGENT          GET BURNED 12:38am03/01/2012&lt;br /&gt;## 151636          WASHED OUT           AMOR FATI 12:41am03/01/2012&lt;br /&gt;## 151635            COLDPLAY       CHARLIE BROWN 12:45am03/01/2012&lt;br /&gt;## 151634           GROUPLOVE         TONGUE TIED 12:49am03/01/2012&lt;br /&gt;## 151633               SUGAR YOUR FAVORITE THING 12:52am03/01/2012&lt;br /&gt;##             Month        Day   Time&lt;br /&gt;## 151638 2012-03-01 2012-03-01 34M 0S&lt;br /&gt;## 151637 2012-03-01 2012-03-01 38M 0S&lt;br /&gt;## 151636 2012-03-01 2012-03-01 41M 0S&lt;br /&gt;## 151635 2012-03-01 2012-03-01 45M 0S&lt;br /&gt;## 151634 2012-03-01 2012-03-01 49M 0S&lt;br /&gt;## 151633 2012-03-01 2012-03-01 52M 0S&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;Using the sqldf package, I can easily see what the most played artists and songs are from the data I scraped.&lt;br /&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;sqldf(&#34;Select Artist, Count(Artist) as PlayCount&lt;br /&gt;       From playlist&lt;br /&gt;       Group By Artist&lt;br /&gt;       Order by PlayCount DESC&lt;br /&gt;       Limit 10&#34;)&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;##                   Artist PlayCount&lt;br /&gt;## 1      SILVERSUN PICKUPS      2340&lt;br /&gt;## 2         THE BLACK KEYS      2203&lt;br /&gt;## 3                   MUSE      1988&lt;br /&gt;## 4              THE SHINS      1885&lt;br /&gt;## 5    OF MONSTERS AND MEN      1753&lt;br /&gt;## 6            PASSION PIT      1552&lt;br /&gt;## 7              GROUPLOVE      1544&lt;br /&gt;## 8  RED HOT CHILI PEPPERS      1514&lt;br /&gt;## 9                 METRIC      1495&lt;br /&gt;## 10          ATLAS GENIUS      1494&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;&lt;br /&gt;sqldf(&#34;Select Artist, Song, Count(Song) as PlayCount&lt;br /&gt;      From playlist&lt;br /&gt;      Group By Artist, Song&lt;br /&gt;      Order by PlayCount DESC&lt;br /&gt;      Limit 10&#34;)&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;##                 Artist                    Song PlayCount&lt;br /&gt;## 1          PASSION PIT             TAKE A WALK       828&lt;br /&gt;## 2    SILVERSUN PICKUPS                PIT, THE       825&lt;br /&gt;## 3         ATLAS GENIUS                 TROJANS       819&lt;br /&gt;## 4        WALK THE MOON                ANNA SUN       742&lt;br /&gt;## 5       THE BLACK KEYS LITTLE BLACK SUBMARINES       736&lt;br /&gt;## 6          DIVINE FITS  WOULD THAT NOT BE NICE       731&lt;br /&gt;## 7        THE LUMINEERS                  HO HEY       722&lt;br /&gt;## 8       CAPITAL CITIES          SAFE AND SOUND       712&lt;br /&gt;## 9  OF MONSTERS AND MEN          MOUNTAIN SOUND       711&lt;br /&gt;## 10               ALT J            BREEZEBLOCKS       691&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;I am a little surprised that Silversun Pickups are the number one band, but everyone on the list makes sense. Looking at how the plays of the top artists have varied from month to month, you can see a few patterns. Muse has been more popular recently and The Shins and Grouplove have lost some steam.&lt;br /&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;artist.month=sqldf(&#34;Select Month, Artist, Count(Song) as Num&lt;br /&gt;      From playlist&lt;br /&gt;      Group By Month, Artist&lt;br /&gt;      Order by Month, Artist&#34;)&lt;br /&gt;artist=sqldf(&#34;Select Artist, Count(Artist) as Num&lt;br /&gt;      From playlist&lt;br /&gt;      Group By Artist&lt;br /&gt;      Order by Num DESC&#34;)&lt;br /&gt;p=ggplot(subset(artist.month,Artist %in% head(artist$Artist,8)),aes(Month,Num))&lt;br /&gt;p+geom_bar(stat=&#34;identity&#34;,aes(fill=Artist),position=&#39;fill&#39;,colour=&#34;grey&#34;)+&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt; labs(y=&#34;Percentage of Plays&#34;)&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-ZbWCMOMUX8E/UhPvnh1UoqI/AAAAAAAAH8I/EIAlTwFKjxg/s1600/PlayProportionByMonth.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-ZbWCMOMUX8E/UhPvnh1UoqI/AAAAAAAAH8I/EIAlTwFKjxg/s1600/PlayProportionByMonth.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;For the play count of the top artists, I see some odd numbers in June and July of 2012. The number of plays went way down.&lt;br /&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;p + geom_area(aes(fill = Artist), position = &#34;stack&#34;, colour = 1) + labs(y = &#34;Number of Plays&#34;)&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/--0tLMWDJuAI/UhPvnsQAl2I/AAAAAAAAH8M/tl2RtleoH2k/s1600/PlayCountByMonth.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/--0tLMWDJuAI/UhPvnsQAl2I/AAAAAAAAH8M/tl2RtleoH2k/s1600/PlayCountByMonth.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Looking into this further, I plotted the date and time that the song was played by the cumulative number of songs played since the beginning of the list. The plot should be a line with a constant slope, meaning that the plays per day are relatively constant. You can see in June and July of 2012 there are flat spots where there is no playlist history. &lt;br /&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;qplot(playlist$Day + playlist$Time, 1:length(dates), geom = &#34;path&#34;)&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-jzAKuFMa_ak/UhPvnqrS-_I/AAAAAAAAH8E/DgJDVUUNyD8/s1600/PlaysByTime.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-jzAKuFMa_ak/UhPvnqrS-_I/AAAAAAAAH8E/DgJDVUUNyD8/s1600/PlaysByTime.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;There are also smaller flat spots in September and December, but I am going to decide that those are small enough not to affect any further analyses. From this, I am going to use data only from August 2012 to present.&lt;br /&gt;&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;playlist = subset(playlist, Day &amp;gt;= mdy(&#34;8/1/12&#34;))&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;Next up, I am going to use this data to analyze the plays of artists from &lt;a href=&#34;http://cd1025.com/summerfest&#34;&gt;Summerfest&lt;/a&gt;, and try to infer if the play counts varied once they were added to the bill.&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Copying Data from Excel to R and Back</title>
      <link>/2013/02/24/copying-data-from-excel-to-r-and-back/</link>
      <pubDate>Sun, 24 Feb 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/02/24/copying-data-from-excel-to-r-and-back/</guid>
      <description>
&lt;div class=&#39;post&#39;&gt;
A lot of times we are given a data set in Excel format and we want to run a quick analysis using R&#39;s functionality to look at advanced statistics or make better visualizations. There are packages for importing/exporting data from/to Excel, but I have found them to be hard to work with or only work with old versions of Excel (*.xls, not *.xlsx). So for a one time analysis, I usually save the file as a csv and import it into R.&lt;br /&gt;&lt;br /&gt;This can be a little burdensome if you are trying to do something quick and creates a file that needs to be cleaned up later. An easier option is to copy and paste the data directly into R. This can be done by using &#34;clipboard&#34; as the file and specifying that it is tab delimited, since that is how Excel&#39;s clipboard stores the data.&lt;br /&gt;&lt;br /&gt;For example, say you have a table in excel you want to copy into R. First, copy it in Excel.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-qPkCaCo_zWw/USpK8GLtKsI/AAAAAAAAHq0/9LrZwb05LVg/s1600/Capture.PNG&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-qPkCaCo_zWw/USpK8GLtKsI/AAAAAAAAHq0/9LrZwb05LVg/s1600/Capture.PNG&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;br /&gt;Then go into R and use this function.&lt;br /&gt;&lt;br /&gt;&lt;div style=&#34;overflow: auto;&#34;&gt;&lt;div class=&#34;geshifilter&#34;&gt;&lt;pre class=&#34;r geshifilter-R&#34; style=&#34;font-family: monospace;&#34;&gt;read.excel &amp;lt;- &lt;a href=&#34;http://inside-r.org/r-doc/base/function&#34;&gt;&lt;span style=&#34;color: #003399; font-weight: bold;&#34;&gt;function&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;color: #009900;&#34;&gt;(&lt;/span&gt;header=&lt;span style=&#34;color: black; font-weight: bold;&#34;&gt;TRUE&lt;/span&gt;&lt;span style=&#34;color: #339933;&#34;&gt;,&lt;/span&gt;...&lt;span style=&#34;color: #009900;&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color: #009900;&#34;&gt;{&lt;/span&gt;&lt;br /&gt;  &lt;a href=&#34;http://inside-r.org/r-doc/utils/read.table&#34;&gt;&lt;span style=&#34;color: #003399; font-weight: bold;&#34;&gt;read.table&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;color: #009900;&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color: blue;&#34;&gt;&#34;clipboard&#34;&lt;/span&gt;&lt;span style=&#34;color: #339933;&#34;&gt;,&lt;/span&gt;sep=&lt;span style=&#34;color: blue;&#34;&gt;&#34;&lt;span style=&#34;color: #000099; font-weight: bold;&#34;&gt;\t&lt;/span&gt;&#34;&lt;/span&gt;&lt;span style=&#34;color: #339933;&#34;&gt;,&lt;/span&gt;header=header&lt;span style=&#34;color: #339933;&#34;&gt;,&lt;/span&gt;...&lt;span style=&#34;color: #009900;&#34;&gt;)&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;color: #009900;&#34;&gt;}&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;dat=read.excel&lt;span style=&#34;color: #009900;&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color: #009900;&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;br /&gt;This function specifies that you are reading data from the clipboard, that it is tab delimited, and that it has a header.&lt;br /&gt;&lt;br /&gt;Similarly, you can copy from R to Excel using the same logic. Here I also make &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;row.name=FALSE&lt;/span&gt; as default since I rarely have meaningful row names and they mess up the header alignment.&lt;br /&gt;&lt;br /&gt;&lt;div style=&#34;overflow: auto;&#34;&gt;&lt;div class=&#34;geshifilter&#34;&gt;&lt;pre class=&#34;r geshifilter-R&#34; style=&#34;font-family: monospace;&#34;&gt;write.excel &amp;lt;- &lt;a href=&#34;http://inside-r.org/r-doc/base/function&#34;&gt;&lt;span style=&#34;color: #003399; font-weight: bold;&#34;&gt;function&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;color: #009900;&#34;&gt;(&lt;/span&gt;x&lt;span style=&#34;color: #339933;&#34;&gt;,&lt;/span&gt;&lt;a href=&#34;http://inside-r.org/r-doc/base/row.names&#34;&gt;&lt;span style=&#34;color: #003399; font-weight: bold;&#34;&gt;row.names&lt;/span&gt;&lt;/a&gt;=&lt;span style=&#34;color: black; font-weight: bold;&#34;&gt;FALSE&lt;/span&gt;&lt;span style=&#34;color: #339933;&#34;&gt;,&lt;/span&gt;col.names=&lt;span style=&#34;color: black; font-weight: bold;&#34;&gt;TRUE&lt;/span&gt;&lt;span style=&#34;color: #339933;&#34;&gt;,&lt;/span&gt;...&lt;span style=&#34;color: #009900;&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color: #009900;&#34;&gt;{&lt;/span&gt;&lt;br /&gt;  &lt;a href=&#34;http://inside-r.org/r-doc/utils/write.table&#34;&gt;&lt;span style=&#34;color: #003399; font-weight: bold;&#34;&gt;write.table&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;color: #009900;&#34;&gt;(&lt;/span&gt;x&lt;span style=&#34;color: #339933;&#34;&gt;,&lt;/span&gt;&lt;span style=&#34;color: blue;&#34;&gt;&#34;clipboard&#34;&lt;/span&gt;&lt;span style=&#34;color: #339933;&#34;&gt;,&lt;/span&gt;sep=&lt;span style=&#34;color: blue;&#34;&gt;&#34;&lt;span style=&#34;color: #000099; font-weight: bold;&#34;&gt;\t&lt;/span&gt;&#34;&lt;/span&gt;&lt;span style=&#34;color: #339933;&#34;&gt;,&lt;/span&gt;&lt;a href=&#34;http://inside-r.org/r-doc/base/row.names&#34;&gt;&lt;span style=&#34;color: #003399; font-weight: bold;&#34;&gt;row.names&lt;/span&gt;&lt;/a&gt;=&lt;a href=&#34;http://inside-r.org/r-doc/base/row.names&#34;&gt;&lt;span style=&#34;color: #003399; font-weight: bold;&#34;&gt;row.names&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;color: #339933;&#34;&gt;,&lt;/span&gt;col.names=col.names&lt;span style=&#34;color: #339933;&#34;&gt;,&lt;/span&gt;...&lt;span style=&#34;color: #009900;&#34;&gt;)&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;color: #009900;&#34;&gt;}&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;write.excel&lt;span style=&#34;color: #009900;&#34;&gt;(&lt;/span&gt;dat&lt;span style=&#34;color: #009900;&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;a href=&#34;http://www.inside-r.org/pretty-r&#34; title=&#34;Created by Pretty R at inside-R.org&#34;&gt;Created by Pretty R at inside-R.org&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;These functions can be added to you .RProfile so that they are always ready for a quick analysis!&lt;br /&gt;&lt;br /&gt;Obviously, this technique does not encourage reproducible research. It is meant to be used for quick, ad hoc analysis and plotting; not something you would use for an analysis that needs to be done on a regular basis.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;Comments&lt;/h2&gt;
&lt;div class=&#39;comments&#39;&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Justin Tapp&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
I&amp;#39;m using this function in R for Windows. When I view the data numerically, it&amp;#39;s fine. But when I go to plot the data I get nonsense. How to describe it...the window has 0 to 250 on the x-axis (regardless of my data series) and white boxes across the middle. &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Marek Sz&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
I create similar functions and got few tips. For reading from excel following settings can be useful:&lt;br /&gt;na.strings = &amp;quot;&amp;quot; # to prevent replacing NA string to missing value&lt;br /&gt;comment.char = &amp;quot;&amp;quot; # to not loose everything after # sign&lt;br /&gt;quote = &amp;quot;&amp;quot; # or &amp;#39; or &amp;quot; could mess with data&lt;br /&gt;check.names = FALSE # if you want column names as in excel (spaces, special characters, etc.). You need to use `column name` in R to reference such columns.&lt;br /&gt;&lt;br /&gt;For writing na=&amp;quot;&amp;quot; replace missing values by empty string and not &amp;quot;NA&amp;quot; as on default.&lt;br /&gt;&lt;br /&gt;Second thing is that you can increase size of clipboard by using e.g. &amp;quot;clipboard-10240&amp;quot; instead of &amp;quot;clipboard&amp;quot; (it&amp;#39;s a size in Kb, so it&amp;#39;s around 10Mb; see help for connection, section Clipboard) which allow to copy and paste larger tables.&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Anonymous&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
rkward (http://rkward.sourceforge.net/) has a very nifty feature (Edit -&amp;gt; Paste Special...), that allows you to paste the copied data directly into your R source code, already formatted as a single string, vector or matrix. &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;William Yarberry&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
Excellent article.  Real people are always busy and this is just the kind of article that helps us all.  I have been doing scan() but when you start entering a few thousand rows that way, it gets a bit slow.  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Anonymous&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
scan() allows you to just paste...&lt;br /&gt;&lt;br /&gt;y &amp;lt;- scan()&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Anonymous&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
Thank you for these!!&lt;br /&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Anonymous&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
Thank you for the tip, this will help me a lot.&lt;br /&gt;Also, it seems that, libreOffice also uses clipboard to store copied things. This function also works for libreOffice&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Anonymous&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
I can&amp;#39;t resist: you could just use&lt;br /&gt;read.delim(&amp;quot;clipboard&amp;quot;)&lt;br /&gt;&lt;br /&gt;(The &amp;quot;clipboard&amp;quot; parameter is &amp;#39;doze only for the foreseeable future)&lt;br /&gt;&lt;br /&gt;From &amp;quot;?read.delim&amp;quot;&lt;br /&gt;read.delim(file, header = TRUE, sep = &amp;quot;\t&amp;quot;, quote=&amp;quot;\&amp;quot;&amp;quot;, dec=&amp;quot;.&amp;quot;,&lt;br /&gt;           fill = TRUE, comment.char=&amp;quot;&amp;quot;, ...)&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Anonymous&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
Thank you very much Tony for your quick answer (on a  Sunday afternoon!!). &lt;br /&gt;&lt;br /&gt;Ernesto&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Tony Hirst&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
@Ernesto It seems that on a Mac, you can use pbpaste ( http://stackoverflow.com/questions/9035674/r-function-to-copy-to-clipboard-on-mac-osx )&lt;br /&gt;&lt;br /&gt;read.clipboard.mac &amp;lt;- function(header=TRUE,...) {&lt;br /&gt;  read.table(pipe(&amp;quot;pbpaste&amp;quot;),sep=&amp;quot;\t&amp;quot;,header=header,...)&lt;br /&gt;}&lt;br /&gt; &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Anonymous&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
Thank you very much. Very useful. I work in both Windows and Mac Environments. The trick you show seems to work only in Windows. Any idea what to do in Mac? Thanks in advance,&lt;br /&gt;&lt;br /&gt;Ernesto&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Magical Sparse Matrix</title>
      <link>/2012/07/20/the-magical-sparse-matrix/</link>
      <pubDate>Fri, 20 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>/2012/07/20/the-magical-sparse-matrix/</guid>
      <description>
&lt;div class=&#39;post&#39;&gt;
I have been toying around with Kaggle&#39;s &lt;a href=&#34;http://www.kaggle.com/c/msdchallenge/&#34;&gt;Million Song Dataset Challenge&lt;/a&gt; recently because I have some interest in &lt;a href=&#34;http://en.wikipedia.org/wiki/Collaborative_filtering&#34;&gt;collaborative filtering&lt;/a&gt; (using &lt;a href=&#34;http://www2.research.att.com/%7Evolinsky/papers/ieeecomputer.pdf&#34; target=&#34;_blank&#34;&gt;matrix factorization&lt;/a&gt;). I haven&#39;t made much progress with the competition (all 3 of my submissions are below the baseline), but I have learned a few things about dealing with large amounts of data.&lt;br /&gt;&lt;br /&gt;The goal of the competition is to predict the 500 most likely songs each of 110,000 users will listen to next. As the name implies, there are 1,000,000 songs in the full dataset. To simplify things, I decided to concentrate on the most popular songs. I created a 110,000 x 2,000 matrix of 0&#39;s and 1&#39;s. Row i, column j is 1 if user i had listened to song j (the jth most popular song) and 0 if user i had not. As you can imagine, there are a lot more 0&#39;s than 1&#39;s in this matrix. The first few rows and columns look like this:&lt;br /&gt;&lt;br /&gt;&lt;div style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&lt;span style=&#34;font-size: small;&#34;&gt;0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 ...&lt;/span&gt;&lt;/div&gt;&lt;div style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&lt;span style=&#34;font-size: small;&#34;&gt;0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...&lt;/span&gt;&lt;/div&gt;&lt;div style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&lt;span style=&#34;font-size: small;&#34;&gt;1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...&lt;/span&gt;&lt;/div&gt;&lt;div style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&lt;span style=&#34;font-size: small;&#34;&gt;...&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;This matrix was about 430 Mb and took a while to load into MATLAB. So I wisened up and created a &lt;a href=&#34;http://en.wikipedia.org/wiki/Sparse_matrix&#34;&gt;sparse matrix&lt;/a&gt;. A sparse matrix realizes that most of the values are 0&#39;s and does not record them. Instead, it lists the locations of the non-zero elements and what the value is. For example, this is what the first few rows of the sparse matrix looks like:&lt;br /&gt;&lt;br /&gt;&lt;div style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&lt;span style=&#34;font-size: small;&#34;&gt;1 3 1&lt;br /&gt;1 7 1&lt;br /&gt;1 10 1&lt;br /&gt;1 13 1&lt;br /&gt;1 82 1&lt;br /&gt;1 717 1&lt;br /&gt;2 1111 1&lt;br /&gt;2 2972 1&lt;br /&gt;2 3516 1&lt;/span&gt;&lt;/div&gt;&lt;div style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&lt;span style=&#34;font-size: small;&#34;&gt;...&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;The first column is the row number, the second is the column number, and the third is the value at that location. In this application, all the values are 1. For this matrix, I used the 50,000 most popular songs (instead of just 2,000), and the size was much smaller -- just 17 Mb.&lt;br /&gt;&lt;br /&gt;It is easy to load the sparse matrix into MATLAB with the &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;spconvert&lt;/span&gt; command, and many of MATLAB&#39;s functions (like singular value decomposition) are optimized for sparse matrices.&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
