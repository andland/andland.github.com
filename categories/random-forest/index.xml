<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Random forest on Statistically Significant</title>
    <link>/categories/random-forest/</link>
    <description>Recent content in Random forest on Statistically Significant</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Jul 2012 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/categories/random-forest/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Random Forest Variable Importance</title>
      <link>/2012/07/19/random-forest-variable-importance/</link>
      <pubDate>Thu, 19 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>/2012/07/19/random-forest-variable-importance/</guid>
      <description>
&lt;div class=&#39;post&#39;&gt;
&lt;a href=&#34;http://en.wikipedia.org/wiki/Random_forest&#34;&gt;Random&lt;/a&gt; &lt;a href=&#34;http://www.stat.berkeley.edu/%7Ebreiman/RandomForests/&#34;&gt;forests&lt;/a&gt; â„¢ are great. They are one of the best &#34;black-box&#34; supervised learning methods. If you have lots of data and lots of predictor variables, you can do worse than random forests. They can deal with messy, real data. If there are lots of extraneous predictors, it has no problem. It automatically does a good job of finding interactions as well. There are no assumptions that the response has a linear (or even smooth) relationship with the predictors.&lt;br /&gt;&lt;br /&gt;As cited in the Wikipedia article, they do lack some interpretability. But what they lack in interpretation, they more than make up for in prediction power, which I believe is much more important than interpretation in most cases. Even though you cannot easily tell how one variable affects the prediction, you can easily create a &lt;i&gt;partial dependence plot&lt;/i&gt; which shows how the response will change as you change the predictor. You can also do this for two variables at once to see the interaction of the two.&lt;br /&gt;&lt;br /&gt;Also helping in the interpretation is that they can output a list of predictor variables that they believe to be important in predicting the outcome. If nothing else, you can subset the data to only include the most &#34;important&#34; variables, and use that with another model. The randomForest package in R has two measures of importance. &lt;a href=&#34;http://rss.acs.unt.edu/Rdoc/library/randomForest/html/importance.html&#34;&gt;One &lt;/a&gt;is &#34;total decrease in node impurities from splitting on the variable, averaged over all trees.&#34; I do not know much about this one, and will not talk about it further. The other is based on a &lt;a href=&#34;http://en.wikipedia.org/wiki/Resampling_%28statistics%29#Permutation_tests&#34;&gt;permutation test&lt;/a&gt;. The idea is that if the variable is not important (the null hypothesis), then rearranging the values of that variable will not degrade prediction accuracy. Random forests use out-of-bag (OOB) samples to measure prediction accuracy.&lt;br /&gt;&lt;br /&gt;In my experience, it does a pretty good job of finding the most important predictors, but it has issues with correlated predictors. For example, I was working on a problem where I was predicting the price that electricity trades. One feature that I knew would be very important was the amount of electricity being used at that same time. But I thought there might also be a relationship between price and the electricity being used a few hours before and after. When I ran the random forest with these variables, the electricity used 1 hour after was found to be more important than the electricity used at the same time. When including the 1 hour after electricity use instead of the current hour electricity use, the cross validation (CV) error increased. Using both did not significantly change the CV error compared to using just current hour. Because the electricity used at the current hour and the hour after are very correlated, it had trouble telling which one was more important. In truth, given the electricity use at the current hour, the electricity use at the hour after did not improve the predictive accuracy.&lt;br /&gt;&lt;br /&gt;Why does the importance measure give more weight correlated predictors? &lt;a href=&#34;http://www.biomedcentral.com/1471-2105/9/307/&#34;&gt;Strobl et al.&lt;/a&gt; give some intuition behind what is happening and propose a solution. Basically, the permutation test is ill-posed. The permutation test is testing that the variable is independent of the response as well as all other predictors. Since the correlated predictors are obviously not independent, we get high importance scores. They propose a permutation test where you condition on the correlated predictors. This is a little tricky when the correlated predictors are continuous, but you can read the paper for more details.&lt;br /&gt;&lt;br /&gt;Another way to think of it is that, since each split only considers a subset of the possible variables, a variable that is correlated with an &#34;important&#34; variable may be considered without the &#34;important&#34; variable. This would cause the correlated variable to be selected for the split. The correlated value does hold some predictive value, but only because of the truly important variable, so it is understandable why this procedure would consider it important.&lt;br /&gt;&lt;br /&gt;I ran a simulation experiment (similar to the one in the paper) to demonstrate the issue. I simulated 5 predictor variables. Only the first one is related to the response, but the second one has a correlation of about 0.7 with the first one. Luckily, Strobl et al. included their version of importance in the package &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;party&lt;/span&gt; in &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;R&lt;/span&gt;. I compare variable importance from the &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;randomForest &lt;/span&gt;package and the importance with and without taking correlated predictors into account from the &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;party&lt;span style=&#34;font-family: inherit;&#34;&gt; &lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;font-family: inherit;&#34;&gt;pac&lt;/span&gt;kage.&lt;br /&gt;&lt;br /&gt;&lt;pre class=&#34;brush: r&#34;&gt;# simulate the data&lt;br /&gt;x1=rnorm(1000)&lt;br /&gt;x2=rnorm(1000,x1,1)&lt;br /&gt;y=2*x1+rnorm(1000,0,.5)&lt;br /&gt;df=data.frame(y,x1,x2,x3=rnorm(1000),x4=rnorm(1000),x5=rnorm(1000))&lt;br /&gt;&lt;br /&gt;# run the randomForest implementation&lt;br /&gt;library(randomForest)&lt;br /&gt;rf1 &amp;lt;- randomForest(y~., data=df, mtry=2, ntree=50, importance=TRUE)&lt;br /&gt;importance(rf1,type=1)&lt;br /&gt;&lt;br /&gt;# run the party implementation&lt;br /&gt;library(party)&lt;br /&gt;cf1 &amp;lt;- cforest(y~.,data=df,control=cforest_unbiased(mtry=2,ntree=50))&lt;br /&gt;varimp(cf1)&lt;br /&gt;varimp(cf1,conditional=TRUE)&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;For the &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;randomForest&lt;/span&gt;, the ratio of importance of the the first and second variable is 4.53. For &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;party&lt;/span&gt; without accounting for correlation it is 7.35. And accounting for correlation, it is 369.5. The higher ratios are better because it means that the importance of the first variable is more prominent. party&#39;s implementation is clearly doing the job.&lt;br /&gt;&lt;br /&gt;There is a downside. It takes much longer to calculate importance with correlated predictors than without. For the party package in this example, it took 0.39 seconds to run without and 204.34 seconds with. I could not even run the correlated importance on the electricity price example. There might be a research opportunity to get a quicker approximation.&lt;br /&gt;&lt;br /&gt;Possibly up next: confidence limits for random forest predictions.&lt;/div&gt;
&lt;h2&gt;Comments&lt;/h2&gt;
&lt;div class=&#39;comments&#39;&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Abhijeet Patil&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
hi,m new to random forest.. i ve some doubts..&lt;br /&gt;1)&amp;quot;we need to choose m number of variables randomly for each node..&amp;quot;-can u pleas explain it..&lt;br /&gt;OR&lt;br /&gt;2)&amp;quot;take 1 bootstrap sample,choose some variables and create a decision tree&amp;quot;-is it correct??&lt;br /&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Anonymous&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
Could you tell me what the numbers mean when ctree and randonForest return variable importance numbers please? I can get the output but I don&amp;#39;t know how to interpret them.&lt;br /&gt;&lt;br /&gt;Than you very much.&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Anonymous&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
HI Andrew, When i tried to calculate the variable importance, I get the error &lt;br /&gt;&amp;quot;Error in model.matrix.default(as.formula(f), data = blocks) : &lt;br /&gt;  allocMatrix: too many elements specified&amp;quot;&lt;br /&gt;&lt;br /&gt;I&amp;#39;m not sure how to deal with it because I have 2 factor levels which I&amp;#39;m predicting with 23 continuous variable predictors (about 2200 data points. I know it&amp;#39;s a lot to work with, but not sure how to get the correct VarImp values with this large data set.&lt;br /&gt;&lt;br /&gt;I tried increasing the threshold, but it didn&amp;#39;t help. fit.varimp=varimp(fit.cf,threshold=0.8,conditional=TRUE)&lt;br /&gt;&lt;br /&gt;Do you have any suggestions? Thanks.&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Andrew Landgraf&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
Unfortunately, I didn&amp;#39;t set the seed. #nexttime&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Anonymous&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
you should provide the seed to reproduce your example&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Andrew&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
It should be able to give you a solution. I have no idea what ramifications this would have on accuracy or variable importance.&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Anonymous&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
I have microarray data on 30 entries and thought of using RF to identify important response genes. I understand that this is a N&amp;lt;&amp;lt;p type of problems, but will &amp;#39;party&amp;#39; be able to come up with a solution?&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Andrew&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
The simple answer is using the partialPlot function in the randomForest package for R (http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=randomForest:partialPlot). &lt;br /&gt;&lt;br /&gt;What it is doing isn&amp;#39;t all that complicated. Say you want a partial dependence plot for the variable X_1. For each value of X_1=x that you want to plot, you take the average of the prediction with X_1=x and the other explanatory variables equal to the n values that they are in the data set. You are trying to average out the other variables.&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;nanounanue&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
Great explanation!&lt;br /&gt;&lt;br /&gt;I have a question for you: In your text you said:&lt;br /&gt;&lt;br /&gt;&amp;quot;&amp;quot;&amp;quot;&lt;br /&gt; ... Even though you cannot easily tell how one variable affects the prediction, you can easily create a partial dependence plot which shows how the response will change as you change the predictor. You can also do this for two variables at once to see the interaction of the two.&lt;br /&gt;&amp;quot;&amp;quot;&amp;quot;&lt;br /&gt;&lt;br /&gt;Could you provide, please, an example of how build the partial dependence plot?&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Thank you for the post and thanks in advance&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#39;comment&#39;&gt;
&lt;div class=&#39;author&#39;&gt;Anonymous&lt;/div&gt;
&lt;div class=&#39;content&#39;&gt;
Great post!  Thank you.&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
